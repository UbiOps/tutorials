{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BFz2g-DOUZzt"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://ubiops.com/wp-content/uploads/2020/12/Group-2.svg\" width=\"100\"/>\n",
        "</div>\n",
        "\n",
        "# Convert your MNIST model from Tensorflow to ONNX and run it on UbiOps twice as fast\n",
        "\n",
        "ONNX is an open format that is used to represent various Machine Learning models. It can also function as a model compression technique.  In this tutorial we will show you how to convert a Tensorflow based image classification algorithm to ONNX and \n",
        "run it on UbiOps using the ONNX runtime. We will show that this allows you to run an inferencing job twice as fast!\n",
        "\n",
        "First lets connect to UbiOps and load all of our dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02fnzn8UUaeT"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.10 tf2onnx==1.13.0 tqdm==4.64.1 'ubiops>=3.12.0' 'protobuf>=3.19.4'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kRcy5owqWoVD"
      },
      "source": [
        "First connect to our API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScyAuiUWVFPk"
      },
      "outputs": [],
      "source": [
        "API_TOKEN = 'Token ' # Fill in your token here\n",
        "PROJECT_NAME = ''    # Fill in your project name here\n",
        "DEPLOYMENT_NAME = 'tf-vs-onnx-test'\n",
        "import ubiops\n",
        "import shutil\n",
        "import random, glob\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "\n",
        "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
        "configuration.api_key['Authorization'] = API_TOKEN\n",
        "\n",
        "client = ubiops.ApiClient(configuration)\n",
        "api = ubiops.CoreApi(client)\n",
        "api.service_status()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AFa90uRXWvfy"
      },
      "source": [
        "## Converting the model\n",
        "\n",
        "We first download an h5 model from our public online bucket, then convert it as a `SavedModel`. Lastly, we convert it to an `onnx` model using the `tf2onnx` package.\n",
        "\n",
        "If everything worked correctly you should have the ONNX model at ```mnist_deployment_onnx_package/mnist.onnx```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ9178eBe3DT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#Get bucket from online repo\n",
        "bucket_name = \"ubiops\"\n",
        "file_path = \"demo-helper-files/cnn.zip\"\n",
        "\n",
        "# Create the URL for the file\n",
        "url = f\"https://storage.googleapis.com/{bucket_name}/{file_path}\"\n",
        "\n",
        "#Write zipfile to cnn folder\n",
        "urllib.request.urlretrieve(url, \"cnn\")\n",
        "\n",
        "#write modelfile to cnn_dir folder\n",
        "with zipfile.ZipFile(\"cnn\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('cnn_dir')\n",
        "\n",
        "model = load_model(\"cnn_dir/cnn.h5\")\n",
        "\n",
        "#Save as a SavedModel to the mnist_model directory\n",
        "!mkdir mnist_model\n",
        "model.save(\"mnist_model\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MDr_aGktf2qF"
      },
      "source": [
        "## Preparing the comparison\n",
        "\n",
        "The next step is to create two deployments. One with the original Tensorflow based runtime and the second with the ONNX model runnning on the ONNX runtime.\n",
        "\n",
        "The following code will save the Tensorflow model, the requirements.txt's and the deployments.py's to the mnist_deployment_package directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R29Ouc-cf-AK"
      },
      "outputs": [],
      "source": [
        "!mkdir mnist_deployment_package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUuC9l1ekp3g"
      },
      "outputs": [],
      "source": [
        "#Copy the tensorflowmodel to the deployment package\n",
        "import shutil\n",
        "shutil.copy('cnn_dir/cnn.h5', 'mnist_deployment_package/cnn.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr5Muas_f_wr"
      },
      "outputs": [],
      "source": [
        "%%writefile ./mnist_deployment_package/requirements.txt\n",
        "\n",
        "# This file contains package requirements for the deployment\n",
        "# installed via PIP. Installed before deployment initialization\n",
        "tensorflow==2.10\n",
        "imageio==2.26.0\n",
        "h5py==3.8.0\n",
        "numpy==1.24.1\n",
        "Pillow==9.4.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8Qo4TFyf5RT"
      },
      "outputs": [],
      "source": [
        "%%writefile ./mnist_deployment_package/deployment.py\n",
        "\"\"\"\n",
        "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
        "class and 'request' method.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from imageio import imread\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Deployment:\n",
        "\n",
        "    def __init__(self, base_directory, context):\n",
        "\n",
        "        print(\"Initialising deployment\")\n",
        "\n",
        "        weights = os.path.join(base_directory, \"cnn.h5\")\n",
        "        self.model = load_model(weights)\n",
        "\n",
        "    def request(self, data):\n",
        "\n",
        "        print(\"Processing request\")\n",
        "\n",
        "        x = imread(data['image'])\n",
        "        # convert to a 4D tensor to feed into our model\n",
        "        x = x.reshape(1, 28, 28, 1)\n",
        "        x = x.astype(np.float32) / 255\n",
        "\n",
        "        out = self.model.predict(x)\n",
        "\n",
        "        # here we set our output parameters in the form of a json\n",
        "        return {'prediction': int(np.argmax(out)), 'probability': float(np.max(out))}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlmPpxmjlsU7"
      },
      "source": [
        "Now build a deployment package that hosts the ONNX model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4aM1fFEqse5"
      },
      "outputs": [],
      "source": [
        "!mkdir mnist_deployment_onnx_package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU3HEzpiqt8S"
      },
      "outputs": [],
      "source": [
        "#Convert the model from SavedModel format to onnx, and store inside the ONNX deployment package\n",
        "!python3 -m tf2onnx.convert --saved-model mnist_model --opset 13 --output mnist_deployment_onnx_package/mnist.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDApd2j6lnCS"
      },
      "outputs": [],
      "source": [
        "%%writefile ./mnist_deployment_onnx_package/deployment.py\n",
        "\"\"\"\n",
        "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
        "class and 'request' method.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import onnxruntime as rt\n",
        "from imageio import imread\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Deployment:\n",
        "\n",
        "    def __init__(self, base_directory, context):\n",
        "        self.sess = rt.InferenceSession(\"mnist.onnx\")\n",
        "        self.input_name = self.sess.get_inputs()[0].name\n",
        "\n",
        "    def request(self, data):\n",
        "\n",
        "\n",
        "        x = imread(data['image'])\n",
        "        # convert to a 4D tensor to feed into our model\n",
        "        x = x.reshape(1, 28, 28, 1) \n",
        "        x = x.astype(np.float32) / 255\n",
        "\n",
        "        print(\"Prediction being made\")\n",
        "\n",
        "        prediction = self.sess.run(None, {self.input_name: x})[0]\n",
        "\n",
        "        return {'prediction': int(np.argmax(prediction)), 'probability': float(np.max(prediction))}\n",
        "\n",
        "       \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCitKKaol7Pk"
      },
      "outputs": [],
      "source": [
        "%%writefile ./mnist_deployment_onnx_package/requirements.txt\n",
        "\n",
        "# This file contains package requirements for the deployment\n",
        "# installed via PIP. Installed before deployment initialization\n",
        "\n",
        "onnx==1.12.0\n",
        "onnxruntime==1.12.0\n",
        "imageio==2.26.0\n",
        "numpy==1.24.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fOC4_VilmHwH"
      },
      "source": [
        "Now that the deployment packages are created, you can upload them to UbiOps. We will make one deployment with two versions, one running Tensorflow while the other is running ONNX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq9nbkpDl-6B"
      },
      "outputs": [],
      "source": [
        "mnist_template = ubiops.DeploymentCreate(\n",
        "    name=DEPLOYMENT_NAME,\n",
        "    description='A deployment to classify handwritten digits.',\n",
        "    input_type='structured',\n",
        "    output_type='structured',\n",
        "    input_fields=[\n",
        "        {'name': 'image', 'data_type': 'file'}\n",
        "    ],\n",
        "    output_fields=[\n",
        "        {'name': 'prediction', 'data_type': 'int'},\n",
        "        {'name': 'probability', 'data_type': 'double'}\n",
        "    ]\n",
        ")\n",
        "\n",
        "mnist_deployment = api.deployments_create(project_name=PROJECT_NAME, data=mnist_template)\n",
        "print(mnist_deployment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaCozzcKmJVh"
      },
      "outputs": [],
      "source": [
        "version_template = ubiops.DeploymentVersionCreate(\n",
        "    version=\"onnx\",\n",
        "    environment='python3-10',\n",
        "    instance_type_group_name='1024 MB + 0.25 vCPU',\n",
        "    maximum_instances=1,\n",
        "    minimum_instances=0,\n",
        "    maximum_idle_time=1800, # = 30 minutes\n",
        "    request_retention_mode='full',  # input/output of requests will be stored\n",
        "    request_retention_time=3600  # requests will be stored for 1 hour\n",
        ")\n",
        "\n",
        "version = api.deployment_versions_create(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    data=version_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T4S84gEmW9f"
      },
      "outputs": [],
      "source": [
        "# Zip the deployment package\n",
        "shutil.make_archive('mnist_deployment_onnx_package', 'zip', '.', 'mnist_deployment_onnx_package')\n",
        "\n",
        "\n",
        "upload_response = api.revisions_file_upload(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    version=\"onnx\",\n",
        "    file='mnist_deployment_onnx_package.zip'\n",
        ")\n",
        "print(upload_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FnSJ2B6mX9z"
      },
      "outputs": [],
      "source": [
        "version_template = ubiops.DeploymentVersionCreate(\n",
        "    version=\"tf\",\n",
        "    environment='python3-10',\n",
        "    instance_type_group_name='1024 MB + 0.25 vCPU',\n",
        "    maximum_instances=1,\n",
        "    minimum_instances=0,\n",
        "    maximum_idle_time=1800, # = 30 minutes\n",
        "    request_retention_mode='full',  # input/output of requests will be stored\n",
        "    request_retention_time=3600  # requests will be stored for 1 hour\n",
        ")\n",
        "\n",
        "version = api.deployment_versions_create(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    data=version_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC3xy3NrmY5J"
      },
      "outputs": [],
      "source": [
        "# Zip the deployment package\n",
        "shutil.make_archive('mnist_deployment_package', 'zip', '.', 'mnist_deployment_package')\n",
        "\n",
        "\n",
        "upload_response = api.revisions_file_upload(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    version=\"tf\",\n",
        "    file='mnist_deployment_package.zip'\n",
        ")\n",
        "print(upload_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And let's wait until the deployment versions are built until we continue.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ubiops.utils.wait_for_deployment_version(client = api.api_client,\n",
        "                                   project_name = PROJECT_NAME,\n",
        "                                   deployment_name = DEPLOYMENT_NAME,\n",
        "                                   version = \"onnx\")\n",
        "\n",
        "ubiops.utils.wait_for_deployment_version(client = api.api_client,\n",
        "                                   project_name = PROJECT_NAME,\n",
        "                                   deployment_name = DEPLOYMENT_NAME,\n",
        "                                   version = \"tf\")\n",
        "\n",
        "print(\"Deployments are ready\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1gu9RMyIme-L"
      },
      "source": [
        "## Benchmarking\n",
        "\n",
        "If everything went well there should now be a deployment in UbiOps with two versions. We can now compare the average request time by sending both versions a list of 100 images (one image per request.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRuUrO5dn9cg"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import zipfile\n",
        "#Get dummy data from our online bucket\n",
        "bucket_name = \"ubiops\"\n",
        "file_path = \"demo-helper-files/mnist_png.zip\"\n",
        "\n",
        "# Create the URL for the file\n",
        "url = f\"https://storage.googleapis.com/{bucket_name}/{file_path}\"\n",
        "\n",
        "urllib.request.urlretrieve(url, \"mnist_png.zip\")\n",
        "\n",
        "with zipfile.ZipFile(\"mnist_png.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6v4CPU2mc82"
      },
      "outputs": [],
      "source": [
        "pattern = \"mnist_png/testing/*/*.png\" # (or \"*.*\")\n",
        "filenames = random.choices(glob.glob(pattern),k=100)\n",
        "print(filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create one large batch requests that we can send to the deployment in one go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_request_data = []\n",
        "for image_file in tqdm(filenames):\n",
        "    # First upload the image\n",
        "    file_uri = ubiops.utils.upload_file(client, PROJECT_NAME, image_file)\n",
        "    # Make a request using the file URI as input.\n",
        "    data = {'image': file_uri}\n",
        "    batch_request_data.append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "requests_onnx = api.batch_deployment_version_requests_create(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    version=\"onnx\",\n",
        "    data=batch_request_data\n",
        ")\n",
        "requests_onnx_ids = [request_onnx.id for request_onnx in requests_onnx]\n",
        "\n",
        "requests_tf = api.batch_deployment_version_requests_create(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    version=\"tf\",\n",
        "    data=batch_request_data\n",
        ")\n",
        "requests_tf_ids = [request_tf.id for request_tf in requests_tf]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then we wait until all requests are finished.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vHLbl2SoP2i"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    requests_onnx = api.deployment_version_requests_batch_get(\n",
        "        project_name=PROJECT_NAME,\n",
        "        deployment_name=DEPLOYMENT_NAME,\n",
        "        version=\"onnx\",\n",
        "        data=requests_onnx_ids\n",
        "    )\n",
        "\n",
        "    requests_tf = api.deployment_version_requests_batch_get(\n",
        "        project_name=PROJECT_NAME,\n",
        "        deployment_name=DEPLOYMENT_NAME,\n",
        "        version=\"tf\",\n",
        "        data=requests_tf_ids\n",
        "    )\n",
        "\n",
        "    # Calculate the percentage of completed requests\n",
        "    onnx_completed_pct = sum(req.status == \"completed\" for req in requests_onnx) / len(requests_onnx) * 100 if requests_onnx else 0\n",
        "    tf_completed_pct = sum(req.status == \"completed\" for req in requests_tf) / len(requests_tf) * 100 if requests_tf else 0\n",
        "\n",
        "    print(f\"ONNX Completed Percentage: {onnx_completed_pct:.2f}%\")\n",
        "    print(f\"TensorFlow Completed Percentage: {tf_completed_pct:.2f}%\")\n",
        "\n",
        "    if onnx_completed_pct == 100 and tf_completed_pct == 100:\n",
        "        break\n",
        "\n",
        "    time.sleep(1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7sWX3TLSobie"
      },
      "source": [
        "## Comparing the results\n",
        "\n",
        "Now that the request are finished we can look at the results. You can do that either by looking at the 'Metrics' tab of \n",
        "the UbiOps webappby running the following piece of code.\n",
        "\n",
        "Note that it can take up to two minutes before metrics become available through our API. So might be required to sleep a \n",
        "bit more:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#First get the version ids so that we can filter the relevant metrics\n",
        "\n",
        "tf_version_id = api.deployment_versions_get(PROJECT_NAME,DEPLOYMENT_NAME, \"tf\").id\n",
        "onnx_version_id = api.deployment_versions_get(PROJECT_NAME,DEPLOYMENT_NAME, \"onnx\").id\n",
        "print(f\"Tensorflow deployment version id: {tf_version_id}\")\n",
        "print(f\"ONNX deployment version id: {onnx_version_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNvpThBmoSHJ"
      },
      "outputs": [],
      "source": [
        "tf_time_series = api.time_series_data_list(\n",
        "    project_name=PROJECT_NAME,\n",
        "    metric = \"deployments.request_duration\",\n",
        "    start_date=str((datetime.today()- timedelta(days=1)).isoformat()),\n",
        "    end_date=str(datetime.today().isoformat()),\n",
        "    aggregation_period = 60*60*24, # seconds/day\n",
        "    labels = f\"deployment_version_id:{tf_version_id}\"\n",
        ")\n",
        "print(f\"Average Tensorflow request duration: {tf_time_series.data_points[-1].value}s \")\n",
        "\n",
        "onnx_time_series = api.time_series_data_list(\n",
        "    project_name=PROJECT_NAME,\n",
        "    metric = \"deployments.request_duration\",\n",
        "    start_date=str((datetime.today()- timedelta(days=1)).isoformat()),\n",
        "    end_date=str(datetime.today().isoformat()),\n",
        "    aggregation_period = 60*60*24, # seconds/day\n",
        "    labels = f\"deployment_version_id:{onnx_version_id}\"\n",
        ")\n",
        "\n",
        "print(f\"Average ONNX request duration :{onnx_time_series.data_points[-1].value}s\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JcqJpeyJoioV"
      },
      "source": [
        "# Cleaning up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d3FZhlmodpX"
      },
      "outputs": [],
      "source": [
        "api.client_close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
