{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p-S1EGtc4FA"
   },
   "source": [
    "# Train and inference FinBERT on an MLOps pipeline by combining Weights & Biases and UbiOps\n",
    "\n",
    "In this notebook we will cover how to\n",
    "\n",
    "1.   Use UbiOps for model training, hyperparameter tuning and running inference\n",
    "2.   Use W&B for experiment tracking, model evaluation and comparison\n",
    "3.   Use W&B as a model registry to track models that have been moved to inference on UbiOps\n",
    "4.   Use UbiOps to transform the model in a live and scalable API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qL6wHgq9-dA"
   },
   "source": [
    "FinBERT, a pre-trained natural language processing (NLP) model, is designed to assess the sentiment of financial text. It is built by training the BERT language model in the finance domain on a substantial financial dataset. In this example, we will fine-tune it on a custom dataset. The model provides softmax outputs corresponding to three labels: positive, negative, or neutral sentiment. This model can be used through the library of HuggingFace `transformers`.\n",
    "\n",
    "We are going to use the UbiOps platform to fine-tune FinBERT on different hyperparameter configurations on a financial news dataset, in parallel, in the cloud, using CPUs. FinBERT is a generic model, thus one would also want to further fine-tune it on their own datasets. We will do three training jobs to find the best combination of hyperparameters, thus finding the best training flow.\n",
    "\n",
    "While the training jobs are running, we head over to Weights & Biases to analyze performance metrics during our training runs, and to compare the final models. After checking the accuracy metrics of all three training runs, we will store our best performing ML model on the Weights & Biases Model Registry, and deploy it by turning it into a live and scalable API endpoint on UbiOps. The model can be conveniently exposed to end-users via the API endpoint in a production set-up, allowing it to scale, depending on the demand.\n",
    "\n",
    "If you have a paid subscription account, you may upgrade your CPU instance to a GPU instance (e.g. `instance_type_group_name = 16384 MB + 4 vCPU + NVIDIA Tesla T4`)! You would also need to select an environment with CUDA compiled. More on this matter can be read [here](https://ubiops.com/docs/deployments/gpu-deployments/).\n",
    "\n",
    "The FinBERT model can be found [here](https://huggingface.co/ProsusAI/finbert) and the dataset [here](https://huggingface.co/datasets/Jean-Baptiste/financial_news_sentiment).\n",
    "The dataset we use is from the HuggingFace `datasets` library, and it consists of ~2000 Canadian news articles with manually validated financial sentiment. It also has a topic label, which can be used for further experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZAxAB3Sc4FD"
   },
   "outputs": [],
   "source": [
    "# This step may take a while\n",
    "!pip install -qU wandb\n",
    "!pip install -qU ubiops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfV-kB9zc4FE"
   },
   "outputs": [],
   "source": [
    "import ubiops\n",
    "\n",
    "\n",
    "API_TOKEN = \"\"  # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"\"  # You can name your UbiOps project however you want, but it must be globally unique and created in advance.\n",
    "\n",
    "ENVIRONMENT_NAME = \"finbert-environment\"\n",
    "EXPERIMENT_NAME = \"finbert-training\"\n",
    "\n",
    "INFERENCING_DEPLOYMENT_NAME = \"finbert-inference\"\n",
    "INFERENCING_DEPLOYMENT_VERSION = \"v1\"\n",
    "\n",
    "WANDB_ENTITY = \"\" # this is either your W&B username, or a W&B team you are part of.\n",
    "WANDB_PROJECT = \"finbert-training\"\n",
    "WANDB_API_KEY = \"\" # You can get your API key here: https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rJVQSiaHvzs"
   },
   "source": [
    "Set up a connection to the UbiOps API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5MLK_i5dDat"
   },
   "outputs": [],
   "source": [
    "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "api_client = ubiops.ApiClient(configuration)\n",
    "core_instance = ubiops.CoreApi(api_client=api_client)\n",
    "training_instance = ubiops.Training(api_client=api_client)\n",
    "print(core_instance.service_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjnCEGyfw2M5"
   },
   "source": [
    "Below we will build a training job that the API of UbiOps understands. A training job in UbiOps is called a run. To run the training job on UbiOps, we need to create a file named `train.py` and include our code here. This code will execute as a single *Run* as part of an *Experiment*. An *Experiment* can contain multiple training runs. Training runs inside the experiment run on top of an *Environment*. The *Environment* contains an instance type (hardware) and code dependencies. So let us start with making a directory store to store our environment instructions. Here, it contains a `requirements.txt` that contains the Python dependencies that our code needs to be able to run. In this case we use TensorFlow 2.13.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YpYvsNTc4FE"
   },
   "outputs": [],
   "source": [
    "!mkdir environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_e1st84b6ya"
   },
   "outputs": [],
   "source": [
    "%%writefile environment/requirements.txt\n",
    "datasets\n",
    "tensorflow==2.13.0\n",
    "transformers\n",
    "wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0ST0yML4yVB"
   },
   "source": [
    "Let's take a look at the training script. The script needs to contain a `train()` function, with input parameters `training_data` (a file path to your training data) and `parameters` (a dictionary that contains the parameters of your choice). The `training_data` path can be set to `None`, in case data is grabbed from an external location, such as an online object storage, or from a data science package, as we do in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPL0T5sdxMm-"
   },
   "outputs": [],
   "source": [
    "!mkdir training_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyP_TnLLc4FF"
   },
   "outputs": [],
   "source": [
    "%%writefile training_code/train.py\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "\n",
    "def train(training_data, parameters, context):\n",
    "    # Prepare the stock headlines datasets\n",
    "    dataset = load_dataset(\"Jean-Baptiste/financial_news_sentiment\")\n",
    "\n",
    "    # Split them into train and test datasets\n",
    "    train_ds = dataset[\"train\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "\n",
    "    hyperparameters = dict(\n",
    "        epochs=parameters.get(\"nr_epochs\", 10),\n",
    "        batch_size=parameters.get(\"batch_size\", 32),\n",
    "        learning_rate=parameters.get(\"learning_rate\", 2e-5),\n",
    "        weight_decay=parameters.get(\"weight_decay\", 0.01),\n",
    "    )\n",
    "\n",
    "    wandb_entity = os.getenv(\"WANDB_ENTITY\")\n",
    "    wandb_project = os.getenv(\"WANDB_PROJECT\")\n",
    "    wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    with wandb.init(entity=wandb_entity, project=wandb_project, config=hyperparameters) as train_run:\n",
    "        # Get FinBERT model using transformers\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "        finbert = TFBertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "        print(f\"This is the original finbert model with details: {finbert.summary()}\")\n",
    "        print(f\"Type of finbert{type(finbert)}\")\n",
    "\n",
    "        # Tokenize all dataset without padding\n",
    "        train_ds = train_ds.map(lambda x: tokenizer(x[\"title\"]), batched=True)\n",
    "\n",
    "        # Convert HuggingFace dataset to TF Data and combine sentences into batches with padding\n",
    "        train_ds = train_ds.to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "            label_cols=\"labels\",\n",
    "            batch_size=train_run.config.batch_size,\n",
    "            collate_fn=DataCollatorWithPadding(\n",
    "                tokenizer=tokenizer, return_tensors=\"tf\"\n",
    "            ),\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        # Also convert the test dataset\n",
    "        test_ds = test_ds.map(lambda x: tokenizer(x[\"title\"]), batched=True)\n",
    "        test_ds = test_ds.to_tf_dataset(\n",
    "            columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "            label_cols=\"labels\",\n",
    "            batch_size=train_run.config.batch_size,\n",
    "            collate_fn=DataCollatorWithPadding(\n",
    "                tokenizer=tokenizer, return_tensors=\"tf\"\n",
    "            ),\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Compile and fit model on the training dataset\n",
    "        finbert.compile(\n",
    "            optimizer=tf.keras.optimizers.AdamW(\n",
    "                learning_rate=train_run.config.learning_rate,\n",
    "                weight_decay=train_run.config.weight_decay,\n",
    "            ),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        finbert.fit(\n",
    "            x=train_ds,\n",
    "            epochs=train_run.config.epochs,\n",
    "            callbacks=[wandb.keras.WandbCallback()],\n",
    "            validation_data=test_ds,\n",
    "        )\n",
    "\n",
    "    # Evaluate the model\n",
    "    final_loss, final_accuracy = finbert.evaluate(x=test_ds)\n",
    "\n",
    "    # Save the model file with tf.keras\n",
    "    finbert.save(\"finbert.keras\")\n",
    "\n",
    "    return {\n",
    "        \"artifact\": \"finbert.keras\",\n",
    "        \"metadata\": {},\n",
    "        \"metrics\": {\"accuracy\": final_accuracy, \"loss\": final_loss},\n",
    "        \"additional_output_files\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIwiBao85dKa"
   },
   "source": [
    "Now we zip our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWhoigCrc4FH"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "environment_archive = shutil.make_archive(\n",
    "    \"environment\", \"zip\", \".\", \"environment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPpyOHJXcVAB"
   },
   "source": [
    "Let's enable training and create the environment! This needs to be done once in your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5dEtaHHc4FL"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    training_instance.initialize(project_name=PROJECT_NAME)\n",
    "except ubiops.exceptions.ApiException as e:\n",
    "    print(\n",
    "        f\"The training feature may already have been initialized in your project: {e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWBiGLUtdXqq"
   },
   "source": [
    "Let's create a new environment now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t51c0-dGdabz"
   },
   "outputs": [],
   "source": [
    "core_instance.environments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=ubiops.EnvironmentCreate(\n",
    "        name=ENVIRONMENT_NAME,\n",
    "        display_name=ENVIRONMENT_NAME,\n",
    "        base_environment=\"ubuntu22-04-python3-11\",\n",
    "        description=\"Environment with TensorFlow 2.13, wandb and HuggingFace libraries\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5VT2yt-79o6"
   },
   "source": [
    "Finally, we upload our environment archive to UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MGEMYZYhLxs"
   },
   "outputs": [],
   "source": [
    "core_instance.environment_revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    environment_name=ENVIRONMENT_NAME,\n",
    "    file=environment_archive,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM-qoi0lc4FL"
   },
   "source": [
    "Note that building an environment can take long if this is the first time, because all packages from the\n",
    "`requirements.txt` need to be installed inside the environment. This is a one-time process per environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_f5R3F0c4FL"
   },
   "outputs": [],
   "source": [
    "ubiops.utils.wait_for_environment(\n",
    "    core_instance.api_client, PROJECT_NAME, ENVIRONMENT_NAME, timeout=1800, stream_logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr4DlGpyeOjY"
   },
   "source": [
    "Let's create an experiment. Experiments segment different training runs. We select our compute resources to have 8GB of RAM. When we upload a training job, the training code will be run on top of our environment on the selected compute resource. Within this experiment, we can easily try out different training codes, or run the same training code with different hyperparameters. In this example, we will do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NciVGPwzeRNA"
   },
   "outputs": [],
   "source": [
    "experiment = training_instance.experiments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=ubiops.ExperimentCreate(\n",
    "        instance_type_group_name=\"8192 MB + 2 vCPU\",  # Change this to \"16384 MB + 4 vCPU + NVIDIA Tesla T4\" if you want to use GPUs\n",
    "        description=\"FinBERT training experiment runs\",\n",
    "        name=EXPERIMENT_NAME,\n",
    "        environment=ENVIRONMENT_NAME,\n",
    "        default_bucket=\"default\",\n",
    "\n",
    "    ),\n",
    ")\n",
    "ubiops.utils.wait_for_experiment(core_instance.api_client, PROJECT_NAME, EXPERIMENT_NAME, timeout=1800, quiet=False, stream_logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF7aUuMnl_tt"
   },
   "source": [
    "Let's add the WANDB variables as environment variables to our experiment! This way we can connect to Weight and Biases from our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxOBDKsMmGX7"
   },
   "outputs": [],
   "source": [
    "wandb_api_key_environment_variable = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"WANDB_API_KEY\", value=WANDB_API_KEY, secret=True\n",
    ")\n",
    "\n",
    "wandb_project_environment_variable = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"WANDB_PROJECT\", value=WANDB_PROJECT, secret=False\n",
    ")\n",
    "\n",
    "wandb_entity_environment_variable = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"WANDB_ENTITY\", value=WANDB_ENTITY, secret=False\n",
    ")\n",
    "\n",
    "core_instance.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=\"training-base-deployment\",\n",
    "    version=EXPERIMENT_NAME,\n",
    "    data=wandb_api_key_environment_variable,\n",
    ")\n",
    "\n",
    "core_instance.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=\"training-base-deployment\",\n",
    "    version=EXPERIMENT_NAME,\n",
    "    data=wandb_project_environment_variable,\n",
    ")\n",
    "\n",
    "core_instance.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=\"training-base-deployment\",\n",
    "    version=EXPERIMENT_NAME,\n",
    "    data=wandb_entity_environment_variable,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0673RnQc4FL"
   },
   "source": [
    "With everything set up , we can start sending training jobs to our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6kWorNXc4FM"
   },
   "outputs": [],
   "source": [
    "# Define the input parameters\n",
    "data_experiments = [\n",
    "    {\"batch_size\": 32, \"nr_epochs\": 5, \"learning_rate\": 5e-6, \"weight_decay\": 0.01},\n",
    "    {\"batch_size\": 32, \"nr_epochs\": 5, \"learning_rate\": 2e-5, \"weight_decay\": 0.005},\n",
    "    {\"batch_size\": 16, \"nr_epochs\": 5, \"learning_rate\": 2e-5, \"weight_decay\": 0.005},\n",
    "]\n",
    "\n",
    "# Initiate three training runs using the input parameters\n",
    "run_ids = []\n",
    "for index, data_experiment in enumerate(data_experiments):\n",
    "    new_run = training_instance.experiment_runs_create(\n",
    "        project_name=PROJECT_NAME,\n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        data=ubiops.ExperimentRunCreate(\n",
    "            name=f\"training-run-{index}\",\n",
    "            description=f'Trying out a run with {data_experiment [\"nr_epochs\"]} epochs, batch size {data_experiment[\"batch_size\"]}, learning rate {data_experiment[\"learning_rate\"]} and weight decay {data_experiment [\"weight_decay\"]}.',\n",
    "            training_code=\"training_code/train.py\",\n",
    "            parameters=data_experiment\n",
    "        ),\n",
    "        timeout=14400\n",
    "    )\n",
    "    run_ids.append(new_run.id)\n",
    "        \n",
    "for run_id in run_ids:\n",
    "    ubiops.utils.wait_for_experiment_run(core_instance.api_client, PROJECT_NAME, EXPERIMENT_NAME, run_id, timeout=1800, quiet=False, stream_logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjKiwzmTc4FM"
   },
   "source": [
    "We can now head to [wandb.ai](https://wandb.ai/home), go to our project, and monitor our results! We can check that our models run on CPUs, and monitor metrics after each epoch!\n",
    "\n",
    "Using this information, we can select the model with the highest final validation accuracy that we would like to save to the W&B model registry, and deploy to UbiOps.\n",
    "\n",
    "We can also do this in a more automated way, using the W&B API to identify the best model, link it to the W&B model registry, and then deploy it to UbiOps for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrmXJ2Iac4FM"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "wandb_api = wandb.Api()\n",
    "# Download the best model from our best run based on the final val_accuracy\n",
    "best_training_run = wandb_api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\", order=\"-summary_metrics.val_accuracy\")[0].name\n",
    "best_model = wandb_api.artifact(\n",
    "    name=f\"{WANDB_ENTITY}/{WANDB_PROJECT}/model-{best_training_run}:latest\", type=\"model\"\n",
    ")\n",
    "print(f\"This is the best training run: {best_training_run}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvVDJFfHx0cf"
   },
   "source": [
    "Now that we have identified the best performing training run in our experiments, let's log the model from that experiment to the Weights & Biases model registry. We can also give the model version an alias of \"production\", that reflects the phase of the lifecycle the model is in, and can also be used to for automated deployments to our UbiOps inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ved7faHtc4FM"
   },
   "outputs": [],
   "source": [
    "with wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT) as run:\n",
    "    run.link_artifact(best_model, f\"{WANDB_ENTITY}/model-registry/Financial Classifier\", aliases=[\"production\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sMnFH52c4FN"
   },
   "source": [
    "Next we are going to deploy the model and create an inference endpoint on UbiOps . This is called a *Deployment* in UbiOps and contains the following Python code. The Python code is again executed in an environment with the proper dependencies loaded. For this deployment we will use the same environment as before. We use the initialization function of our deployment to grab our latest model from the W&B model registry and to load it in memory. The request function is used to classify a new input text using the three classes *positive*, *negative* and *neutral*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKbDdPgic4FN"
   },
   "outputs": [],
   "source": [
    "!mkdir inference_deployment_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R24Ivis9c4FO"
   },
   "outputs": [],
   "source": [
    "%%writefile inference_deployment_package/deployment.py\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self):\n",
    "        print(\"Initialising deployment\")\n",
    "\n",
    "        wandb_entity = os.getenv(\"WANDB_ENTITY\")\n",
    "        # Download the model version aliased 'production' from the W&B model registry and pass reference to load_model\n",
    "        wandb_api = wandb.Api()\n",
    "        artifact_obj = wandb_api.artifact(f\"{wandb_entity}/model-registry/Financial Classifier:production\")\n",
    "        artifact_path = \"artifact_folder\"\n",
    "        artifact_obj.download(artifact_path)\n",
    "\n",
    "        self.finbert = tf.keras.models.load_model(artifact_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "    def request(self, data):\n",
    "        print(\"Processing request\")\n",
    "\n",
    "        input = self.tokenizer(text=data[\"text\"], return_tensors=\"tf\")\n",
    "\n",
    "        output = self.finbert(input)\n",
    "        output = tf.math.softmax(output[\"logits\"], axis=-1)\n",
    "\n",
    "        prediction = {\n",
    "            \"positive\": float(output[0][0]),\n",
    "            \"negative\": float(output[0][1]),\n",
    "            \"neutral\": float(output[0][2]),\n",
    "        }\n",
    "\n",
    "        # Here we set our output parameters in the form of a json\n",
    "        return {\"prediction\": prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOBIzu3sc4FP"
   },
   "outputs": [],
   "source": [
    "shutil.make_archive(\n",
    "    \"inference_deployment_package\", \"zip\", \".\", \"inference_deployment_package\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-av3Wb-Ic4FP"
   },
   "outputs": [],
   "source": [
    "inference_deployment_template = ubiops.DeploymentCreate(\n",
    "    name=INFERENCING_DEPLOYMENT_NAME,\n",
    "    description=\"A deployment to label stock headlines by financial sentiment.\",\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[{\"name\": \"text\", \"data_type\": \"string\"}],\n",
    "    output_fields=[{\"name\": \"prediction\", \"data_type\": \"dict\"}],\n",
    ")\n",
    "\n",
    "inference_deployment = core_instance.deployments_create(\n",
    "    project_name=PROJECT_NAME, data=inference_deployment_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb57alrv6yuy"
   },
   "source": [
    "We add the WANDB API Token and entity so that our deployment can grab the model from our model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DpQyEjGc4FP"
   },
   "outputs": [],
   "source": [
    "core_instance.deployment_environment_variables_create(\n",
    "    PROJECT_NAME,\n",
    "    INFERENCING_DEPLOYMENT_NAME,\n",
    "    data=wandb_api_key_environment_variable,\n",
    ")\n",
    "\n",
    "core_instance.deployment_environment_variables_create(\n",
    "    PROJECT_NAME,\n",
    "    INFERENCING_DEPLOYMENT_NAME,\n",
    "    data=wandb_entity_environment_variable,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXrzDOoGc4FP"
   },
   "source": [
    "We set up a CPU instance for our inferencing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90VLKWJ4c4FQ"
   },
   "outputs": [],
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=INFERENCING_DEPLOYMENT_VERSION,\n",
    "    environment=ENVIRONMENT_NAME,\n",
    "    instance_type_group_name=\"2048 MB + 0.5 vCPU\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=1800,  # = 30 minutes\n",
    "    request_retention_mode=\"full\",  # input/output of requests will be stored\n",
    "    request_retention_time=3600,  # requests will be stored for 1 hour\n",
    ")\n",
    "\n",
    "version = core_instance.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=INFERENCING_DEPLOYMENT_NAME,\n",
    "    data=version_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9aYj-tS7EDF"
   },
   "source": [
    "Then we upload our code, finalizing the model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2i9U6wOc4FQ"
   },
   "outputs": [],
   "source": [
    "file_upload_result = core_instance.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=INFERENCING_DEPLOYMENT_NAME,\n",
    "    version=INFERENCING_DEPLOYMENT_VERSION,\n",
    "    file=\"inference_deployment_package.zip\",\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_revision(\n",
    "    client=api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=INFERENCING_DEPLOYMENT_NAME,\n",
    "    version=INFERENCING_DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result.revision,\n",
    "    stream_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llaIMYxb7JAb"
   },
   "source": [
    "We can now request our model using its API endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs_NTrhi7MkF"
   },
   "outputs": [],
   "source": [
    "TEXT = \"Stocks rallied and the British pound gained nothing.\"\n",
    "\n",
    "request = core_instance.deployment_version_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=INFERENCING_DEPLOYMENT_NAME,\n",
    "    version=INFERENCING_DEPLOYMENT_VERSION,\n",
    "    data={\"text\": TEXT},\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version_request(\n",
    "    core_instance.api_client, PROJECT_NAME, INFERENCING_DEPLOYMENT_NAME, INFERENCING_DEPLOYMENT_VERSION, request.id,\n",
    "    timeout=1800, quiet=False, stream_logs=True,\n",
    ")\n",
    "\n",
    "print(f\"Predictions are: {request.result[f'prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1LuxgPW694_"
   },
   "source": [
    "So thatâ€™s it! We have used the training insights from Weights & Biases, and the compute resources and deployment possibilities from UbiOps to create a live and scalable model.\n",
    "\n",
    "We can reach our model via its API endpoint, when we provide the correct authentication credentials. After setting up the baseline model, you can easily add new deployment versions and tweak the scaling settings. You can scale down to zero in the development phase, and scale up if you want to be able to run multiple inference jobs in parallel! We can actively monitor when and how often our model was requested using the monitoring tabs.\n",
    "Do you want to try out this workflow for your own training runs, yourself? Feel free to sign up via [ubiops.com](https://app.ubiops.com/sign-up).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c56527485b7c9dd36c8c8808345694ae9d0a642c0f00dfb6356f9a65494ceb50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
