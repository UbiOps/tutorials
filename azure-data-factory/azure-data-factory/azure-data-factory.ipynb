{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Data Factory integration\n",
    "**Note**: This notebook runs on Python 3.9 and uses UbiOps CLient Library 3.15.0.\n",
    "\n",
    "In this notebook we will show you how to create a pipeline that consists of 2 deployments: one that does preprocessing on the input data and the other that uses a KNN classifier to predict whether someone has diabetes.\n",
    "\n",
    "\n",
    "If you run this entire notebook after filling in your access token, the pipeline will be deployed to your UbiOps environment. You can thus check your environment after running the notebook to explore. You can also check the individual steps in this notebook to see what we did exactly and how you can adapt it to your own use case.\n",
    "\n",
    "We recommend to run the cells step by step, as some cells can take a few minutes to finish. You can run everything in one go as well and it will work, just allow a few minutes for building the deployment.\n",
    "\n",
    "## Establishing a connection with your UbiOps environment\n",
    "Add your API token. Then we will provide a project name, deployment name and deployment version name. Afterwards we initialize the client library. This way we can deploy the two pipelines to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = '<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>' # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME= '<INSERT PROJECT NAME IN YOUR ACCOUNT>'\n",
    "# Import all necessary libraries\n",
    "import ubiops\n",
    "import requests\n",
    "\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate local repositories and download the deployment packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pack = requests.get('https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-data-factory/preprocessing_package.zip')\n",
    "pred_pack = requests.get('https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-data-factory/predictor_package.zip')\n",
    "funct = requests.get('https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-data-factory/function.zip')\n",
    "\n",
    "with open('preprocessing_package.zip', 'wb') as f:\n",
    "  f.write(prep_pack.content)\n",
    "\n",
    "with open('predictor_package.zip', 'wb') as f:\n",
    "  f.write(pred_pack.content)\n",
    "\n",
    "with open('functions.zip', 'wb') as f:\n",
    "  f.write(funct.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the preprocessing deployment\n",
    "\n",
    "First of all, we will create the deployment that pre-processes the CSV files, before passing this input to the next deployment consisting of a KNN classifier.\n",
    "\n",
    "The deployment has the following input:\n",
    "- data: a csv file with the training data or with test data\n",
    "- training: a boolean indicating whether we using the data for training or not. In the case this boolean is set to true the target outcome is split of of the training data.\n",
    "\n",
    "The use of the boolean input \"training\" allows us to reuse this block later in a production pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME = 'data-preprocessor'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='Pre-process incoming csv file',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'string'},\n",
    "        {'name':'training', 'data_type':'bool'}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {'name':'cleaned_data', 'data_type':'file'},\n",
    "        {'name':'target_data', 'data_type':'file'}\n",
    "    ],\n",
    "    labels={'demo': 'azure-data-factory'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-9',\n",
    "    instance_type='512mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800, # = 30 minutes\n",
    "    request_retention_mode='full', # input/output of requests will be stored\n",
    "    request_retention_time=3600 # = 1 hour\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "#shutil.make_archive('preprocessing_package', 'zip', '.', 'preprocessing_package')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result1 = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='preprocessing_package.zip'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Deployment created and deployed!\n",
    "\n",
    "Now that the preprocessing deployment has been successfully created and deployed, we can move to the next step.\n",
    "\n",
    "In the next step, we will create the deployment that uses an already trained KNN classifier to predict whether someone has diabetes or not.\n",
    "\n",
    "## Creating the KNN classifier deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name='knn-model',\n",
    "    description='KNN model for diabetes prediction',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'file'},\n",
    "        {'name':'data_cleaning_artefact', 'data_type':'file'}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {'name':'prediction', 'data_type':'file'},\n",
    "        {'name':'predicted_diabetes_instances', 'data_type':'int'}\n",
    "    ],\n",
    "    labels={'demo': 'azure-data-factory'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-9',\n",
    "    instance_type='512mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800, # = 30 minutes\n",
    "    request_retention_mode='full', # input/output of requests will be stored\n",
    "    request_retention_time=3600 # = 1 hour\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "#shutil.make_archive('predictor_package', 'zip', '.', 'predictor_package')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result2 = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='predictor_package.zip'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifier deployment successfully created and deployed!\n",
    "\n",
    "In the next step, we will wait for the `knn-model` and the `preprocessor` to finish building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result1.revision\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result2.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployments passed the building stage\n",
    "\n",
    "If the output of the previous cell is \"available\" for each of the deployments, the deployments have been successfully created, deployed and built.\n",
    "\n",
    "In the next step, we will create a pipeline which uses the preprocessing & KNN deployments that we have created in the previous cells of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"example-pipeline\"\n",
    "\n",
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"A simple pipeline that cleans up data and let's a KNN model predict on it.\",\n",
    "    input_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'string'},\n",
    "        {'name':'training', 'data_type':'bool'}\n",
    "    ],\n",
    "    output_type='structured',\n",
    "    output_fields=[\n",
    "        {'name':'prediction', 'data_type':'file'},\n",
    "        {'name':'predicted_diabetes_instances', 'data_type':'int'}\n",
    "    ],\n",
    "    labels={'demo': 'azure-data-factory'}\n",
    ")\n",
    "\n",
    "api.pipelines_create(project_name=PROJECT_NAME, data=pipeline_template)\n",
    "\n",
    "PIPELINE_VERSION = DEPLOYMENT_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_template = ubiops.PipelineVersionCreate(\n",
    "    version=PIPELINE_VERSION,\n",
    "    request_retention_mode='full',\n",
    "    objects=[\n",
    "        # Preprocessor\n",
    "        {\n",
    "            'name': DEPLOYMENT_NAME,\n",
    "            'reference_name': DEPLOYMENT_NAME,\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        },\n",
    "        # KNN model\n",
    "        {\n",
    "            'name': 'knn-model',\n",
    "            'reference_name': 'knn-model',\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        }\n",
    "    ],\n",
    "    attachments=[\n",
    "        # start --> preprocessor\n",
    "        {\n",
    "            'destination_name': DEPLOYMENT_NAME,\n",
    "            'sources': [{\n",
    "                'source_name': 'pipeline_start',\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'data','destination_field_name': 'data'},\n",
    "                    {\"source_field_name\": 'training','destination_field_name': 'training'}\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        # preprocessor --> KNN model\n",
    "        {\n",
    "            'destination_name': 'knn-model',\n",
    "            'sources': [{\n",
    "                'source_name': DEPLOYMENT_NAME,\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'cleaned_data','destination_field_name': 'data'},\n",
    "                    {\"source_field_name\": 'target_data','destination_field_name': 'data_cleaning_artefact'}\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        # KNN model --> pipeline end\n",
    "        {\n",
    "            'destination_name': 'pipeline_end',\n",
    "            'sources': [{\n",
    "                'source_name': 'knn-model',\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'prediction','destination_field_name': 'prediction'},\n",
    "                    {\"source_field_name\": 'predicted_diabetes_instances','destination_field_name': 'predicted_diabetes_instances'}\n",
    "                ]\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "api.pipeline_versions_create(project_name=PROJECT_NAME, pipeline_name=PIPELINE_NAME, data=pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done! Let's close the client properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: If you get an error like: \"error\":\"Version is not available: The version is currently in the building stage\"\n",
    "Your deployment is not yet available and still building. \n",
    "Check in the UI if your deployment is ready and then rerun the block above.\n",
    "\n",
    "# Pipeline successfuly created!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request and exploring further\n",
    "You can go ahead to the Web App and take a look in the user interface at what you have just built. If you want you can create a request to the pipeline using data from the [\"dummy_data_for_predicting.csv\"](https://storage.googleapis.com/ubiops/data/Integration%20with%20cloud%20provider%20tools/azure-data-factory/dummy_data_for_predicting.csv) and setting the \"training\" input to \"False\". The dummy data is just the diabetes data without the Outcome column. \n",
    "\n",
    "So there we have it! We have made a pipeline in UbiOps that can be connected to Azure Data Factory. Be sure to run the rest of the steps of this tutorial as mentioned in the README, to explore how to set up the integration with Azure Data Factory. \n",
    "\n",
    "For any questions, feel free to reach out to us via the customer service portal: https://ubiops.atlassian.net/servicedesk/customer/portals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
