{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Falcon 1B \n",
    "\n",
    "Note: This notebook runs on Python 3.11 and uses UbiOps CLient Library 4.1.0.\n",
    "\n",
    "This notebook shows how you can fine-tune the Falcon 1B model from Huggingface on \n",
    "[English quotes](https://huggingface.co/datasets/Abirate/english_quotes) using the UbiOps \n",
    "[training functionallity](https://ubiops.com/docs/training/). In order to fine-tune the model we'll need to create an \n",
    "experiment which defines the training set up, in this experiment we willll then iniate training runs which are the \n",
    "actual code executions. In the case of this notebook the code inside these training runs will be executed on the dataset \n",
    "specified earlier. For this guide you will also need to have initialized the training functionallity inside your project, \n",
    "which can be done by going to the **Training** page and clicking **Enable training**. \n",
    "\n",
    "Note that the aim of this guide is to show you ***how*** you can fine-tune Falcon, as such Falcon will not be fine-tuned \n",
    "for a specific benchmark.\n",
    "\n",
    "The fine-tuning will be done in four steps:\n",
    "1. Connecting with the UbiOps API client\n",
    "2. Create the environment for training experiment\n",
    "3. Create the training experiment\n",
    "4. Initialize two training runs (more explanation will follow below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Connecting with the UbiOps API client\n",
    "\n",
    "To use the UbiOps API from our notebook, we need to install the UbiOps Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ubiops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up a connection with the UbiOps platform API we need the name of your UbiOps project and an API token with `project-editor` permissions.\n",
    "\n",
    "Once you have your project name and API token, paste them below in the following cell before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ubiops\n",
    "\n",
    "API_TOKEN = '<API TOKEN>' # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = '<PROJECT_NAME>'    # Fill in your project name here\n",
    "\n",
    "configuration = ubiops.Configuration()\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "api_client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.api.CoreApi(api_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the environment for training experiment\n",
    "\n",
    "An environment on UbiOps is built up out of a `base environment`, which for this tutorial will be `ubuntu22-04-python3-11-cuda11-7-1` \n",
    "to which we can add aditional dependencies. The packages\n",
    "that will be used inside the deployment will be defined in a `requirements.txt`. For this guide a package from `git` is also\n",
    "required, [which can be done](https://ubiops.com/docs/howto/howto-load-from-git/) by creating a `ubiops.yaml`, the `ubiops.yaml` can be used for packages that need to be downloaded\n",
    "on OS-level. These files will be added to a directory, zipped, and then uploaded to UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir fine-tuning-environment-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Create the `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fine-tuning-environment-files/requirements.txt\n",
    "ubiops\n",
    "joblib\n",
    "torch\n",
    "scipy                      \n",
    "bitsandbytes\n",
    "git+https://github.com/huggingface/transformers.git\n",
    "git+https://github.com/huggingface/peft.git\n",
    "git+https://github.com/huggingface/accelerate.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Create the `ubiops.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fine-tuning-environment-files/ubiops.yaml\n",
    "apt:\n",
    "  packages:\n",
    "    - git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to zip the fine-tuning-environment-files directory, and define the coding environment so we can upload it to\n",
    "UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_name = \"fine-tuning-environment-files\"\n",
    "ENVIRONMENT_NAME = \"fine-tuning-falcon1b\"\n",
    "shutil.make_archive(zip_name, \"zip\", \"fine-tuning-environment-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment and upload the zip file to UbiOps\n",
    "data = ubiops.EnvironmentCreate(\n",
    "    name=ENVIRONMENT_NAME, base_environment=\"ubuntu22-04-python3-11-cuda11-7-1\"\n",
    ")\n",
    "\n",
    "api_response = api.environments_create(PROJECT_NAME, data)\n",
    "print(api_response)\n",
    "api_response = api.environment_revisions_file_upload(\n",
    "    PROJECT_NAME, ENVIRONMENT_NAME, file=f\"{zip_name}.zip\"\n",
    ")\n",
    "print(api_response)\n",
    "\n",
    "# Wait for the environment to finish building\n",
    "ubiops.utils.wait_for_environment(\n",
    "    client=api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    environment_name=ENVIRONMENT_NAME,\n",
    "    timeout=1800,\n",
    "    stream_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Create the training experiment & environment variables\n",
    "\n",
    "Now that the environment has been created we can start defining the training set-up, i.e., the `experiment`. Here we define\n",
    "the environment, the instance type and the storage location (bucket) for the training runs. Defining the instance type and environment \n",
    "on experiment level makes it possible to apply techniques like [hyper-parameter tuning](https://ubiops.com/docs/ubiops_tutorials/xgboost-training/xgboost-training/).\n",
    "If you do not have access to GPU, you will need to change the instance type below to `16384mb_t4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from ubiops.training.training import Training\n",
    "\n",
    "\n",
    "training_instance = Training(api_client)\n",
    "\n",
    "# Create experiment\n",
    "EXPERIMENT_NAME = f\"falcon-fine-tuning-{datetime.now().date()}\"\n",
    "\n",
    "api_response = training_instance.experiments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=ubiops.ExperimentCreate(\n",
    "        name=EXPERIMENT_NAME,\n",
    "        instance_type_group_name=\"16384 MB + 4 vCPU + NVIDIA Tesla T4\",  # You can use '16384 MB + 4 vCPU' if you run on CPU\n",
    "        description=\"A finetuning experiment for Falcon\",\n",
    "        environment=ENVIRONMENT_NAME,\n",
    "        default_bucket=\"default\",\n",
    "        labels={\"type\": \"pytorch\", \"model\": \"flaconLLM\"},\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Wait for the experiment to finish building\n",
    "ubiops.utils.wait_for_experiment(\n",
    "    client=api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    timeout=1800,\n",
    "    stream_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an environment variable so we can access the *default* bucket. The results of the model will be stored inside this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment variable for the api token\n",
    "envvar_projname = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"API_TOKEN\", value=API_TOKEN, secret=True\n",
    ")\n",
    "api.deployment_version_environment_variables_create(\n",
    "    PROJECT_NAME,\n",
    "    deployment_name=\"training-base-deployment\",\n",
    "    version=EXPERIMENT_NAME,\n",
    "    data=envvar_projname,\n",
    ")\n",
    "\n",
    "# Create an environment variable for the project name\n",
    "envvar_projname = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"PROJECT_NAME\", value=PROJECT_NAME, secret=False\n",
    ")\n",
    "api.deployment_version_environment_variables_create(\n",
    "    PROJECT_NAME,\n",
    "    deployment_name=\"training-base-deployment\",\n",
    "    version=EXPERIMENT_NAME,\n",
    "    data=envvar_projname,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Set up the training runs\n",
    "\n",
    "The training runs are the actual code executions on a specific dataset. For each run you can configure different training \n",
    "code (in the form of a `train.py`), training data and different parameters.\n",
    "\n",
    "For this experiment we will initiate two training runs, which are the actual code executions:\n",
    "- A preparation run (`prepare.py`) in which will the models' checkpoints and dataset will be downloaded, after which \n",
    "they will be stored inside the (*default*) UbiOps bucket which we defined when we created the experiment.\n",
    "- A training run (`train.py`) in which these files will be downloaded, and then used to fine-tune the Falcon 1B model. The\n",
    " model will be stored in the same bucket as the models' weights and the dataset from the preparation run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a) Create and initialize the `prepare.py`\n",
    "\n",
    "First we create the `prepare.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prepare.py\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tarfile\n",
    "import ubiops\n",
    "import requests\n",
    "import os \n",
    "\n",
    "def train(training_data, parameters, context = {}):\n",
    "\n",
    "    configuration = ubiops.Configuration(api_key={'Authorization': os.environ[\"API_TOKEN\"]})\n",
    "    api_client = ubiops.ApiClient(configuration)\n",
    "    api = ubiops.api.CoreApi(api_client)\n",
    "    \n",
    "    # Load model weights\n",
    "    print(\"Load model weights\")\n",
    "    model_id = \"tiiuae/falcon-rw-1b\"\n",
    "    cache_dir = \"checkpoint\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir)\n",
    "    with tarfile.open(f'{cache_dir}.tar', 'w') as tar: \n",
    "         tar.add(f\"./{cache_dir}/\")\n",
    "         \n",
    "    # Uploading weights\n",
    "    print('Uploading weights')\n",
    "    file_uri = ubiops.utils.upload_file( \n",
    "      client=api_client, \n",
    "      project_name=os.environ[\"PROJECT_NAME\"], \n",
    "      file_path=f'{cache_dir}.tar', \n",
    "      bucket_name=\"default\", \n",
    "      file_name=f'{cache_dir}.tar'\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Load dataset\")\n",
    "    ds = \"quotes.jsonl\"\n",
    "    r = requests.get(\"https://huggingface.co/datasets/Abirate/english_quotes/resolve/main/quotes.jsonl\")\n",
    "    with open(ds, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        \n",
    "    # Uploading dataset\n",
    "    file_uri = ubiops.utils.upload_file( \n",
    "      client=api_client, \n",
    "      project_name=os.environ[\"PROJECT_NAME\"], \n",
    "      file_path=ds, \n",
    "      bucket_name=\"default\", \n",
    "      file_name=ds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the a training run which will execute the code inside the `prepare.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training_instance.experiment_runs_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    data=ubiops.ExperimentRunCreate(\n",
    "        name=\"Load\",\n",
    "        description=\"Load model\",\n",
    "        training_code=\"./prepare.py\",\n",
    "        parameters={}\n",
    "    ),\n",
    "    timeout=14400\n",
    ")\n",
    "\n",
    "# Wait for the prepare.py run to complete\n",
    "ubiops.utils.wait_for_experiment_run(\n",
    "    client=api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_id=run.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b) Create and intialize the `train.py`\n",
    "\n",
    "As with the `prepare.py` we first define the code for the `train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import ubiops\n",
    "import os\n",
    "import tarfile\n",
    "import json\n",
    "import joblib\n",
    "from typing import List\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from ubiops import utils\n",
    "\n",
    "class QuotesDataset(Dataset):\n",
    "    def __init__(self, data: List[dict], tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        return self.tokenizer(self.data[idx][\"quote\"])\n",
    "\n",
    "def train(training_data, parameters, context = {}):\n",
    "\n",
    "    configuration = ubiops.Configuration(api_key={'Authorization': os.environ[\"API_TOKEN\"]})\n",
    "    api_client = ubiops.ApiClient(configuration)\n",
    "    api = ubiops.api.CoreApi(api_client)       \n",
    "  # First step is to load model weights and and dataset of english quotes into deployment\n",
    "    for f in [\"checkpoint.tar\",\"quotes.jsonl\"]: \n",
    "        file_uri = ubiops.utils.download_file(\n",
    "          client= api_client, #a UbiOps API client, \n",
    "          file_name=f,\n",
    "          project_name=os.environ[\"PROJECT_NAME\"],\n",
    "          output_path=\".\",\n",
    "          bucket_name=\"default\"\n",
    "        )\n",
    "\n",
    "    with tarfile.open(\"checkpoint.tar\", 'r') as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    \n",
    "    # This config allow to represent model in a lower percision. It means every weight in it is going to take 4bits instead 32bit. So we will use ~ 8 times less vram.\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model_id=\"tiiuae/falcon-rw-1b\"\n",
    "    cache_dir = \"checkpoint\"\n",
    "\n",
    "    # Loading model weights and allocating them according to config\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_dir, quantization_config=nf4_config)\n",
    "  \n",
    "    # Also enabling checkpointing, a technique that allows us to save memory by recomputing some nodes multiple times.\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Lora is another technique that allows to save memory. However this time by reducing absolute number of trainable parameters. It also defines a task for our fine tuning as CAUSAL_LM, it means llm will learn to perdict next word based on previous words in a quote.\n",
    "    config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query_key_value\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    lines = list()\n",
    "    with open(\"quotes.jsonl\", 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    \n",
    "    dataset = QuotesDataset(lines, tokenizer)\n",
    "\n",
    "    # Run trainer from the transformers library.\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=10,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\"\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    model.config.use_cache = False  # Silence the warnings. Please re-enable for inference!\n",
    "    finetuned_model = trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(finetuned_model, \"finetuned_falcon.pkl\")\n",
    "\n",
    "    file_uri = ubiops.utils.upload_file( \n",
    "      client=api_client, \n",
    "      project_name=os.environ[\"PROJECT_NAME\"], \n",
    "      file_path=f'{cache_dir}.tar', \n",
    "      bucket_name=\"default\", \n",
    "      file_name=f'{cache_dir}.tar'\n",
    "    )\n",
    "    \n",
    "    return {\"artifact\": \"finetuned_falcon.pkl\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize a training run which will execute the code inside the `train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train_run = training_instance.experiment_runs_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    data=ubiops.ExperimentRunCreate(\n",
    "        name=\"Train\",\n",
    "        description=\"training run\",\n",
    "        training_code=\"./train.py\"\n",
    "    ),\n",
    "    timeout=14400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there you have it!\n",
    "\n",
    "We have just fine-tuned the Falcon 1B model from Huggingface on the `quotes.json` dataset. The model can be accessed by \n",
    "going to the **Train** run inside the **falcon-fine-tuning** experiment we created and clicking on the link from the \n",
    "**Output artefact**. From there you can download the model by clicking on the **Download** button.\n",
    "\n",
    "You can also copy the `file_uri` by clicking on the **copy** button. The `file_uri` can then be used to import the model \n",
    "inside a deployment, by using something like the code snippet below:\n",
    "```\n",
    "ubiops.utils.download_file(api_client,\n",
    "                        PROJECT_NAME,\n",
    "                        file_uri=file_uri,\n",
    "                        output_path='checkpoint.tar'\n",
    "                        )\n",
    "```\n",
    "\n",
    "More information about using files in a deployment can be found in [our documentation](https://ubiops.com/docs/input-output/). We also provide several Howto's that explain how you can use files inside a deployment, these can be found on the bottom of the Storage documentation page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
