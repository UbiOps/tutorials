{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonR-Python Deployment\n",
    "\n",
    "\n",
    "Note: This notebook runs on Python 3.8 and uses UbiOps Client Library 3.15.0\n",
    "\n",
    "In this notebook we will show you the following:\n",
    "\n",
    "How to create a pipeline that prepares the data in R and makes a prediction of house prices using an XGboost model.\n",
    "\n",
    "If you run the R script and this notebook according to the order described below, the DEA-deployment and XGB-deployment will be deployed to your UbiOps environment. You can thus check your environment after running to explore. You can also check the individual steps in these notebooks to see what we did exactly and how you can adapt it to your own use case.\n",
    "\n",
    "<b>IMPORTANT</b> due to the fact that this notebook creates a pipeline that contains both R and Python deployments, it is required to run the notebooks in the following order:\n",
    "1) Run this notebook up untill and including step 5\n",
    "\n",
    "2) Run the R script\n",
    "\n",
    "3) Run step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  1. Establishing a connection with your UbiOps environmentÂ¶\n",
    "\n",
    "Add your API token and your project name. We provide a deployment name and deployment version name. Afterwards we initialize the client library. This way we can deploy the XGBoost model to your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>\" # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<INSERT YOUR PROJECT NAME HERE>\"\n",
    "\n",
    "\n",
    "DEPLOYMENT_NAME = \"python-pred-deployment\"\n",
    "DEPLOYMENT_VERSION = \"v1\"\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops\n",
    "\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={\"Authorization\": API_TOKEN},\n",
    "                                               host=\"https://api.ubiops.com/v2.1\"))\n",
    "api = ubiops.CoreApi(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating the model\n",
    "\n",
    "This example will contain a simple linear regression with XGboost, where the data exploration has been done in R.\n",
    "\n",
    "Since this document will be focused on the deploying side of the ML process. We will not cover the development of the model in-depth and make use of the pre-trained model below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==0.24.2\n",
    "!pip install xgboost==1.3.1\n",
    "!pip install numpy==1.19.5\n",
    "!pip install pandas==1.1.5\n",
    "!pip install joblib==1.0.0\n",
    "!pip install scipy==1.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import joblib\n",
    "\n",
    "# Read the data into a dataframe \n",
    "data= pd.read_csv(\"kc_house_data.csv\")\n",
    "\n",
    "# Train a simple linear regression model\n",
    "regr = linear_model.LinearRegression()\n",
    "new_data = data[[\"sqft_living\",\"grade\", \"sqft_above\", \"sqft_living15\",\"bathrooms\",\"view\",\"sqft_basement\",\"lat\",\"waterfront\",\"yr_built\",\"bedrooms\"]]\n",
    "\n",
    "X = new_data.values\n",
    "y = data.price.values\n",
    "\n",
    "# Create train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Let\"s make a XGboost prediction model\n",
    "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "\n",
    "traindf, testdf = train_test_split(X_train, test_size = 0.2)\n",
    "\n",
    "# Train the model\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions using the xgboost model\n",
    "predictions = xgb.predict(X_test)\n",
    "\n",
    "\n",
    "# Check how the xgboost model scores on accuracy on our test set\n",
    "xgboost_score = explained_variance_score(predictions,y_test)\n",
    "\n",
    "print(f\"Score of the xgboost model {xgboost_score}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb, \"xgb-deployment/xgb-deployment.joblib\")\n",
    "print(\"XGBoost model built and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating the XGboost deployment\n",
    "Now that we have our model saved it is time to create a deployment in UbiOps so we can use it in a pipeline later.\n",
    "\n",
    "In the cell below you can view the deployment.py which will take data about the house we wish to predict the price of.\n",
    "\n",
    "As you can see in the initialization step we load the model we created earlier, then in the request method we make use of it to make a prediction. Input to this model is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file containing the deployment code is required to be called \"deployment.py\" and should contain the \"Deployment\"\n",
    "class and \"request\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import load\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - environment (str): the environment in which the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising xgboost model\")\n",
    "\n",
    "        XGBOOST_MODEL = os.path.join(base_directory, \"xgb-deployment.joblib\")\n",
    "        self.model = load(XGBOOST_MODEL)\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "        print('Loading data')\n",
    "        input_data = pd.read_csv(data['clean_data'])\n",
    "        \n",
    "        print(\"Prediction being made\")\n",
    "        prediction = self.model.predict(input_data.values)\n",
    "        \n",
    "        # Writing the prediction to a csv for further use\n",
    "        print('Writing prediction to csv')\n",
    "        pd.DataFrame(prediction).to_csv('prediction.csv', header = ['house_prices'], index_label= 'index')\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": 'prediction.csv'\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deploying to UbiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description=\"python-pred-deployment\",\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name=\"clean_data\",\n",
    "            data_type=\"file\",\n",
    "        ),\n",
    "    ],\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name=\"prediction\",\n",
    "            data_type=\"file\"\n",
    "        ),\n",
    "    ],\n",
    "    labels={\"demo\": \"python-pred-deployment\"}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment=\"python3-8\",\n",
    "    instance_type='512mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800 # = 30 minutes\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive(\"xgb-deployment\", \"zip\", \".\", \"xgb-deployment\")\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result =api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=\"xgb-deployment.zip\"\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please open up your R IDE and run the script. When the R deployment is done and available, continue to step 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating the prediction pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"pythonr-pipeline\"\n",
    "\n",
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"\",\n",
    "    input_type=\"structured\",\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name=\"input\",\n",
    "            data_type=\"file\",\n",
    "        )\n",
    "    ],\n",
    "    output_type=\"structured\",\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name=\"prediction\",\n",
    "            data_type=\"file\"\n",
    "        )\n",
    "    ],\n",
    "    labels={\"demo\": \"PythonR-pipeline\"}\n",
    ")\n",
    "\n",
    "api.pipelines_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=pipeline_template\n",
    ")\n",
    "\n",
    "PIPELINE_VERSION = DEPLOYMENT_VERSION\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a pipeline we can create a version with the components in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_template = ubiops.PipelineVersionCreate(\n",
    "    version=PIPELINE_VERSION,\n",
    "    request_retention_mode='full',\n",
    "    objects=[\n",
    "        # python-pred-deployment\n",
    "        {\n",
    "            'name': DEPLOYMENT_NAME,\n",
    "            'reference_name': DEPLOYMENT_NAME,\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        },\n",
    "        # r-eda-deployment\n",
    "        {\n",
    "            'name': \"r-eda-deployment\",\n",
    "            'reference_name': \"r-eda-deployment\",\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        }\n",
    "    ],\n",
    "    attachments=[\n",
    "        # Start -> r-eda-deployment\n",
    "        {\n",
    "            'destination_name': \"r-eda-deployment\",\n",
    "            'sources': [{\n",
    "                'source_name': 'pipeline_start',\n",
    "                'mapping': [{\n",
    "                    \"source_field_name\": \"input\",\n",
    "                    \"destination_field_name\": \"raw_data\"\n",
    "                }]\n",
    "            }]\n",
    "        },\n",
    "        # r-eda-deployment -> python-pred-deployment\n",
    "        {\n",
    "            'destination_name': DEPLOYMENT_NAME,\n",
    "            'sources': [{\n",
    "                'source_name': \"r-eda-deployment\",\n",
    "                'mapping': [{\n",
    "                    \"source_field_name\": \"clean_data\",\n",
    "                    \"destination_field_name\": \"clean_data\"\n",
    "                }]\n",
    "            }]\n",
    "        },\n",
    "        # python-pred-deployment -> pipeline end\n",
    "        {\n",
    "            'destination_name': 'pipeline_end',\n",
    "            'sources': [{\n",
    "                'source_name': DEPLOYMENT_NAME,\n",
    "                'mapping': [{\n",
    "                    \"source_field_name\": \"prediction\",\n",
    "                    \"destination_field_name\": \"prediction\"\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "api.pipeline_versions_create(project_name=PROJECT_NAME, pipeline_name=PIPELINE_NAME, data=pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done! Let's close the client properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request and exploring further\n",
    "Now you can go to the Web App and take a look in the user interface at what you have just built, it should look something like this:\n",
    "\n",
    "![pythonr_overview](pythonr_overview.png)\n",
    "\n",
    "\n",
    " If you want you can create a request to the XGboost deployment using the \"dummy_data.csv\". The dummy data is a small test subset from the original data.\n",
    "\n",
    "So there we have it! You have created a pipeline where one the data exploration deployment is written in R and the prediction model is written in Python. These two notebooks can be used as an example of how to make a pipeline that combines R and Python. Just change the code in the deployment packages and alter the input and output fields to your wish and you should be good to go.\n",
    "\n",
    "For any questions, feel free to reach out to us via the customer service portal:\n",
    "https://ubiops.atlassian.net/servicedesk/customer/portals\n",
    "\n",
    "Or connect with using our Slack channel:\n",
    "https://ubiops-community.slack.com/join/shared_invite/zt-np02blts-5xyFK0azBOuhJzdRSYwM_w#/shared-invite/email"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
