{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4evWVHgDz4cS"
   },
   "source": [
    "<img src=\"https://ubiops.com/wp-content/uploads/2020/12/Group-2.svg\" title=\"UbiOps Logo\" width=100px/>\n",
    "\n",
    "# Training a Tensorflow model on UbiOps\n",
    "\n",
    "**This tutorial is part of a blogpost.**\n",
    "\n",
    "**In this notebook, we will show how to run a training job for a Tensorflow model on the UbiOps platform.**\n",
    "\n",
    "We will define and create a UbiOps training script. Using the UbiOps Python client we will configure the environment in which this script can be run, and an experiment which is used to analyse and track our results\n",
    "\n",
    "You can try it yourself by using a valid UbiOps API token and project name in the cell below.\n",
    "\n",
    "##### **About the training code**\n",
    "The training function we will deploy expects a path to a zipped `training data` file,  `the number of epochs`, and the `batch_size` as input. As output it will give the trained `model artifact` as well as the final `loss` and `accuracy` for the training job.\n",
    "- The training code and data is based on one of the Tensorflow tutorials for training a model on the 'flowers dataset'. Source: https://www.tensorflow.org/tutorials/load_data/images\n",
    "- The corresponding URL for the training data archive is: https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
    "\n",
    "| Training code in- and output variables |        |                            |\n",
    "|---------------------------------------|--------|----------------------------|\n",
    "|                                       | **Fields (type)**                  | **Keys of dictionary**     |\n",
    "| **Input fields**                      | training_data (file) |                            |\n",
    "|                                       | parameters (dict) | {epochs (*data_type=* as integer), batch_size  (*data_type=* integer)} |\n",
    "| **Output fields**                     | artifact (file) |                  |\n",
    "|                                       | metrics (dict) | {accuracy (*data_type=* float), loss (*data_type=* float), loss_history (*data_type=* list[float]), acc_history (*data_type=* list[float])}           |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ENmvHTJ9thkK"
   },
   "source": [
    "To interface with UbiOps through code we need the UbiOps Python client library. In the following cell it will be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqyjLCWrtgwU"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade ubiops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-OL16bYmz4cX"
   },
   "source": [
    "***\n",
    "# 1) Set project variables and initialize the UbiOps API Client\n",
    "First, make sure you create an **API token** with `project editor` permissions in your UbiOps project and paste it below. Also fill in your corresponding UbiOps project name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxwQqaLXz4ca"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = datetime.now()\n",
    "import yaml\n",
    "import os\n",
    "import ubiops\n",
    "\n",
    "API_TOKEN = \"Token \"  # Paste your API token here. Don't forget the `Token` prefix\n",
    "PROJECT_NAME = \"\"  # Fill in the corresponding UbiOps project name\n",
    "\n",
    "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "api_client = ubiops.ApiClient(configuration)\n",
    "core_instance = ubiops.CoreApi(api_client=api_client)\n",
    "training_instance = ubiops.Training(api_client=api_client)\n",
    "print(core_instance.service_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate the training functionallity in your UbiOps project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jfcOalcZubvK"
   },
   "source": [
    "Set-up a training instance in case you have not done this yet in your project. This action will create a base training deployment, that is used to host training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0aJAuTbua-u"
   },
   "outputs": [],
   "source": [
    "training_instance = ubiops.Training(api_client=api_client)\n",
    "try:\n",
    "    training_instance.initialize(project_name=PROJECT_NAME)\n",
    "except ubiops.exceptions.ApiException as e:\n",
    "    print(\n",
    "        f\"The training feature may already have been initialized in your project:\\n{e}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "20YqLAk7t2Zh"
   },
   "source": [
    "## Defining the code environment\n",
    "\n",
    "Our training code needs an environment to run in, with a specific Python language version, and some dependencies, like `Tensorflow`. You can create and manage environments in your UbiOps project. \n",
    "We create an environment named 'python3-11-tensorflow-training', select Python 3.11 and upload a `requirements.txt` which contains the relevant dependencies.\n",
    "\n",
    "The environment can be  reused and updated for different training jobs (and deployments!). The details  of the environment are visible in the 'environments' tab in the UbiOps UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pL1CV-uit9Dk"
   },
   "outputs": [],
   "source": [
    "training_environment_dir = \"training_environment\"\n",
    "ENVIRONMENT_NAME = \"python3-11-tensorflow-training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zOzmP_VuJdk"
   },
   "outputs": [],
   "source": [
    "%mkdir {training_environment_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwuXmiUHuKka"
   },
   "outputs": [],
   "source": [
    "%%writefile {training_environment_dir}/requirements.txt\n",
    "numpy==1.24.1\n",
    "tensorflow==2.10.0\n",
    "joblib==1.2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "s6VcKzc8uOrf"
   },
   "source": [
    "Now zip the environment like you would zip a deployment package, and create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTvXoN2XuQNc"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "training_environment_archive = shutil.make_archive(\n",
    "    f\"{training_environment_dir}\", \"zip\", \".\", f\"{training_environment_dir}\"\n",
    ")\n",
    "\n",
    "# Create experiment. Your environment is set-up in this step. It may take some time to run.\n",
    "\n",
    "try:\n",
    "    api_response = core_instance.environments_create(\n",
    "        project_name=PROJECT_NAME,\n",
    "        data=ubiops.EnvironmentCreate(\n",
    "            name=ENVIRONMENT_NAME,\n",
    "            # display_name=ENVIRONMENT_NAME,\n",
    "            base_environment=\"python3-11\",\n",
    "            description=\"Test training environment with tensorflow 2.10 and some helper functions\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    core_instance.environment_revisions_file_upload(\n",
    "        project_name=PROJECT_NAME,\n",
    "        environment_name=ENVIRONMENT_NAME,\n",
    "        file=training_environment_archive,\n",
    "    )\n",
    "except ubiops.exceptions.ApiException as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hkf5AQ0vuT5t"
   },
   "source": [
    "## Configure an experiment\n",
    "The basis for model training in UbiOps is an 'Experiment'. An experiment has a fixed code environment and hardware (instance) definition, but it can hold many different 'Runs'.\n",
    "\n",
    "You can create an experiment in the WebApp or use the client library, as we're here.\n",
    "\n",
    "This bucket will be used to store your training jobs and model callbacks. In case you want to continue without creating a bucket, you can use the `default` bucket, that is always present inside your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFo4UggAuXho"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"training-experiment-demo\"  # str\n",
    "BUCKET_NAME = \"default\"\n",
    "\n",
    "try:\n",
    "    experiment = training_instance.experiments_create(\n",
    "        project_name=PROJECT_NAME,\n",
    "        data=ubiops.ExperimentCreate(\n",
    "            instance_type_group_name=\"4096 MB + 1 vCPU\",\n",
    "            description=\"Train test experiment\",\n",
    "            name=EXPERIMENT_NAME,\n",
    "            environment=ENVIRONMENT_NAME,\n",
    "            default_bucket=BUCKET_NAME,\n",
    "        ),\n",
    "    )\n",
    "except ubiops.exceptions.ApiException as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DLjEyVtBDcqL"
   },
   "source": [
    "## Load the training data\n",
    "\n",
    "We will download the publicly available `flower photos` dataset. We will our model on this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYV3ZVztDphU"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "training_data = \"flower_photos.tgz\"\n",
    "\n",
    "urllib.request.urlretrieve(url, training_data)\n",
    "\n",
    "print(f\"File downloaded successfully to '{training_data}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the dataset by untarring the tarfile, this step is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pquRqxxzFU53"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "file_dir = \"flower_photos\"\n",
    "with tarfile.open(training_data, \"r:gz\") as tar:\n",
    "    path = tar.extractall(\"./\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rZh8uE1ewM-C"
   },
   "source": [
    "## Define and start a training run\n",
    "\n",
    "A training job in UbiOps is called a run. To run any Python code on UbiOps, we need to create a file named `train.py` and include our training code here. This code will execute as a single 'Run' as part of an 'Experiment' and uses the code environment and instance type (hardware) as defined with the experiment as shown before.\n",
    "Let’s take a look at the training script. The UbiOps `train.py` structure is quite simple. It only requires a train() function, with input parameters `training_data` (a file path to your training data) and `parameters`(a dictionary that contains parameters of your choice).  If we upload this training code, along with the `training_data` file and some values for our input parameters, a training run is initiated! You can run different training runs in parallel, with different scripts or different hyperparameters. An example of this set up can be\n",
    "found in the [XGBoost training tutorial](https://ubiops.com/docs/ubiops_tutorials/xgboost-training/xgboost-training/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFw2QNf0wOwL"
   },
   "outputs": [],
   "source": [
    "RUN_NAME = \"training-run\"\n",
    "RUN_SCRIPT = f\"{RUN_NAME}.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4Gbpw_NwQT6"
   },
   "outputs": [],
   "source": [
    "%%writefile {RUN_SCRIPT}\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import pathlib\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "def train(training_data, parameters, context = {}):\n",
    "    '''All code inside this function will run when a call to the deployment is made.'''\n",
    "\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    batch_size = int(parameters['batch_size']) #Specify the batch size\n",
    "    nr_epochs = int(parameters['nr_epochs']) #Specify the number of epochs\n",
    "  \n",
    "\n",
    "    # Load the training data\n",
    "    extract_dir = \"flower_photos\"\n",
    "\n",
    "    with tarfile.open(training_data, 'r:gz') as tar:\n",
    "      tar.extractall(\"./\")\n",
    "\n",
    "    data_dir = pathlib.Path(extract_dir)\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    print(class_names)\n",
    "\n",
    "\n",
    "    # Standardize the data\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    image_batch, labels_batch = next(iter(normalized_ds))\n",
    "\n",
    "    # Configure the dataset for performance\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    num_classes = 5\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "    ]) \n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=nr_epochs    \n",
    "    )\n",
    "    \n",
    "    eval_res = model.evaluate(val_ds)\n",
    "    \n",
    "    \n",
    "    # Return the trained model file and metrics\n",
    "    joblib.dump(model, 'model.pkl')\n",
    "    fin_loss = eval_res[0]\n",
    "    fin_acc = eval_res[1]\n",
    "    \n",
    "    print(history)\n",
    "    print(history.history)\n",
    "    return {\n",
    "        \"artifact\": 'model.pkl',\n",
    "        \"metrics\": {'fin_loss' : fin_loss,\n",
    "                    'fin_acc' : fin_acc,\n",
    "                    \"loss_history\": history.history[\"loss\"],\n",
    "                    \"acc_history\" : history.history[\"accuracy\"]},\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZeOd1ogzW9B"
   },
   "outputs": [],
   "source": [
    "new_run = training_instance.experiment_runs_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    data=ubiops.ExperimentRunCreate(\n",
    "        name=RUN_NAME,\n",
    "        description=\"Trying out a first run run\",\n",
    "        training_code=RUN_SCRIPT,\n",
    "        training_data=training_data,\n",
    "        parameters={\"nr_epochs\": 2, \"batch_size\": 32},  # example parameters\n",
    "    ),\n",
    "    timeout=14400,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JmK1jCD9VQXH"
   },
   "source": [
    "We can easily finetune our training code and execute a new training code, and analyse the logs along the way.\n",
    "When training a model it is important to keep track of the training progress and convergence. We do this by looking at the training loss and accuracy metrics. Packages like Tensorflow will print these for you continuously, and we’re able to track them in the logging page of the UbiOps UI.\n",
    "If you notice a training job is not converging, you’re able to cancel the request and try it again with different data or different parameters.\n",
    "\n",
    "Additionaly you can create custom metrics in UbiOps, you can find more information about that [here](https://ubiops.com/docs/monitoring/metrics/#custom-metrics)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "norMPKSdU6Ji"
   },
   "source": [
    "## Evaluating the output\n",
    "When the training runs are completed, the training run will provide you with the trained parameter file, the final accuracy and loss. The parameter file is stored inside a UbiOps bucket. You can easily navigate to this location from the training-run interface.\n",
    "You can compare metrics of different training runs easily inside the Evaluation page of the Training tab, allowing you to analyze which code or which hyperparameters worked best."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "G32dUVokzhQo"
   },
   "source": [
    "And that’s it, you just trained a Tensorflow model on UbiOps!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
