{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit template\n",
    "**Note**: This notebook runs on Python 3.9 and uses UbiOps CLient Library 3.15.0.\n",
    "\n",
    "In this notebook we will show you the following:\n",
    "\n",
    "- How to make a training pipeline in UbiOps which preprocesses the data and trains and tests a model using scikit\n",
    "- How to make a production pipeline in UbiOps which takes in new data, processes it and feeds it to a trained model for prediction/classification\n",
    "\n",
    "For this example we will use a diabetes dataset from Kaggle to create a KNN classifier to predict if someone will have diabetes or not. [Link to original dataset](https://kaggle.com/uciml/pima-indians-diabetes-database).\n",
    "\n",
    "If you run this entire notebook after filling in your access token, the two pipelines and all the necessary models will be deployed to your UbiOps environment. You can thus check your environment after running to explore. You can also check the individual steps in this notebook to see what we did exactly and how you can adapt it to your own use case.\n",
    "\n",
    "We recommend to run the cells step by step, as some cells can take a few minutes to finish. You can run everything in one go as well and it will work, just allow a few minutes for building the individual deployments.\n",
    "\n",
    "## Establishing a connection with your UbiOps environment\n",
    "Add your API token. Then we will provide a project name, deployment name and deployment version name. Afterwards we connect to the UbiOps API, which allows us to create deployments and pipelines in our project. This way we can deploy the two pipelines to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"<INSERT YOUR TOKEN HERE>\" # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<INSERT PROJECT NAME>\"\n",
    "DEPLOYMENT_NAME = 'data-preprocessor'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops\n",
    "import requests\n",
    "\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a training pipeline\n",
    "\n",
    "Our training pipeline will consist of two steps: preprocessing the data, and training a model.\n",
    "For each of these two steps we will create a separate deployment in UbiOps. This way the processing step can be reused later in the deployment pipeline (or in other pipelines) and each block will be scaled separately, increasing speed.\n",
    "\n",
    "### Preprocessing the data\n",
    "In the cell below the deployment.py of the preprocessing block is loaded. In the request function you can see that the deployment will clean up the data for further use and output that back in the form of two csv files. \n",
    "The deployment has the following input:\n",
    "- data: a csv file with the training data or with real data\n",
    "- training: a boolean indicating whether we using the data for training or not. In the case this boolean is set to true the target outcome is split of of the training data.\n",
    "\n",
    "The use of the boolean input \"training\" allows us to reuse this block later in a production pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We initiate three empty directories that we fill with our deployment codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"preprocessing_package\")\n",
    "os.mkdir(\"predictor_package\")\n",
    "os.mkdir(\"training_package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing_package/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising My Deployment\")\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Processing request for My Deployment\")\n",
    "        #Load the dataset\n",
    "        print(\"Loading data\")\n",
    "        diabetes_data = pd.read_csv(data[\"data\"])\n",
    "        # The data contains some zero values which make no sense (like 0 skin thickness or 0 BMI). \n",
    "        # The following columns/variables have invalid zero values:\n",
    "        # glucosem bloodPressure, SkinThicknes, Insulin and BMI\n",
    "        # We will replace these zeros with NaN and after that we will replace them with a suitable value.\n",
    "        \n",
    "        print(\"Imputing missing values\")\n",
    "        # Replacing with NaN\n",
    "        diabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n",
    "        # Imputing NaN\n",
    "        diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace = True)\n",
    "        diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace = True)\n",
    "        diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace = True)\n",
    "        diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace = True)\n",
    "        diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace = True)\n",
    "        \n",
    "        # If this deployment is used for training, the target column\n",
    "        # needs to be split from the data\n",
    "        if data[\"training\"] == True:\n",
    "            X = diabetes_data.drop([\"Outcome\"], axis = 1) \n",
    "            y = diabetes_data.Outcome\n",
    "        else:\n",
    "            X = diabetes_data\n",
    "            y = pd.DataFrame([1])\n",
    "            \n",
    "            \n",
    "        print(\"Scaling data\")\n",
    "        # Since we are using a distance metric based algorithm we will use scikits standard scaler to scale all the features to [-1,1]\n",
    "        sc_X = StandardScaler()\n",
    "        X =  pd.DataFrame(sc_X.fit_transform(X,),\n",
    "                columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "               'BMI', 'DiabetesPedigreeFunction', 'Age'])\n",
    "        \n",
    "        # UbiOps expects JSON serializable output or files, so we convert the dataframes to csv\n",
    "        X.to_csv('X.csv', index = False)\n",
    "        y.to_csv('y.csv', index = False, header = False)\n",
    "\n",
    "        return {\n",
    "            \"cleaned_data\": 'X.csv', \"target_data\": 'y.csv'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing_package/requirements.txt\n",
    "\n",
    "pandas==1.3.5\n",
    "numpy==1.21.5\n",
    "scikit-learn==1.0.2\n",
    "scipy==1.7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a deployment and a deployment version for the package in the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='Clean up data',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'file'},\n",
    "        {'name':'training', 'data_type':'bool'}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {'name':'cleaned_data', 'data_type':'file'},\n",
    "        {'name':'target_data', 'data_type':'file'}\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-9',\n",
    "    instance_type='512mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800, # = 30 minutes\n",
    "    request_retention_mode='none' # we don't need request storage in this example\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive('preprocessing_package', 'zip', '.', 'preprocessing_package')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result1 = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='preprocessing_package.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model will now have been deployed to your UbiOps environment. Go ahead and take a look in the UI in the tab deployments to see it for yourself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "Now that we have the preprocessing deployment in UbiOps, we need a deployment that can take the output of the preprocessing step and train a KNN model on it. The code for this is in the \"training_package\" directory and can be seen in the next cell. We are going to perform the same steps as above to deploy this code in UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_package/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising My Deployment\")\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Processing request for My Deployment\")\n",
    "        # Load the dataset\n",
    "        print(\"Loading data\")\n",
    "        \n",
    "        X = pd.read_csv(data[\"cleaned_data\"])\n",
    "        y = pd.read_csv(data[\"target_data\"], header = None)\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=42, stratify=y)\n",
    "\n",
    "        \n",
    "        # Setup a knn classifier with k neighbors\n",
    "        knn = KNeighborsClassifier(n_neighbors=7) \n",
    "        \n",
    "        # Fit the model on training data\n",
    "        knn.fit(X_train,y_train)\n",
    "        \n",
    "        # Get accuracy on test set. Note: In case of classification algorithms score method represents accuracy.\n",
    "        score = knn.score(X_test,y_test)\n",
    "        print('KNN accuracy: ' + str(score))\n",
    "        \n",
    "        # let us get the predictions using the classifier we had fit above\n",
    "        y_pred = knn.predict(X_test)\n",
    "                \n",
    "        # Output classification report\n",
    "        print('Classification report:')\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        \n",
    "        # Persisting the model for use in UbiOps\n",
    "        with open('knn.joblib', 'wb') as f:\n",
    "           dump(knn, 'knn.joblib')\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"trained_model\": 'knn.joblib', \"model_score\": score\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_package/requirements.txt\n",
    "\n",
    "pandas==1.3.5\n",
    "numpy==1.21.5\n",
    "scikit-learn==1.0.2\n",
    "scipy==1.7.3\n",
    "joblib==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to deploy this step to UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template_t = ubiops.DeploymentCreate(\n",
    "    name='model-training',\n",
    "    description='Trains a KNN model',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'cleaned_data', 'data_type':'file'},\n",
    "        {'name':'target_data', 'data_type':'file'}\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {'name':'trained_model', 'data_type':'file'},\n",
    "        {'name':'model_score', 'data_type':'double'}\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template_t\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-9',\n",
    "    instance_type='512mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800, # = 30 minutes\n",
    "    request_retention_mode='none' # we don't need request storage in this example\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='model-training',\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive('training_package', 'zip', '.', 'training_package')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result2 = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='model-training',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='training_package.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if both deployments, preprocessing and training, are available for further use. We can only use the models inside a pipeline after they have been built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result1.revision\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='model-training',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result2.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a training pipeline\n",
    "\n",
    "So right now we have two deployments: one cleaning up the input data and one using that data for training a model. We want to tie these two blocks together to create a workflow. We can use pipelines for that. Let's create a pipeline that takes the same input as the preprocessing block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_name = \"training-pipeline\"\n",
    "\n",
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=training_pipeline_name,\n",
    "    description='A simple pipeline that cleans up data and trains a KNN model on it.',\n",
    "    input_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'file'},\n",
    "        {'name':'training', 'data_type':'bool'}\n",
    "    ],\n",
    "    output_type='structured',\n",
    "    output_fields=[\n",
    "        {'name':'trained_model', 'data_type':'file'},\n",
    "        {'name':'model_score', 'data_type':'double'}\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.pipelines_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=pipeline_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pipeline, now we just need to create a version and add our deployments to it.\n",
    "\n",
    "**IMPORTANT**: If you get an error like: \"error\":\"Version is not available: The version is currently in the building stage\"\n",
    "Your model is not yet available and still building. \n",
    "Check in the UI if your model is ready and then rerun the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_version = \"v1\"\n",
    "\n",
    "pipeline_template = ubiops.PipelineVersionCreate(\n",
    "    version=training_pipeline_version,\n",
    "    request_retention_mode='full',\n",
    "    objects=[\n",
    "        # preprocessor\n",
    "        {\n",
    "            'name': DEPLOYMENT_NAME,\n",
    "            'reference_name': DEPLOYMENT_NAME,\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        },\n",
    "        # model-training\n",
    "        {\n",
    "            'name': 'model-training',\n",
    "            'reference_name': 'model-training',\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        }\n",
    "    ],\n",
    "    attachments=[\n",
    "        # start --> preprocessor\n",
    "        {\n",
    "            'destination_name': DEPLOYMENT_NAME,\n",
    "            'sources': [{\n",
    "                'source_name': 'pipeline_start',\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'data','destination_field_name': 'data'},\n",
    "                    {\"source_field_name\": 'training','destination_field_name': 'training'}\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        # preprocessor --> model-training\n",
    "        {\n",
    "            'destination_name': 'model-training',\n",
    "            'sources': [{\n",
    "                'source_name': DEPLOYMENT_NAME,\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'cleaned_data','destination_field_name': 'cleaned_data'},\n",
    "                    {\"source_field_name\": 'target_data','destination_field_name': 'target_data'}\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        # model-training -> pipeline end\n",
    "        {\n",
    "            'destination_name': 'pipeline_end',\n",
    "            'sources': [{\n",
    "                'source_name': 'model-training',\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'trained_model','destination_field_name': 'trained_model'},\n",
    "                    {\"source_field_name\": 'model_score','destination_field_name': 'model_score'}\n",
    "                ]\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "api.pipeline_versions_create(project_name=PROJECT_NAME, pipeline_name=training_pipeline_name, data=pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline done!\n",
    "If you check in your UbiOps account under pipeline you will find a training-pipeline with our components in it and connected. Let's make a request to it. You can also make a request in the UI with the \"create direct request button\".\n",
    "\n",
    "This might take a while since the models will need a cold start as they have never been used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_name = \"training-pipeline\"\n",
    "\n",
    "csv = requests.get('https://storage.googleapis.com/ubiops/data/Deploying%20with%20popular%20DS%20libraries/sci-kit-deployment/diabetes.csv')\n",
    "\n",
    "with open(\"diabetes.csv\", \"wb\") as f:\n",
    "    f.write(csv.content)\n",
    "\n",
    "file_uri = ubiops.utils.upload_file(client, PROJECT_NAME, 'diabetes.csv')\n",
    "\n",
    "data = {'data': file_uri, 'training': True}\n",
    "pipeline_result = api.pipeline_version_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    pipeline_name=training_pipeline_name,\n",
    "    version=training_pipeline_version,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "print(pipeline_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with the trained model\n",
    "\n",
    "Our model is trained and ready. Now we still need to deploy a predictor to UbiOps that uses this model for predicting. \n",
    "\n",
    "I already have the code and the requirements ready that need to be deployed to UbiOps. However, the joblib file is still missing in this folder. We dont want to manually download the joblib file output from the training pipeline, but automatically put it in the deployment package for the predictor. After that we can zip up the folder and push it to UbiOps like we did with the previous two packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predictor_package/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import load\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising KNN model\")\n",
    "\n",
    "        KNN_MODEL = os.path.join(base_directory, \"knn.joblib\")\n",
    "        self.model = load(KNN_MODEL)\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "        \"\"\"\n",
    "        print('Loading data')\n",
    "        input_data = pd.read_csv(data['data'])\n",
    "        \n",
    "        print(\"Prediction being made\")\n",
    "        prediction = self.model.predict(input_data)\n",
    "        diabetes_instances = int(sum(prediction))\n",
    "        \n",
    "        # Writing the prediction to a csv for further use\n",
    "        print('Writing prediction to csv')\n",
    "        pd.DataFrame(prediction).to_csv('prediction.csv', header = ['diabetes_prediction'], index_label= 'index')\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": 'prediction.csv', \"predicted_diabetes_instances\": diabetes_instances\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predictor_package/requirements.txt\n",
    "\n",
    "pandas==1.3.5\n",
    "numpy==1.21.5\n",
    "scikit-learn==1.0.2\n",
    "scipy==1.7.3\n",
    "joblib==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to download the trained model joblib and put it in the predictor package directory\n",
    "base_directory = os.path.dirname(os.path.abspath(\"scikit-deployment\"))\n",
    "output_path = os.path.join(base_directory, \"predictor_package\")\n",
    "\n",
    "ubiops.utils.download_file(client, PROJECT_NAME, file_name='knn.joblib', output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to zip the deployment package\n",
    "shutil.make_archive('predictor_package', 'zip', '.', 'predictor_package')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the KNN model\n",
    "The folder is ready, now we need to make a deployment in UbiOps. Just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name='knn-model',\n",
    "    description='KNN model for diabetes prediction',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'file'},\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {'name':'prediction', 'data_type':'file'},\n",
    "        {'name':'predicted_diabetes_instances', 'data_type':'int'}\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-9',\n",
    "    instance_type='512mb',\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800, # = 30 minutes\n",
    "    request_retention_mode='none' # we don't need request storage in this example\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='predictor_package.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the deployment is ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name='knn-model',\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the production pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_pipeline_name = \"production-pipeline\"\n",
    "\n",
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=prod_pipeline_name,\n",
    "    description=\"A simple pipeline that cleans up data and let's a KNN model predict on it.\",\n",
    "    input_type='structured',\n",
    "    input_fields=[\n",
    "        {'name':'data', 'data_type':'file'},\n",
    "        {'name':'training', 'data_type':'bool'}\n",
    "    ],\n",
    "    output_type='structured',\n",
    "    output_fields=[\n",
    "        {'name':'prediction', 'data_type':'file'},\n",
    "        {'name':'predicted_diabetes_instances', 'data_type':'int'}\n",
    "    ],\n",
    "    labels={'demo': 'scikit-deployment'}\n",
    ")\n",
    "\n",
    "api.pipelines_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=pipeline_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pipeline, now we just need to create a version and add our deployments to it.\n",
    "\n",
    "**IMPORTANT**: If you get an error like: \"error\":\"Version is not available: The version is currently in the building stage\"\n",
    "Your model is not yet available and still building. \n",
    "Check in the UI if your model is ready and then rerun the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_pipeline_version = DEPLOYMENT_VERSION\n",
    "\n",
    "pipeline_template = ubiops.PipelineVersionCreate(\n",
    "    version=prod_pipeline_name,\n",
    "    request_retention_mode='none',\n",
    "    objects=[\n",
    "        # Preprocessor\n",
    "        {\n",
    "            'name': DEPLOYMENT_NAME,\n",
    "            'reference_name': DEPLOYMENT_NAME,\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        },\n",
    "        # KNN model\n",
    "        {\n",
    "            'name': 'knn-model',\n",
    "            'reference_name': 'knn-model',\n",
    "            'version': DEPLOYMENT_VERSION\n",
    "        }\n",
    "    ],\n",
    "    attachments=[\n",
    "        # start --> preprocessor\n",
    "        {\n",
    "            'destination_name': DEPLOYMENT_NAME,\n",
    "            'sources': [{\n",
    "                'source_name': 'pipeline_start',\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'data','destination_field_name': 'data'},\n",
    "                    {\"source_field_name\": 'training','destination_field_name': 'training'}\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        # preprocessor --> KNN model\n",
    "        {\n",
    "            'destination_name': 'knn-model',\n",
    "            'sources': [{\n",
    "                'source_name': DEPLOYMENT_NAME,\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'cleaned_data','destination_field_name': 'data'},\n",
    "                ]\n",
    "            }]\n",
    "        },\n",
    "        # KNN model --> pipeline end\n",
    "        {\n",
    "            'destination_name': 'pipeline_end',\n",
    "            'sources': [{\n",
    "                'source_name': 'knn-model',\n",
    "                'mapping': [\n",
    "                    {\"source_field_name\": 'prediction','destination_field_name': 'prediction'},\n",
    "                    {\"source_field_name\": 'predicted_diabetes_instances','destination_field_name': 'predicted_diabetes_instances'}\n",
    "                ]\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "api.pipeline_versions_create(project_name=PROJECT_NAME, pipeline_name=prod_pipeline_name, data=pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done! Let's close the client properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a request and exploring further\n",
    "You can go ahead to the Web App and take a look in the user interface at what you have just built. If you want you can create a request to the production pipeline using the [\"dummy_data_for_predicting.csv\"](https://storage.googleapis.com/ubiops/data/Deploying%20with%20popular%20DS%20libraries/sci-kit-deployment/dummy_data_for_predicting.csv) and setting the \"training\" input to \"False\". The dummy data is just the diabetes data with the Outcome column chopped of. \n",
    "\n",
    "So there we have it! We have made a training pipeline and a production pipeline using the scikit learn library. You can use this notebook to base your own pipelines on. Just adapt the code in the deployment packages and alter the input and output fields as you wish and you should be good to go. \n",
    "\n",
    "For any questions, feel free to reach out to us via the [customer service portal](https://ubiops.atlassian.net/servicedesk/customer/portals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
