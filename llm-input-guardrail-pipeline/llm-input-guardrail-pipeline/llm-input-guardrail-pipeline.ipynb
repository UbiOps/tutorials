{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement custom input gaurdrails on UbiOps\n",
    "\n",
    "\n",
    "This notebook shows an example on how to implement input custom guardrails into a UbiOps pipeline. Input guardrails are mechanisms \n",
    "designed to filter and validate user input before it reaches the LLM. The input request can then either be blocked, or adjusted,\n",
    "to steer the LLM response into a certain direction.\n",
    "\n",
    "Different input guardrail mechanisms exist. You can use a high-throughput LLm to classify a response, or simply use regex.\n",
    "As an example, we will guide you through a simple regex guardrail example.\n",
    "\n",
    "We will implement a pipeline that connects two deployments. One deployment applies the regex filter. The second deployment\n",
    "proxies a request to an LLM. The return of the LLM will be streamed.\n",
    "\n",
    "The pipeline can be called with OpenAI-compatible input bodies.  In the next release of UbiOps, the pipeline will be exposed\n",
    "via an [OpenAI compatible chat/completions endpoint](https://ubiops.com/docs/requests/openai/#openai-compatible-requests).\n",
    "\n",
    "This solution can be set up in your UbiOps environment in four steps:\n",
    "1) Establish a connection with your UbiOps environment\n",
    "2) Create the deployment for the input guardrailing\n",
    "3) Create the deployment for the proxy LLM, using connection strings\n",
    "4) Create a pipeline that combines the two deployments created in step 2 and 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Connecting with the UbiOps API client\n",
    "\n",
    "To use the UbiOps API from our notebook, we need to install the UbiOps Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade ubiops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up a connection with the UbiOps platform API we need the name of your UbiOps project and an API token with `project-editor` permissions.\n",
    "\n",
    "Once you have your project name and API token, paste them below in the following cell before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ubiops\n",
    "\n",
    "API_TOKEN = \"<UBIOPS_API_TOKEN>\"  # Used to create the deployments and pipeline, make sure this is in the format \"Token token-code\"\n",
    "API_HOST_URL = \"<API_HOST_URL>\"  # Standard UbiOps API URL is 'https://api.ubiops.com/v2.1', your URL may differ depending on your environment\n",
    "PROJECT_NAME = \"<PROJECT_NAME>\"  # Fill in your project name here\n",
    "\n",
    "configuration = ubiops.Configuration()\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "configuration.host = API_HOST_URL\n",
    "\n",
    "api_client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.api.CoreApi(api_client)\n",
    "\n",
    "\n",
    "print(api.service_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to be able to send requests to an LLM that accepts [OpenAI compatible chat completion requests](https://ubiops.com/docs/requests/openai/#openai-compatible-requests). You can use an external supplier, such as OpenAI or Mistral, or configure such an LLM on UbiOps yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"<BASE_URL>\" # The base URL of your LLM. If hosted on UbiOps, use f\"{API_HOST_URL}/projects/{PROJECT_NAME}/openai-compatible/v1/\"\n",
    "MODEL_NAME = \"<MODEL_NAME>\" # The name of your LLM model. If hosted on UbiOps, use  f\"ubiops-deployment/{DEPLOYMENT_NAME}//<the name of your model>\"\n",
    "API_KEY = \"<API_KEY>\" # Used to create requests within the proxy deployment. If hosted on UbiOps, use a valid API Token with atleast `deployment-request-user` permissions to request the deployment, but without the 'Token ' prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the deployments for the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have established a connection with our UbiOps environment, we can start creating our deployment packages. Each\n",
    "package will consist of two files:\n",
    "- The `deployment.py`, which is where we will define the actual code to run the embedding model and LLM\n",
    "- The `requirements.txt`, which will contain additional dependencies that our codes needs to run properly\n",
    "\n",
    "These deployment packages will be zipped, and uploaded to UbiOps, after which we will add them to a pipeline. The pipeline\n",
    "will consist out of two deployments:\n",
    "- One deployment will host the embedding model\n",
    "- One will host the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUARDRAIL_DEPLOYMENT_NAME = \"filter-apple-guardrail\"\n",
    "GUARDRAIL_DEPLOYMENT_PACKAGE_DIR = \"input_guardrail_deployment_package\"\n",
    "PROXY_LLM_DEPLOYMENT_NAME = \"proxy-llm\"\n",
    "PROXY_LLM_DEPLOYMENT_PACKAGE_DIR = \"proxy_llm_deployment_package\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Create the Input guardrail deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This deployment adds a simple input guardrail before messages reach the main LLM. It checks if the user mentions the word \"apple\" and, if so, inserts a system message instructing them to talk about other fruits instead. It also validates that the input is properly formatted JSON with a \"messages\" list. If not, a public error is returned to the end-user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir {GUARDRAIL_DEPLOYMENT_PACKAGE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the `deployment.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {GUARDRAIL_DEPLOYMENT_PACKAGE_DIR}/deployment.py\n",
    "import re\n",
    "import json\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self):\n",
    "        self.guard = SimpleWordChecker()\n",
    "\n",
    "    def request(self, data: str) -> str:\n",
    "        # Load the OpenAI-Compatible input body\n",
    "        try:\n",
    "            parsed = json.loads(data)\n",
    "        except json.JSONDecodeError:\n",
    "            raise PublicError(\"Invalid JSON: Could not parse request body.\")\n",
    "\n",
    "        # Validate required structure\n",
    "        if \"messages\" not in parsed or not isinstance(parsed[\"messages\"], list):\n",
    "            raise PublicError(\"Invalid input: 'messages' key must be present and must be a list.\")\n",
    "\n",
    "        updated = self.guard.check_for_apple(parsed)\n",
    "\n",
    "        # Dump the response back as a string\n",
    "        return json.dumps(updated)\n",
    "\n",
    "\n",
    "class SimpleWordChecker:\n",
    "    def check_for_apple(self, body):\n",
    "        '''\n",
    "        Checks if the last user message contains the word \"apple\", \n",
    "        if so, adds a new system message for the LLM.\n",
    "\n",
    "        Returns the body of messages\n",
    "        '''\n",
    "\n",
    "        messages = body[\"messages\"]\n",
    "        for i in reversed(range(len(messages))):\n",
    "            if messages[i].get(\"role\") == \"user\":\n",
    "\n",
    "                # Find the last user message and check for the forbidden word\n",
    "                if re.search(r\"\\bapple\\b\", messages[i].get(\"content\", \"\"), re.IGNORECASE):\n",
    "                    messages.insert(i + 1, {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"The user used the word apple. Using the word apple is forbidden. Instruct the user to talk about other fruits instead..\"\n",
    "                    })\n",
    "                # We only need to check the last user's message, so stop after this\n",
    "                break\n",
    "\n",
    "        return body\n",
    "\n",
    "class PublicError(Exception):\n",
    "    '''\n",
    "    Raise a public error message to the user \n",
    "    which is visible from the request overview page in UbiOps.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, public_error_message):\n",
    "        super().__init__()\n",
    "        self.public_error_message = public_error_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we create the deployment \n",
    "\n",
    "For the deployment we will specify the in- and output for the model as type `plain`, to support OpenAI-compatible input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=GUARDRAIL_DEPLOYMENT_NAME,\n",
    "    description=\"An example deployment that checks if a user used the world apple and instructs the LLM to make the end-user\" \\\n",
    "    \"aware of this.\",\n",
    "    input_type=\"plain\",\n",
    "    output_type=\"plain\",\n",
    "    labels={\"type\": \"input-guardrail\"},\n",
    ")\n",
    "\n",
    "guardrail_deployment = api.deployments_create(\n",
    "    project_name=PROJECT_NAME, data=deployment_template\n",
    ")\n",
    "print(guardrail_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And finally we create the version\n",
    "\n",
    "Each deployment can have multiple versions. The version of a deployment defines the coding environment, instance type (CPU or GPU) \n",
    "& size, and other settings. We will set `minimum_instances` to warrant fast response time . The code is simple and will not consume a lot of resources. Therefore we select the smallest instance type group available.\n",
    "\n",
    "⚠️ **Warning:** toggle `minimum_instances` to `0` after this tutorial to save up on resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=\"v1\",\n",
    "    environment=\"python3-13\",\n",
    "    instance_type_group_name=\"256 MB + 0.0625 vCPU\",\n",
    "    minimum_instances=1,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=10,\n",
    "    instance_processes = 1,\n",
    "    request_retention_mode=\"full\",  # Input/output of requests will be stored.\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=GUARDRAIL_DEPLOYMENT_NAME, data=version_template\n",
    ")\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we zip the `deployment package` and upload it to UbiOps (this process can take between 5-10 minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(GUARDRAIL_DEPLOYMENT_PACKAGE_DIR, \"zip\", \".\", GUARDRAIL_DEPLOYMENT_PACKAGE_DIR)\n",
    "\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=GUARDRAIL_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    file=f\"{GUARDRAIL_DEPLOYMENT_PACKAGE_DIR}.zip\",\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=GUARDRAIL_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    revision_id=file_upload_result.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create the proxy LLM deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the deployment that will proxy a request to an external LLM by hitting its `v1/chat/completions` endpoint. The deployment functions simply as a passthrough, although\n",
    "you're free to add custom logic. The workflow for creating this deployment is similar to the workflow for creating the previous deployment, except we now add the `openai` python package to the environment, and use environment variables to specify the LLM to which we will apply requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir {PROXY_LLM_DEPLOYMENT_PACKAGE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `deployment.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PROXY_LLM_DEPLOYMENT_PACKAGE_DIR}/deployment.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self, base_directory, context):\n",
    "        print(\"Initializing OpenAI-compatible Deployment\")\n",
    "\n",
    "        try:\n",
    "            self.base_url = os.environ[\"BASE_URL\"]\n",
    "            self.model_name = os.environ[\"MODEL_NAME\"]\n",
    "            self.api_key = os.environ[\"API_KEY\"]  # You might want to add this or similar\n",
    "        except KeyError as e:\n",
    "            raise Exception(f\"Missing required environment variable: {e}\")\n",
    "\n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url\n",
    "        )\n",
    "\n",
    "        self.context = context\n",
    "\n",
    "    def request(self, data, context):\n",
    "        print(\"Processing request for OpenAI-compatible Deployment\")\n",
    "\n",
    "        try:\n",
    "            input_data = json.loads(data)\n",
    "        except (TypeError, ValueError):\n",
    "            raise PublicError(\"Invalid JSON: Could not parse request body.\")\n",
    "\n",
    "        input_data[\"model\"] = self.model_name\n",
    "\n",
    "        # Optional: Include usage info for tracking tokens \n",
    "        is_streaming = input_data.get(\"stream\", False)\n",
    "        if is_streaming:\n",
    "            input_data[\"stream_options\"] = {\"include_usage\": True}\n",
    "\n",
    "        try:\n",
    "            print(f\"Sending request to model {input_data['model']}\")\n",
    "            response = self.client.chat.completions.create(**input_data)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Failed to call model\") from e\n",
    "\n",
    "        if is_streaming:\n",
    "            streaming_callback = context[\"streaming_update\"]\n",
    "            full_response = []\n",
    "            for partial_response in response:\n",
    "                chunk_dump = partial_response.model_dump()\n",
    "                streaming_callback(json.dumps(chunk_dump))\n",
    "                full_response.append(chunk_dump)\n",
    "            return json.dumps(full_response)\n",
    "        else:\n",
    "            full_response = response.model_dump()\n",
    "            return json.dumps(full_response)\n",
    "\n",
    "\n",
    "class PublicError(Exception):\n",
    "    def __init__(self, public_error_message):\n",
    "        super().__init__()\n",
    "        self.public_error_message = public_error_message\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the `requirements.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PROXY_LLM_DEPLOYMENT_PACKAGE_DIR}/requirements.txt\n",
    "openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a deployment\n",
    "\n",
    "Again, we will use input and output types `plain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_template = ubiops.DeploymentCreate(\n",
    "    name=PROXY_LLM_DEPLOYMENT_NAME,\n",
    "    description=\"A deployment that proxies requests to an OpenAI-compatible server\",\n",
    "    input_type=\"plain\",\n",
    "    output_type=\"plain\",\n",
    "    labels={\"type\": \"llm-proxy\"},\n",
    ")\n",
    "\n",
    "llm_deployment = api.deployments_create(project_name=PROJECT_NAME, data=llm_template)\n",
    "print(llm_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a version for the deployment. We will use a slightly larger instance type to ensure that the llm proxy can\n",
    "handle multiple requests concurrently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=\"v1\",\n",
    "    environment=\"python3-13\",\n",
    "    instance_type_group_name=\"512 MB + 0.125 vCPU\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=1,\n",
    "    maximum_idle_time=10, \n",
    "    instance_processes=5,\n",
    "    request_retention_mode=\"full\",  # input/output of requests will be stored)\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=PROXY_LLM_DEPLOYMENT_NAME, data=version_template\n",
    ")\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create environment variables that allow the proxy deployment to request an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=PROXY_LLM_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"BASE_URL\", value=BASE_URL, secret=False),\n",
    ")\n",
    "\n",
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=PROXY_LLM_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"MODEL_NAME\", value=MODEL_NAME, secret=False),\n",
    ")\n",
    "\n",
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=PROXY_LLM_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"API_KEY\", value=API_KEY, secret=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip & upload the files to UbiOps (this process can take between 5-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(PROXY_LLM_DEPLOYMENT_PACKAGE_DIR, \"zip\", \".\", PROXY_LLM_DEPLOYMENT_PACKAGE_DIR)\n",
    "\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=PROXY_LLM_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    file=f\"{PROXY_LLM_DEPLOYMENT_PACKAGE_DIR}.zip\",\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=PROXY_LLM_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    revision_id=file_upload_result.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Create a pipeline and pipeline version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a pipeline that orchestrates the workflow between the deployments above. When a request will be made to this pipeline\n",
    "the first deployment will check the last user's prompt for forbidden words. Then it passes the messages through to the LLM to generate an answer.\n",
    "\n",
    "For a pipeline you will have to define an input & output and create a version, as with a deployment. In addition to this we\n",
    "will also need to define the objects (i.e, deployments) and how to orchestrate the workflow (i.e., how to attach each object\n",
    " to each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"guardrail-pipeline-demo\"\n",
    "PIPELINE_VERSION = \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"A pipeline that applies an input guardrail\",\n",
    "    input_type=\"plain\",\n",
    "    output_type=\"plain\"\n",
    ")\n",
    "\n",
    "api.pipelines_create(project_name=PROJECT_NAME, data=pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the objects, and how to attach the objects together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two objects to be used in the pipeline\n",
    "\n",
    "objects = [\n",
    "    # input-guardrail\n",
    "    {\n",
    "        \"name\": GUARDRAIL_DEPLOYMENT_NAME,\n",
    "        \"reference_name\": GUARDRAIL_DEPLOYMENT_NAME,\n",
    "        \"version\": \"v1\",\n",
    "    },\n",
    "    # LLM-model\n",
    "    {\n",
    "        \"name\": PROXY_LLM_DEPLOYMENT_NAME, \n",
    "        \"reference_name\": PROXY_LLM_DEPLOYMENT_NAME, \n",
    "        \"version\": \"v1\"\n",
    "     },\n",
    "]\n",
    "\n",
    "attachments = [\n",
    "    # start --> input-guardrail\n",
    "    {\n",
    "        \"destination_name\": GUARDRAIL_DEPLOYMENT_NAME,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": \"pipeline_start\",\n",
    "                \"mapping\": [],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # input-guardrail --> LLM\n",
    "    {\n",
    "        \"destination_name\": PROXY_LLM_DEPLOYMENT_NAME,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": GUARDRAIL_DEPLOYMENT_NAME,\n",
    "                \"mapping\": []\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # LLM --> pipeline end\n",
    "    {\n",
    "        \"destination_name\": \"pipeline_end\",\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": PROXY_LLM_DEPLOYMENT_NAME,\n",
    "                \"mapping\": [],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we create a version for this pipeline. Note that we are adding labels, so that the solutions will be returned\n",
    "when using the `/models` [endpoint](https://ubiops.com/docs/requests/openai/#listing-available-models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_template = ubiops.PipelineVersionCreate(\n",
    "    version=PIPELINE_VERSION,\n",
    "    request_retention_mode=\"full\",\n",
    "    objects=objects,\n",
    "    attachments=attachments,\n",
    "    labels = {\"openai-model-names\":\"llm-apple-input-guardrail\", \"openai-compatible\":True}\n",
    ")\n",
    "\n",
    "api.pipeline_versions_create(\n",
    "    project_name=PROJECT_NAME, pipeline_name=PIPELINE_NAME, data=pipeline_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there you have it!\n",
    "\n",
    "We have now set up input guardrails on UbiOps. If you want, you can use the code block below to create a request to your newly created pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =\"\"\"\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"I ate an apple!\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": false\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api.pipeline_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textual response of the LLM reads as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "json.loads(response.result)[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example response would be:\n",
    "\n",
    "\"\"\"I see you mentioned a certain fruit that starts with \"A\". Let's try something different. How about we talk about bananas, oranges, or grapes instead? Which one of those fruits do you like?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also initiate requests via the `openai` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= API_TOKEN[6:] if API_TOKEN.startswith(\"Token \") else API_TOKEN,\n",
    "    base_url = f\"{API_HOST_URL}/projects/{PROJECT_NAME}/openai-compatible/v1\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then fetch all models available within your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    **json.loads(data)\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "At last, let's close our connection to UbiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial just serves as an example. Feel free to reach out to our [support portal](https://www.support.ubiops.com) if you want to discuss your set-up in more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
