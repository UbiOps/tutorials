{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0670d8",
   "metadata": {},
   "source": [
    "# Deploy a Multi-Model OpenAI-Compatible Ollama Inference Server on UbiOps\n",
    "\n",
    "In this tutorial, we will explain how to run multiple models supported by Ollama on UbiOps, including both chat completion models and embedding models within a single deployment. Developers distribute Ollama by publishing a custom `install.sh` [script](https://ollama.com/download). This script allows creating custom Docker images with Ollama by running `install.sh` in a `Dockerfile`. We will create a custom environment based on a UbiOps base environment (so that it [supports the requests format](https://ubiops.com/docs/deployments/docker-support/#supporting-request-format)) and deploy it using the [bring-your-own-image](https://ubiops.com/docs/environments/#bring-your-own-docker-image) feature. By the end, we will make both a chat completion and embedding requests to the multi-model Ollama server using Ubiops `requests` and OpenAI [python package](https://pypi.org/project/openai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a465f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Set up a connection with the UbiOps API client\n",
    "First, we need to install the UbiOps Python Client Library to interface with UbiOps from Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ccb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU ubiops openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7b34c",
   "metadata": {},
   "source": [
    "Now, we need to initialize all the necessary variables for the UbiOps deployment and set up the deployment directory, which we will later zip and upload to UbiOps.\n",
    "\n",
    "To generate the API token you can follow this [guide](https://ubiops.com/docs/organizations/service-users/) (make sure you set up the right permissions).\n",
    "\n",
    "Make sure you have access to the instance type `\"16384 MB + 4 vCPU (Dedicated)\"`, as for now only this instance supports Docker images.\n",
    "\n",
    "Once you have your project name and the API token, paste them below in the following cell before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"<UBIOPS_API_TOKEN>\" # Used to create the deployments and pipeline, make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<YOUR_PROJECT_NAME>\" # Fill in your project name here\n",
    "DEPLOYMENT_NAME = \"ollama-multi-model-server\"\n",
    "ENVIRONMENT_NAME = \"ollama-env\"\n",
    "DEPLOYMENT_VERSION = \"v1\"  \n",
    "INSTANCE_TYPE = \"16384 MB + 4 vCPU (Dedicated)\"\n",
    "API_HOST_URL=\"<API_HOST_URL>\" # Standard UbiOps API URL is 'https://api.ubiops.com/v2.1', your URL may differ depending on your environment\n",
    "\n",
    "print(f\"Your new deployment will be called: {DEPLOYMENT_NAME}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b83529",
   "metadata": {},
   "source": [
    "Next, let's initialize the UbiOps client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ubiops\n",
    "\n",
    "configuration = ubiops.Configuration(host=API_HOST_URL)\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.CoreApi(client)\n",
    "api.service_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20269a1",
   "metadata": {},
   "source": [
    "## 2. Creating a custom environment\n",
    "\n",
    "For our multi-model Ollama deployment, we need to create a custom environment by uploading a Docker image to UbiOps. UbiOps has the [bring-your-own-image](https://ubiops.com/docs/environments/#bring-your-own-docker-image) feature, allowing you to upload custom Docker containers as environments while maintaining full compatibility with UbiOps request handling, scaling, and monitoring capabilities.\n",
    "\n",
    "When you upload a Docker image as a [custom environment](https://ubiops.com/docs/environments/#create-a-custom-environment) in UbiOps, the platform treats it as any other environment that can be selected when creating deployment versions. The key requirement is that your Docker image must be compatible with UbiOps's request format and include the necessary agent implementation to handle incoming requests. \n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- **Docker**: Install [Docker Engine](https://docs.docker.com/engine/install/) or [Docker Desktop](https://docs.docker.com/get-started/get-docker/) on your machine\n",
    "- **UbiOps Base Image**: Access to UbiOps base environment images (contact your account manager or [support portal](https://support.ubiops.com) if unavailable). You will either receive access to a registry, or a single image tar file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf6245",
   "metadata": {},
   "source": [
    "### 2.1 Pull or Load the base image with a UbiOps agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156d7f6",
   "metadata": {},
   "source": [
    "Pull a base image with a UbiOps agent (if you were granted access to a registry):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b056bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull <registry>/ubiops-deployment-instance-ubuntu24.04-python3.13:v5.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51954f2a",
   "metadata": {},
   "source": [
    "Load the base image with a UbiOps agent (if you have received a file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker load -i <FILE_DIRECTORY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65df8b5",
   "metadata": {},
   "source": [
    "### 2.2 Create the dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_file = \"\"\"\n",
    "FROM <registry>/ubiops-deployment-instance-ubuntu24.04-python3.13:v5.17.2\n",
    "USER root\n",
    "RUN apt-get update && \\\n",
    "    apt-get install --no-install-recommends -y git curl && \\\n",
    "    apt-get -y autoremove && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "RUN curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "USER deployment\n",
    "\n",
    "RUN pip install urllib3==1.26.19 jsonschema==3.2.0 django==5.1.4\n",
    "RUN pip install ollama openai\n",
    "\"\"\"\n",
    "\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(docker_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7bb27",
   "metadata": {},
   "source": [
    "Now let's build the new image and save it as a `tar` archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build . -t ollama-ubiops\n",
    "!docker save -o ollama-ubiops.tar ollama-ubiops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09893e60",
   "metadata": {},
   "source": [
    "### 2.3 Creating an environment\n",
    "\n",
    "We need to create an empty environment with `supports_request_format=True` in UbiOps that will serve as a container for our custom Docker image. This step establishes the environment definition in UbiOps, which we will then populate by uploading our custom Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ubiops.EnvironmentCreate(\n",
    "    name='ollama-env',\n",
    "    description=\"Environment with an ollama server that supports requests format\",\n",
    "    supports_request_format=True\n",
    ")\n",
    "api.environments_create(PROJECT_NAME, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabebd8",
   "metadata": {},
   "source": [
    "Now we can upload the image as a revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.environment_revisions_file_upload(\n",
    "   PROJECT_NAME, \n",
    "   ENVIRONMENT_NAME, \n",
    "   file=\"ollama-ubiops.tar\"\n",
    ")\n",
    "ubiops.utils.wait_for_environment(client, PROJECT_NAME, ENVIRONMENT_NAME)\n",
    "api_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d4e66",
   "metadata": {},
   "source": [
    "## 3. Creating a UbiOps deployment\n",
    "In this section, we will create the UbiOps deployment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e93d3",
   "metadata": {},
   "source": [
    "### 3.1 Create UbiOps deployment\n",
    "Now we can create the deployment, where we define the inputs and outputs of the model. Each deployment can have multiple versions. For each version, you can deploy different code, environments, instance types, etc.\n",
    "\n",
    "The deployment will have `supports_request_format` enabled to allow autoscaling and monitoring of requests. We use the request endpoint to pass\n",
    "payloads to the openai compatible chat completions endpoint. Therefore we will use input and output datatypes `plain`:\n",
    "\n",
    "| Type   | Data Type |\n",
    "|--------|-----------|\n",
    "| Input  | Plain     |\n",
    "| Output | Plain     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ubiops.DeploymentCreate(\n",
    "    name = DEPLOYMENT_NAME,\n",
    "    description = \"Ollama multi-model deployment\",\n",
    "    supports_request_format=True,\n",
    "    input_type = \"plain\",\n",
    "    output_type = \"plain\"\n",
    ")\n",
    "\n",
    "deployment = api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "\n",
    "print(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce4e00",
   "metadata": {},
   "source": [
    "### 3.2 Create a deployment version\n",
    "Next we create a version for the deployment. For the version we set the name, environment and size of the instance.\n",
    "\n",
    "We also add labels to the deployment version to enable UbiOps's built-in model discovery system. These labels allow the platform to automatically expose your models through the /models endpoint.\n",
    "\n",
    "Required labels:\n",
    "\n",
    "- `openai-compatible: true` - Indicates this deployment can handle OpenAI-format requests\n",
    "\n",
    "- `openai-model-names: model1;model2;model3` - Semicolon-separated list of model names served by this deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"openai-compatible\": \"true\",\n",
    "    \"openai-model-names\": \"smollm2;smollm;all-minilm-l6-v2;nomic-embed-text\"\n",
    "}\n",
    "\n",
    "data = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment=ENVIRONMENT_NAME,\n",
    "    instance_type_group_name=INSTANCE_TYPE,\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    instance_processes=3,\n",
    "    maximum_idle_time=900,\n",
    "    labels = labels\n",
    ")\n",
    "\n",
    "deployment_version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdc72e",
   "metadata": {},
   "source": [
    "### 3.3 Creating a deployment directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1c8a0",
   "metadata": {},
   "source": [
    "Let's create a deployment package directory, where we will add our [deployment package files](https://ubiops.com/docs/deployments/deployment-package/deployment-structure/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_name = \"deployment_package\"\n",
    "# Create directory for the deployment if it does not exist\n",
    "os.makedirs(dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb86f6f",
   "metadata": {},
   "source": [
    "### 3.4 Creating Deployment Code for UbiOps\n",
    "\n",
    "We will now create the deployment code that will run on UbiOps. This involves creating a `deployment.py` file containing \n",
    "a `Deployment` class with two key methods:\n",
    "\n",
    "- **`__init__` Method**  \n",
    "  This method runs when the deployment starts. It can be used to load models, data artifacts, and other requirements for inference.\n",
    "\n",
    "- **`request()` Method**  \n",
    "  This method executes every time a call is made to the model's REST API endpoint. It contains the logic for processing incoming data.\n",
    "\n",
    "We will configure [`instance_processes`](https://ubiops.com/docs/requests/request-concurrency/#request-concurrency-per-instance) to 3, \n",
    "allowing each deployment instance to handle 3 concurrent requests. Note that Ollama is not optimized to process a large number of requests simultaneously.\n",
    "\n",
    "The Ollama server will be loaded as a background process within the `__init__` of the first process. A client will also be initialized in each process to proxy requests from all running processes to the Ollama server.\n",
    "\n",
    "These environment variables will be set to configure Ollama's behavior:\n",
    "\n",
    "- `OLLAMA_KEEP_ALIVE=-1`: will keep model always loaded in memory\n",
    "- `OLLAMA_HOST=0.0.0.0:11434`: will serve Ollama on a public port. So, it can be also exposed through [port forwarding](https://ubiops.com/docs/deployments/deployment-versions/#opening-up-a-port-from-your-deployment-beta)\n",
    "- `OLLAMA_EMBEDDING_MODELS` and `OLLAMA_CHAT_MODELS`: [environment variables](https://ubiops.com/docs/environment-variables/) configured to define which models the deployment should initialize and make available for inference (we will create these later in the notebook)\n",
    "\n",
    "For a complete overview of the deployment code structure, refer to the [UbiOps documentation](https://ubiops.com/docs/deployments/deployment-package/deployment-structure/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959df70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dir_name}/deployment.py\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "\n",
    "\n",
    "class PublicError(Exception):\n",
    "    def __init__(self, public_error_message):\n",
    "        super().__init__()\n",
    "        self.public_error_message = public_error_message\n",
    "\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        print(\"Initializing deployment...\")\n",
    "        \n",
    "        # Parse models from environment variables\n",
    "        self.embedding_models = self._parse_models_env(\"OLLAMA_EMBEDDING_MODELS\")\n",
    "        self.chat_models = self._parse_models_env(\"OLLAMA_CHAT_MODELS\")\n",
    "        \n",
    "        # Create model type mapping\n",
    "        self.model_types = {}\n",
    "        for model in self.embedding_models:\n",
    "            self.model_types[model] = 'embedding'\n",
    "        for model in self.chat_models:\n",
    "            self.model_types[model] = 'chat'\n",
    "        \n",
    "        print(f\"Loaded {len(self.embedding_models)} embedding models, {len(self.chat_models)} chat models\")\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "        self.environment_variables = {\"OLLAMA_KEEP_ALIVE\": \"-1\", \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
    "        \n",
    "        # Initialize server and models (only in main process)\n",
    "        if context.get('process_id') == 0:\n",
    "            self._start_ollama_server()\n",
    "            self._pull_all_models()\n",
    "            print(\"Deployment ready\")\n",
    "\n",
    "    def request(self, data, context):\n",
    "        \"\"\"Process requests and route to appropriate model type.\"\"\"\n",
    "        input_data = json.loads(data) if isinstance(data, str) else data\n",
    "        \n",
    "        model_name = input_data.get('model')\n",
    "        model_type = self.model_types[model_name]\n",
    "        \n",
    "        # Route request based on model type\n",
    "        if model_type == 'embedding':\n",
    "            response = self.client.embeddings.create(**input_data)\n",
    "            return response.model_dump()\n",
    "        else:  # chat model\n",
    "            if input_data.get('stream', False):\n",
    "                response = self.client.chat.completions.create(**input_data)\n",
    "                full_response = []\n",
    "                for chunk in response:\n",
    "                    chunk_data = chunk.model_dump()\n",
    "                    context['streaming_update'](json.dumps(chunk_data))\n",
    "                    full_response.append(chunk_data)\n",
    "                return full_response\n",
    "            else:\n",
    "                response = self.client.chat.completions.create(**input_data)\n",
    "                return response.model_dump()\n",
    "\n",
    "    def _parse_models_env(self, environment_variable):\n",
    "        \"\"\"Parse comma-separated model names from environment variable.\"\"\"\n",
    "        models_string = os.environ.get(environment_variable, \"\")\n",
    "        return [model.strip() for model in models_string.split(',') if model.strip()]\n",
    "    \n",
    "    def _start_ollama_server(self):\n",
    "        \"\"\"Start the Ollama server process.\"\"\"\n",
    "        print(\"Starting Ollama server...\")\n",
    "        subprocess.Popen(['ollama', 'serve'], env=self.environment_variables | os.environ)\n",
    "        time.sleep(5)\n",
    "    \n",
    "    def _pull_all_models(self):\n",
    "        \"\"\"Download all configured models.\"\"\"\n",
    "        for model_name in self.model_types.keys():\n",
    "            print(f\"Pulling model: {model_name}\")\n",
    "            ollama.pull(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809cea7",
   "metadata": {},
   "source": [
    "We need to archive the deployment directory into a ZIP file before uploading to UbiOps. UbiOps requires all deployment packages to be uploaded as ZIP archives containing the deployment code and dependencies. For more details on the required package structure, see the [UbiOps deployment package documentation](https://ubiops.com/docs/deployments/deployment-package/deployment-structure/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Archive the deployment directory\n",
    "deployment_zip_path = shutil.make_archive(dir_name, 'zip', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536426d",
   "metadata": {},
   "source": [
    "### 3.5 Upload a revision\n",
    "We will now upload the deployment to UbiOps. In the background, This step will take some time, because UbiOps interprets\n",
    "the environment files and builds a docker container out of it. You can check the UI for any progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d32ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_response = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=dir_name+\".zip\",\n",
    ")\n",
    "print(upload_response)\n",
    "\n",
    "# Check if the deployment is finished building. This can take a few minutes\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=upload_response.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c12d0",
   "metadata": {},
   "source": [
    "## 4. Creating environment variables\n",
    "\n",
    "In order to be able to change the Ollama models deployed without modifying the deployment package, we can setup environment variables within the deployment and change their values based on our needs.\n",
    "How it works:\n",
    "\n",
    "`OLLAMA_EMBEDDING_MODELS` - Contains a comma-separated list of embedding models (e.g., \"all-minilm:l6-v2,nomic-embed-text\")\n",
    "\n",
    "`OLLAMA_CHAT_MODELS` - Contains a comma-separated list of chat models (e.g., \"smollm,smollm2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58699238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OLLAMA_EMBEDDING_MODELS Environment Variable\n",
    "embedding_data = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"OLLAMA_EMBEDDING_MODELS\",\n",
    "    value=\"all-minilm:l6-v2,nomic-embed-text\",\n",
    "    secret=False\n",
    ")\n",
    "\n",
    "response = api.deployment_environment_variables_create(\n",
    "    PROJECT_NAME, \n",
    "    DEPLOYMENT_NAME,\n",
    "    embedding_data\n",
    ")\n",
    "\n",
    "# Create OLLAMA_CHAT_MODELS Environment Variable\n",
    "chat_data = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"OLLAMA_CHAT_MODELS\",\n",
    "    value=\"smollm2,smollm\",\n",
    "    secret=False\n",
    ")\n",
    "\n",
    "response = api.deployment_environment_variables_create(\n",
    "    PROJECT_NAME, \n",
    "    DEPLOYMENT_NAME,\n",
    "    chat_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4e78d",
   "metadata": {},
   "source": [
    "## 5. Making requests to the deployment\n",
    "Our deployment is now live on UbiOps! Let's test it out by sending a bunch of requests to it. This request will be a simple prompt to the model, asking it to respond to a question. In case your deployment still needs to scale, it may take some time before your first request is picked up. You can check the logs of your deployment version to see if the Ollama server is ready to accept requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c6274",
   "metadata": {},
   "source": [
    "### 5.1 Send a request\n",
    "Let's first create the request template and write some questions that we can send to the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ee114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "\n",
    "request_template = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"{question}\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"smollm2\", \n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "questions = [\n",
    "    \"What is the weather like today?\",\n",
    "    \"How do I cook pasta?\",\n",
    "    \"Can you explain quantum physics?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I learn Python?\"\n",
    "]\n",
    "\n",
    "requests_data = []\n",
    "for question in questions:\n",
    "    filled_request = copy.deepcopy(request_template)\n",
    "    filled_request['messages'][1]['content'] = question\n",
    "    requests_data.append(filled_request)\n",
    "\n",
    "# Print the resulting requests\n",
    "print(json.dumps(requests_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee79a76",
   "metadata": {},
   "source": [
    "Now let's create a request and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7928d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=requests_data[2]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10a396",
   "metadata": {},
   "source": [
    "Let's also try a different `chat-completion` model. For this we can just change the model name in the request template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_template = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"What is the meaning of life?\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"smollm\", \n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "print(api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=request_template\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907da595",
   "metadata": {},
   "source": [
    "### 5.2 Send a batch of requests\n",
    "This section sends a batch of requests. It allows you to observe how Ollama fetches and processes multiple requests simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_plain_batch = [json.dumps(item) for item in requests_data]\n",
    "\n",
    "requests = api.batch_deployment_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME, \n",
    "    data=send_plain_batch, \n",
    "    timeout=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdb174",
   "metadata": {},
   "source": [
    "We can go over to the UI and inspect how are the `requests` being handled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11343d4",
   "metadata": {},
   "source": [
    "### 5.3 Sending embedding requests\n",
    "\n",
    "Now let's test the embedding functionality. Embedding requests convert text into numerical vectors that capture semantic meaning, commonly used for search and RAG applications. We specify an embedding model and text input (instead of messages), and the result will be a vector representing the text's semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_request = {\n",
    "    \"model\": \"all-minilm:l6-v2\",\n",
    "    \"input\": \"Can you tell me more about openai in exactly 50 words\"\n",
    "}\n",
    "\n",
    "print(api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=embedding_request\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0e9e8",
   "metadata": {},
   "source": [
    "### 5.4 Sending a request with streaming output\n",
    "\n",
    "For this request, we will add the key `stream: true` to the input, enabling streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e644f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"How is the weather?\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"smollm2\",\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "# Create a streaming deployment request\n",
    "for item in ubiops.utils.stream_deployment_request(\n",
    "        client=api.api_client,\n",
    "        project_name=PROJECT_NAME,\n",
    "        deployment_name=DEPLOYMENT_NAME,\n",
    "        version=DEPLOYMENT_VERSION,\n",
    "        data=request_data,\n",
    "        timeout=3600,\n",
    "        full_response=False,\n",
    "):\n",
    "    item_dict = json.loads(item)\n",
    "    if item_dict.get(\"choices\"):\n",
    "        print(item_dict[\"choices\"][0][\"delta\"][\"content\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf9b93",
   "metadata": {},
   "source": [
    "### 5.5 Sending requests to the OpenAI Endpoint\n",
    "We can also connect to this deployment with the UbiOps OpenAI endpoint. \n",
    "\n",
    "First, let's initialize the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_TOKEN.lstrip(\"Token \"),  \n",
    "    base_url=f\"{API_HOST_URL}/projects/{PROJECT_NAME}/openai-compatible/v1/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efd17a",
   "metadata": {},
   "source": [
    "Now we can create the request and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a59a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_var = False\n",
    "\n",
    "print(client.chat.completions.create(\n",
    "    model=f\"ubiops-deployment/{DEPLOYMENT_NAME}/{DEPLOYMENT_VERSION}/smollm\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Can you tell me more about openai in exactly two lines\"}],\n",
    "    stream=stream_var\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2524f9d",
   "metadata": {},
   "source": [
    "We can also list the models that are available for your API token using the /models endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = client.models.list()\n",
    "print(models_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d244f53",
   "metadata": {},
   "source": [
    "Let's also create an embedding request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.embeddings.create(\n",
    "    model=f\"ubiops-deployment/{DEPLOYMENT_NAME}/{DEPLOYMENT_VERSION}/all-minilm:l6-v2\",\n",
    "    input=\"Hello!\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605f982",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "At last, let's close our connection to UbiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1d7f",
   "metadata": {},
   "source": [
    "We have set up a deployment that hosts a multi-model Ollama server. This tutorial just serves as an example. Feel free to reach out to\n",
    "We have set up a deployment that hosts a multi-model Ollama server. This tutorial just serves as an example. Feel free to reach out to\n",
    "our [support portal](https://www.support.ubiops.com) if you want to discuss your set-up in more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
