{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Pachyderm and UbiOps for model training and deploying\n",
    "This notebook shows how to create a complete data science application. \n",
    "A dataset of labelled faces is stored in Pachyderm. This set is used to train an algorithm using Pachyderm pipelines to predict the age of humans. The trained algorithm is then sent to UbiOps for serving. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"architecture.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the train data\n",
    "This data will be used to train the model we are building, it takes a while to download since so it is a good idea to start the download as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pachyderm \n",
    "First we need to setup Pachyderm. The easiest way is to use Pachyderm Hub https://hub.pachyderm.com/\n",
    "![pachyderm hub](pachy_hub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Connecting to your hub\n",
    "After creating your pachyderm hub, click on the connect button and follow the steps to connect your device to the pachyderm environment.\n",
    "![pachyderm connect](pachy_connect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pachyderm repo\n",
    "Now that we have created a Pachyderm hub enviroment we can create a Pachyderm repo to store th face training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create repo faces\n",
    "!pachctl list repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add UbiOps token\n",
    "Because we want Pachyderm to sent the trained model to UbiOps we need to add credentials to Pachyderm. We will do that by adding a UbiOps token to a secret in Pachyderm.\n",
    "\n",
    "You can learn more about getting this token by creating a service user here:\n",
    "https://ubiops.com/docs/organizations/service-users/#creating-a-service-user\n",
    "\n",
    "![UbiOps token](../pictures/create-token.gif)\n",
    "\n",
    "#### Important is to add the role 'project-editor' so that the token has enought rights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your token below to write to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile secret.yaml\n",
    "{\n",
    "    \"apiVersion\": \"v1\",\n",
    "    \"kind\": \"Secret\",\n",
    "    \"metadata\": {\n",
    "       \"name\": \"ubiops\"\n",
    "    },\n",
    "    \"type\": \"Opaque\",\n",
    "    \"stringData\": {\n",
    "       \"token\": \"Token <YOUR_TOKEN_HERE>\" # Add your token here\n",
    "    }\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pachctl delete secret ubiops\n",
    "!pachctl create secret -f secret.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to host the deployment you will be creating\n",
    "!mkdir pachy_source/tensorflow_deployment_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pachyderm pipeline script\n",
    "This script runs in a Pachyderm pipeline and is used to train the model from the face train set. It also sends this trained model to UbiOps for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pachy_source/main.py\n",
    "\n",
    "# Enter your project name below\n",
    "PROJECT_NAME = <YOUR_PROJECT_NAME>\n",
    "DEPLOYMENT_NAME = 'tensorflow-deployment'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "import os\n",
    "import ubiops\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import hydra\n",
    "from hydra.utils import to_absolute_path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from src.factory import get_model, get_optimizer, get_scheduler\n",
    "from src.generator import ImageSequence\n",
    "from hydra.experimental import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops\n",
    "\n",
    "\n",
    "\n",
    "#@hydra.main(config_path=\"age_gender_estimation/src/config.yaml\")\n",
    "#we cannot use this in jupyter notebooks\n",
    "def main(cfg):\n",
    "    if cfg.wandb.project:\n",
    "        import wandb\n",
    "        from wandb.keras import WandbCallback\n",
    "        wandb.init(project=cfg.wandb.project)\n",
    "        callbacks = [WandbCallback()]\n",
    "    else:\n",
    "        callbacks = []\n",
    "        \n",
    "    data_path = Path(\"/pfs/faces/data/imdb_crop\")\n",
    "    #data_path = Path(\"/home/raoulfasel/Documents/pachyderm/age_gender_estimation/data/imdb_crop\")\n",
    "    \n",
    "    csv_path = Path(to_absolute_path(\"./\")).joinpath(\"meta\", f\"{cfg.data.db}.csv\")\n",
    "    #csv_path = Path(to_absolute_path(\"/pfs/faces\")).joinpath(\"meta\", f\"{cfg.data.db}.csv\")\n",
    "    print(csv_path)\n",
    "    df = pd.read_csv(str(csv_path))\n",
    "    train, val = train_test_split(df, random_state=42, test_size=0.1)\n",
    "    train_gen = ImageSequence(cfg, train, \"train\", data_path)\n",
    "    val_gen = ImageSequence(cfg, val, \"val\", data_path)\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = get_model(cfg)\n",
    "        opt = get_optimizer(cfg)\n",
    "        scheduler = get_scheduler(cfg)\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=[\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"],\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    #checkpoint_dir = Path(to_absolute_path(\"age_gender_estimation\")).joinpath(\"checkpoint\")\n",
    "    checkpoint_dir = Path(to_absolute_path(\"/pfs/build\")).joinpath(\"checkpoint\")\n",
    "\n",
    "    print(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    filename = \"_\".join([cfg.model.model_name,\n",
    "                         str(cfg.model.img_size),\n",
    "                         \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"])\n",
    "    callbacks.extend([\n",
    "        LearningRateScheduler(schedule=scheduler),\n",
    "        ModelCheckpoint(str(checkpoint_dir) + \"/\" + filename,\n",
    "                        monitor=\"val_loss\",\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode=\"auto\")\n",
    "    ])\n",
    "\n",
    "    model.fit(train_gen, epochs=cfg.train.epochs, callbacks=callbacks, validation_data=val_gen,\n",
    "              workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    model.save(\"tensorflow_deployment_package/tensorflow_model.h5\")\n",
    "\n",
    "    # Create the dployment on UbiOps\n",
    "    with open('/opt/ubiops/token', 'r') as reader:\n",
    "        API_TOKEN = reader.read()\n",
    "    client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "    api = ubiops.CoreApi(client)\n",
    "    \n",
    "    # Create the deployment\n",
    "    deployment_template = ubiops.DeploymentCreate(\n",
    "        name=DEPLOYMENT_NAME,\n",
    "        description='Tensorflow deployment',\n",
    "        input_type='structured',\n",
    "        output_type='structured',\n",
    "        input_fields=[\n",
    "            {'name':'input_image', 'data_type':'file'}\n",
    "        ],\n",
    "        output_fields=[\n",
    "            {'name':'output_image', 'data_type':'file'}\n",
    "        ],\n",
    "        labels={\"demo\": \"tensorflow\"}\n",
    "    )\n",
    "\n",
    "    api.deployments_create(\n",
    "        project_name=PROJECT_NAME,\n",
    "        data=deployment_template\n",
    "    )\n",
    "\n",
    "    # Create the version\n",
    "    version_template = ubiops.DeploymentVersionCreate(\n",
    "        version=DEPLOYMENT_VERSION,\n",
    "        environment='python3-8',\n",
    "        instance_type='2048mb',\n",
    "        minimum_instances=0,\n",
    "        maximum_instances=1,\n",
    "        maximum_idle_time=1800, # = 30 minutes\n",
    "        request_retention_mode='none' # We don't need to store the requests for this deployment\n",
    "    )\n",
    "\n",
    "    api.deployment_versions_create(\n",
    "        project_name=PROJECT_NAME,\n",
    "        deployment_name=DEPLOYMENT_NAME,\n",
    "        data=version_template\n",
    "    )\n",
    "\n",
    "    # Zip the deployment package\n",
    "    shutil.make_archive('tensorflow_deployment_package', 'zip', '.', 'tensorflow_deployment_package')\n",
    "\n",
    "    # Upload the zipped deployment package\n",
    "    file_upload_result =api.revisions_file_upload(\n",
    "        project_name=PROJECT_NAME,\n",
    "        deployment_name=DEPLOYMENT_NAME,\n",
    "        version=DEPLOYMENT_VERSION,\n",
    "        file='tensorflow_deployment_package.zip'\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #with initialize(config_path=\"age_gender_estimation/src/\"):\n",
    "    with initialize(config_path=\"src/\"):\n",
    "        cfg = compose(config_name=\"config\")\n",
    "        print(OmegaConf.to_yaml(cfg))\n",
    "    main(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UbiOps deployment\n",
    "The following script is sent to UbiOps together with a trained weight file for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pachy_source/tensorflow_deployment_package/deployment.py\n",
    "\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import argparse\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "from src.factory import get_model\n",
    "\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - environment (str): the environment in which the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising the model\")\n",
    "\n",
    "        model_file = os.path.join(base_directory, \"tensorflow_model.h5\")\n",
    "        self.model = load_model(model_file)\n",
    "\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "        print('Loading data')\n",
    "        margin = 0.4\n",
    "    \n",
    "\n",
    "        # for face detection\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "        # load model and weights\n",
    "        img_size = 224\n",
    "\n",
    "        img = read_image(data.get('input_image'))\n",
    "        input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w, _ = np.shape(input_img)\n",
    "\n",
    "        # detect faces using dlib detector\n",
    "        detected = detector(input_img, 1)\n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "\n",
    "        if len(detected) > 0:\n",
    "            for i, d in enumerate(detected):\n",
    "                x1, y1, x2, y2, w, h = (\n",
    "                    d.left(),\n",
    "                    d.top(),\n",
    "                    d.right() + 1,\n",
    "                    d.bottom() + 1,\n",
    "                    d.width(),\n",
    "                    d.height(),\n",
    "                )\n",
    "                xw1 = max(int(x1 - margin * w), 0)\n",
    "                yw1 = max(int(y1 - margin * h), 0)\n",
    "                xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "                yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
    "                faces[i] = cv2.resize(\n",
    "                    img[yw1 : yw2 + 1, xw1 : xw2 + 1], (img_size, img_size)\n",
    "                )\n",
    "\n",
    "            # predict ages and genders of the detected faces\n",
    "            results = self.model.predict(faces)\n",
    "            predicted_genders = results[0]\n",
    "            ages = np.arange(0, 101).reshape(101, 1)\n",
    "            predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "            # draw results\n",
    "            for i, d in enumerate(detected):\n",
    "                label = \"{}, {}\".format(\n",
    "                    int(predicted_ages[i]), \"M\" if predicted_genders[i][0] < 0.5 else \"F\"\n",
    "                )\n",
    "                draw_label(img, (d.left(), d.top()), label)\n",
    "\n",
    "            RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(\"prediction.jpg\", RGB_img)\n",
    "\n",
    "        return {\n",
    "            \"output_image\": \"prediction.jpg\"\n",
    "        }\n",
    "\n",
    "def draw_label(\n",
    "    image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=1.5, thickness=2\n",
    "):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        label,\n",
    "        point,\n",
    "        font,\n",
    "        font_scale,\n",
    "        (255, 255, 255),\n",
    "        thickness,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "def read_image(image_path):\n",
    "    img = cv2.imread(str(image_path), 1)\n",
    "\n",
    "    if img is not None:\n",
    "        h, w, _ = img.shape\n",
    "        r = 640 / max(w, h)\n",
    "        return cv2.resize(img, (int(w * r), int(h * r)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the `ubiops.yaml` which allows installing of linux dependencies that are required for some python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pachy_source/tensorflow_deployment_package/ubiops.yaml\n",
    "\n",
    "apt:\n",
    "  packages:\n",
    "    - cmake\n",
    "    - protobuf-compiler\n",
    "    - build-essential\n",
    "    - python3.8-dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "We are using this example as a basis for our model:\n",
    "https://github.com/UbiOps/tutorials/blob/master/tensorflow-example/tensorflow-ubiops-example/tensorflow_template.ipynb\n",
    "We added a part of this repo already. But in order to include them to both the UbiOps and the Pachyderm packages we copy the necessary files to the correct locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp age_gender_estimation/requirements.txt pachy_source/requirements.txt \n",
    "!cp age_gender_estimation/requirements.txt pachy_source/tensorflow_deployment_package/requirements.txt\n",
    "!mkdir pachy_source/tensorflow_deployment_package/age_gender_estimation\n",
    "!cp -r age_gender_estimation/src pachy_source/tensorflow_deployment_package/age_gender_estimation/src\n",
    "!cp -r age_gender_estimation/src pachy_source/src\n",
    "!cp -r age_gender_estimation/meta pachy_source/meta\n",
    "\n",
    "!echo ubiops >> pachy_source/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pachyderm pipeline\n",
    "Now that we put all the files in the correct place we can actually create the Pachyderm pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pachy_source/build_pipeline.json\n",
    "\n",
    "#test build pipeline\n",
    "\n",
    "{\n",
    "  \"pipeline\": {\n",
    "    \"name\": \"faces_train_model\"\n",
    "  },\n",
    "  \"datum_tries\": 1,\n",
    "  \"description\": \"A pipeline that trains our neural network\",\n",
    "  \"transform\": {\n",
    "    \"build\": {\n",
    "      \"image\": \"raoulfaselubiops/pachyderm-builder:latest\", # We are using a custom Pachyderm builder pipeline here\n",
    "      \"path\": \"./\"\n",
    "    },\n",
    "    \"secrets\": [ {\n",
    "        \"name\": \"ubiops\",\n",
    "        \"mount_path\": \"/opt/ubiops\"\n",
    "    },\n",
    "    ] \n",
    "  },\n",
    "  \"input\": {\n",
    "    \"pfs\": {\n",
    "      \"repo\": \"faces\",\n",
    "      \"glob\": \"/*\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl update pipeline -f pachy_source/build_pipeline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the face dataset\n",
    "Finally we can upload the face dataset. This is only uploading a subset of the data because it takes too long, but you can edit it to upload more data. It will take a while before the deloyment shows up in your UbiOps environment (roughly around 10 minutes). Important thing to know is that everytime you run this (upload a file) it will trigger the pipeline on Pachyderm to update the deployment on UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This can take a while, eventhough I am only uploading a small part of the dataset.\n",
    "#It is also faster running from the real terminal\n",
    "!pachctl put file -p=30 --progress=false -r faces@master:data/imdb_crop/00 -f data/imdb_crop/00 #we are using the imdb db for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pachctl list job\n",
    "!pachctl list repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingers crossed\n",
    "If all went well the upload should trigger the Pachyderm pipeline. That means that within a few minutes the trained model will be in UbiOps.\n",
    "\n",
    "And the cool thing is that every new upload will trigger this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Now we have model training and serving figured the next step ofcourse is connecting it to a front end. We are not covering that in this notebook but we have some on our tutorial page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
