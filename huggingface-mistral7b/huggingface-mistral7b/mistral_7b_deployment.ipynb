{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-yetF3Srnk3"
   },
   "source": [
    "# Deploying Mistral 7B to UbiOps with a development set-up\n",
    "\n",
    "This notebook will help you create a cloud-based inference API endpoint for the [Mistral-2-7B-Instruct model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n",
    ", using UbiOps. The Mistral version we will be using is already pretrained and will be \n",
    "loaded from the Huggingface [Mistral AI](https://huggingface.co/mistralai) library. The model has been developed by Mistral AI.\n",
    "\n",
    "[Mistral 7B](https://arxiv.org/abs/2310.06825) is a collection of language model engineered for superior performance and \n",
    "efficiency. Mistral AI claims that the Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks. The model \n",
    "deployed in this tutorial is a fine-tuned version of the Mistral 7B.\n",
    "\n",
    "We will set up the deployment to handle various input configurations and master prompts. You can use this test setup to \n",
    "experiment with different inputs and configurations.\n",
    "\n",
    "In this notebook we will walk you through:\n",
    "\n",
    "1. Connecting with the UbiOps API client\n",
    "2. Creating a code environment for our deployment\n",
    "3. Creating a deployment with a test set-up for the Mistral 7B model\n",
    "4. Calling the Mistral 7B deployment API endpoint\n",
    "\n",
    "Mistral-7B is a text-to-text model. Therefore we will make a deployment which takes a text prompt as an input, and returns\n",
    "a response. We will also add the `system_prompt` and [`config`](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig)\n",
    "to the input of the deployment, so we can experiment with different inputs to see how that changes the response of the model. \n",
    "Note that Mistral is behind a gated repository - in order for you to download the model you will also need to a Huggingface \n",
    "token that has sufficient permissionsto download Mistral.\n",
    "\n",
    "Next to the response, the deployment will also return the used `input` (which consists of the `system_prompt` and the \n",
    "`prompt`) and the `used_config`.\n",
    "\n",
    "When no additional `system_prompt` or `config` are provided, the deployment will use pre-set default values, which\n",
    "you can find in the `__init__` statement of the deployment.\n",
    "\n",
    "|Deployment input & output variables| **Variable name** |**Data type** |\n",
    "|--------------------|--------------|--------------|\n",
    "| **Input fields**   | prompt | string |\n",
    "|                    | system_prompt | string |\n",
    "|                    | config | dictionary|\n",
    "| **Output fields**  | response | string |\n",
    "|                    | input        | string |\n",
    "|                    | used_config  | dictionary |\n",
    "\n",
    "Note that we deploy to a GPU instance by default, which are not accessible in every project. You can \n",
    "[contact us](https://ubiops.com/contact-us/) about this.\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1O0t37LOYJh"
   },
   "source": [
    "\n",
    "## 1. Connecting with the UbiOps API client\n",
    "To use the UbiOps API from our notebook, we need to install the UbiOps Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBDhY41Hw6pz"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade ubiops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBQfwWt4rnk4"
   },
   "source": [
    "To set up a connection with the UbiOps platform API we need the name of your UbiOps project and an API token with `project-editor` permissions.\n",
    "\n",
    "Once you have your project name and API token, paste them below in the following cell before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcMtxyTvrnk4"
   },
   "outputs": [],
   "source": [
    "import ubiops\n",
    "from datetime import datetime\n",
    "\n",
    "API_TOKEN = \"<INSERT API_TOKEN WITH PROJECT EDITOR RIGHTS>\"  # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<INSERT PROJECT NAME IN YOUR ACCOUNT>\"\n",
    "\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"  # You can change this parameter if you want to use a different model from Huggingface.\n",
    "HF_TOKEN = \"<ENTER YOUR HF TOKEN WITH ACCESS TO MISTRAL REPO HERE>\"\n",
    "\n",
    "DEPLOYMENT_NAME = f\"mistral-7b-{datetime.now().date()}\"\n",
    "DEPLOYMENT_VERSION = \"v1\"\n",
    "\n",
    "# Initialize client library\n",
    "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "# Establish a connection\n",
    "client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.CoreApi(client)\n",
    "print(api.projects_get(PROJECT_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0uBONwUtCdm"
   },
   "source": [
    "## 2. Setting up the environment\n",
    "\n",
    "Our environment code contains instructions to install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-AR2GFxlRN4"
   },
   "outputs": [],
   "source": [
    "environment_dir = \"environment_package\"\n",
    "ENVIRONMENT_NAME = \"mistral-7b-environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yESMyTdek6Pu"
   },
   "outputs": [],
   "source": [
    "%mkdir {environment_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLPngTwatSvQ"
   },
   "source": [
    "We first write a `requirements.txt` file, which contains the Python packages that we will use in our deployment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yyybhdawk8ju"
   },
   "outputs": [],
   "source": [
    "%%writefile {environment_dir}/requirements.txt\n",
    "# This file contains package requirements for the environment\n",
    "# installed via PIP.\n",
    "numpy==1.26.3\n",
    "torch==2.0.1+cu118\n",
    "transformers==4.37.0\n",
    "accelerate==0.26.1\n",
    "bitsandbytes==0.42.0\n",
    "huggingface_hub==0.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goTGrRu1thaV"
   },
   "source": [
    "Next we add a `ubiops.yaml` to set a remote pip index. This ensures that we install a CUDA-compatible version of PyTorch. \n",
    "CUDA allows models to be loaded and to run GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBEfEOyHlJc9"
   },
   "outputs": [],
   "source": [
    "%%writefile {environment_dir}/ubiops.yaml\n",
    "environment_variables:\n",
    "- PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1laiUgRt5K9"
   },
   "source": [
    "Now we create a UbiOps [`environment`](https://ubiops.com/docs/environments/#environments). We select python3.11 with CUDA pre-installed as the `base environment` if we want \n",
    "to run on GPUs. If we run on CPUs, then we use `python3-11`.\n",
    "\n",
    "Our additional dependencies are installed on top of this base environment, to create our new `custom_environment` \n",
    "called `mistral-7b-environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnFTXmNll5I0"
   },
   "outputs": [],
   "source": [
    "api_response = api.environments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=ubiops.EnvironmentCreate(\n",
    "        name=ENVIRONMENT_NAME,\n",
    "        # display_name=ENVIRONMENT_NAME,\n",
    "        base_environment=\"python3-11-cuda\",  # use python3-11 when running on CPU\n",
    "        description=\"Environment to run Mistral 7B from Huggingface\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy_3Ih3Xs-At"
   },
   "source": [
    "Package and upload the environment files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqY61U8Us7S8"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "training_environment_archive = shutil.make_archive(\n",
    "    environment_dir, \"zip\", \".\", environment_dir\n",
    ")\n",
    "api.environment_revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    environment_name=ENVIRONMENT_NAME,\n",
    "    file=training_environment_archive,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSTlQ2iKy02n"
   },
   "source": [
    "## 3. Creating a deployment for the Mistral 7B model\n",
    "\n",
    "Now that we have created our code environment in UbiOps, it is time to write the actual code to run the Mistral-7B-Instruct-v0.2\n",
    "model and push it to UbiOps.\n",
    "\n",
    "As you can see we're uploading a `deployment.py` file with a `Deployment` class and two methods:\n",
    "\n",
    "- `__init__` will run when the deployment starts up and can be used to load models, data, artifacts and other requirements for inference.\n",
    "- `request()` will run every time a call is made to the model REST API endpoint and includes all the logic for processing data.\n",
    "\n",
    "Separating the logic between the two methods will ensure fast model response times. We will load the model from Huggingface\n",
    "in the `__init__` method, and code that needs to be ran when a call is made to the deployment in the `request()` method.\n",
    "This way the model only needs to be loaded in when the deployment starts up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m75uYrAYmk9s"
   },
   "outputs": [],
   "source": [
    "deployment_code_dir = \"deployment_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOqhWrwpy9Sn"
   },
   "outputs": [],
   "source": [
    "!mkdir {deployment_code_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGKxscOizDa9"
   },
   "outputs": [],
   "source": [
    "%%writefile {deployment_code_dir}/deployment.py\n",
    "# Code to load from huggingface\n",
    "\"\"\"\n",
    "The file containing the deployment code needs to be called 'deployment.py' and should contain a 'Deployment'\n",
    "class a 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig, pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. Any code inside this method will execute when the deployment starts up.\n",
    "        It can for example be used for loading modules that have to be stored in memory or setting up connections.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising deployment\")\n",
    "\n",
    "        model_id = os.environ.get(\"MODEL_ID\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "        hf_token = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "        \n",
    "        print(f\"Model set as: {model_id}\")\n",
    "\n",
    "        login(token=hf_token)\n",
    "\n",
    "        print(\"Login succesful\")\n",
    "\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        print(\"Loading device\")\n",
    "        self.device = torch.device(\"cuda\") if gpu_available else torch.device(\"cpu\")\n",
    "        print(\"Device loaded in\")\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        print(\"Downloading model\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                          quantization_config = bnb_config,\n",
    "                                                          torch_dtype=torch.bfloat16,\n",
    "                                                          device_map=\"auto\"\n",
    "                                                          )\n",
    "        \n",
    "        print(\"Downloading tokenizer\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            os.environ.get(\"PIPELINE_TASK\", \"text-generation\"),\n",
    "            model=model_id,\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_full_text=False,\n",
    "        )\n",
    "\n",
    "        self.base_prompt = (\n",
    "            \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\"\n",
    "        )\n",
    "\n",
    "        self.default_config = {\n",
    "            'do_sample': True,\n",
    "            'max_new_tokens': 512,\n",
    "            'eos_token_id': self.tokenizer.eos_token_id,\n",
    "            'temperature': 0.3\n",
    "        }    \n",
    "\n",
    "        self.system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\"\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = data[\"prompt\"]\n",
    "\n",
    "        # Update the system prompt if user added a system prompt\n",
    "        if data[\"system_prompt\"]:\n",
    "            system_prompt = data[\"system_prompt\"]\n",
    "        else:\n",
    "            system_prompt = self.system_prompt\n",
    "\n",
    "        config = self.default_config.copy()\n",
    "\n",
    "        # Update config dict if user added a config dict\n",
    "        if data[\"config\"]:\n",
    "            config.update(data[\"config\"])\n",
    "\n",
    "        #Create full prompt\n",
    "        input = self.base_prompt.format(\n",
    "            system_prompt=system_prompt, user_prompt=data[\"prompt\"]\n",
    "        )\n",
    "\n",
    "        model_inputs = self.tokenizer(input, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Here we set the GenrerationConfig to parameteriz the generate method\n",
    "        generation_config = self.default_config.copy()\n",
    "        \n",
    "        #Update config dict if user added a config dict\n",
    "        if data[\"config\"]:\n",
    "            generation_config.update(data[\"config\"])\n",
    "        \n",
    "\n",
    "        print(\"Generating output\")\n",
    "\n",
    "        # Generate text\n",
    "        sequences = self.pipe(\n",
    "            input,\n",
    "            **config\n",
    "        )\n",
    "                                            \n",
    "        response = sequences[0][\"generated_text\"]\n",
    "\n",
    "        # Here we set our output parameters in the form of a json\n",
    "        return {\"response\": response,\n",
    "                \"used_config\": config,\n",
    "                \"input\":input}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkn-xQGyrzpp"
   },
   "source": [
    "### Create a UbiOps deployment\n",
    "\n",
    "Create a deployment. Here we define the in- and outputs of a model. We can create different deployment versions.\n",
    "\n",
    "Note that we added a default `system_prompt` & `config` field to the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUbHzsSbyhgC"
   },
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"prompt\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"system_prompt\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"config\", \"data_type\": \"dict\"},\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {\"name\": \"response\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"input\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"used_config\", \"data_type\": \"dict\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "api.deployments_create(project_name=PROJECT_NAME, data=deployment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gou_ZJxZrnk9"
   },
   "source": [
    "### Create a deployment version\n",
    "\n",
    "Now we will create a version of the deployment. For the version we need to define the name, the environment, the type of instance (CPU or GPU) as well the size of the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IQDht-Krnk-"
   },
   "outputs": [],
   "source": [
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment=ENVIRONMENT_NAME,\n",
    "    instance_type_group_name=\"16384 MB + 4 vCPU + NVIDIA Tesla T4\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=600,  # = 10 minutes\n",
    "    request_retention_mode=\"full\",\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=version_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnD5H2onrnk-"
   },
   "source": [
    "Package and upload the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7x9Gi7xOHSf"
   },
   "outputs": [],
   "source": [
    "# And now we zip our code (deployment package) and push it to the version\n",
    "\n",
    "import shutil\n",
    "\n",
    "deployment_code_archive = shutil.make_archive(\n",
    "    deployment_code_dir, \"zip\", deployment_code_dir\n",
    ")\n",
    "\n",
    "upload_response = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=deployment_code_archive,\n",
    ")\n",
    "print(upload_response)\n",
    "\n",
    "# Check if the deployment is finished building. This can take a few minutes\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=upload_response.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only send requests to our deployment version, after our environment has finished building. \n",
    "\n",
    "NOTE: Building the environment might take a while as we need to download and install all the packages and dependencies. We only need to build our environment once: next time that we spin up an instance of our deployment, we won't need to install all dependencies anymore. Toggle off `stream_logs` to not stream logs of the build process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment variable\n",
    "\n",
    "Here we create an environment variable for the `model_id` and your, which is used to specify which model will be downloaded from Huggingface. If you want to use another version of Mistral you can replace the value of `MODEL_ID` in the cell below, with the `model_id` of the model that you would like to use.\n",
    "\n",
    "Here we create two environment variables, one for the `model_id` and your, which is used to specify which model will be \n",
    "downloaded from Huggingface. And one for your Huggingface token, which you need to download the model from Huggingface.\n",
    "If you want to use another version of Mistral you can replace the value of `MODEL_ID` in the cell below, with the \n",
    "`model_id` of the model that you would like to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=ubiops.EnvironmentVariableCreate(\n",
    "        name=\"model_id\", value=MODEL_ID, secret=False\n",
    "    ),\n",
    ")\n",
    "\n",
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"HF_TOKEN\", value=MODEL_ID, secret=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZCNhtuprnk-"
   },
   "source": [
    "## 4. Calling the Mistral 7B deployment API endpoint\n",
    "\n",
    "Our deployment is now ready to be requested! We can send requests to it via the `deployment-requests-create` or the `batch-deployment-requests-create` API endpoint. During this step a node will be spun up, and the model will be downloaded\n",
    "from Huggingface. Hence why this step can take a while. You can monitor the progress of the process in the \n",
    "[logs](https://ubiops.com/docs/monitoring/logging/). Subsequent results to the deployment will be handled faster. We \n",
    "will use a batch request to kick off our instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a request using the default `system_prompt` and `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"prompt\": \"tell me a joke\", \"system_prompt\": \"\", \"config\": {}}\n",
    "\n",
    "api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=data, timeout=3600\n",
    ").result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a request using other values for the `system_prompt` and `config`.\n",
    "\n",
    "For this request we will instruct the LLM to translate English texts into the style of Shakespearean. We will let the model\n",
    "be more creative with generating sequences by lowering the `temperature` parameter. The text used for this example is shown\n",
    "in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In the village of Willowbrook lived a girl named Amelia, known for her kindness and curiosity. One autumn day, she ventured into the forest and stumbled upon an old cottage filled with dusty tomes of magic. Amelia delved into the ancient spells, discovering her own hidden powers. As winter approached, a darkness loomed over the village. Determined to protect her home, Amelia confronted the source of the darkness deep in the forest. With courage and magic, she banished the shadows and restored peace to Willowbrook.Emerging triumphant, Amelia returned home, her spirit ablaze with newfound strength. From that day on, she was known as the brave sorceress who saved Willowbrook, a legend of magic and courage that echoed through the ages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"prompt\": text,\n",
    "    \"system_prompt\": \"You are a friendly chatbot that translates texts into the style of Shakespearean.\",\n",
    "    \"config\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.5,\n",
    "    },\n",
    "}\n",
    "\n",
    "api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=data, timeout=3600\n",
    ").result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SctNpKvZoaBy"
   },
   "source": [
    "So that's it! You now have your own on-demand, scalable Mistral-7B-Instruct-v0.2 model running in the cloud, with a REST API that you can reach from anywhere!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
