{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement RAG with Langchain on UbiOps\n",
    "\n",
    "\n",
    "Note: This notebook runs on Python 3.11 and uses UbiOps 4.1.0\n",
    "\n",
    "This notebook shows you how you can implement a Retrieval-Augmented Generation (RAG) framework for your LLM using the pipeline\n",
    "functionallity of UbiOps and Langchain. RAG is a framework that retrieves relevant or supporting context, and \n",
    "adds them to the input. The input and additional documents are then fed to the LLM which, produces the final output. For this \n",
    "tutorial we will be hosting the [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) LLM, \n",
    "use the embeddings from Cohere, and Pinecone to store these embeddings. The set-up we will create in this tutorial will \n",
    "give the user better recommandations of where to travel too.\n",
    "\n",
    "The framework will be set-up in a pipeline that contains two deployments: one that hosts & searchers through an embedding\n",
    "database (the instructor depoyment) that will be used to concatenate the user's prompt with additional embedding, \n",
    "and one deployment for where the LLM will be run.\n",
    "\n",
    "This framework can be set up in your UbiOps environment in four steps:\n",
    "1) Establish a connection with your UbiOps environment\n",
    "2) Create the deployment for the embeddings\n",
    "3) Create the deployment for the LLM\n",
    "4) Create a pipeline that combines the two deployments created in step 2 and 3\n",
    "\n",
    "**NOTE:** In order to complete this tutorial you will need an API key from [Cohere](https://dashboard.cohere.com/welcome/register)\n",
    " and [Pinecone](https://login.pinecone.io), you can acquire an API key after making an account for both platforms.\n",
    "Since Mistral is behind a gated repository - you will also need to a Huggingface token that has sufficient permissions \n",
    "to download Mistral.\n",
    "\n",
    "For this tutorial the [environments](https://ubiops.com/docs/environments/) will be created implicitly. This means that we \n",
    "will create two deployment packages. which will contain two files:\n",
    "- `deployment.py`, the code that runs when a request is made (i.e., the embedding model & LLM model)\n",
    "- `requirements.txt`,which will contain additional dependencies that UbiOps will add to the base environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Connecting with the UbiOps API client\n",
    "\n",
    "To use the UbiOps API from our notebook, we need to install the UbiOps Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ubiops\n",
    "!pip install langchain\n",
    "!pip install pinecone-client\n",
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up a connection with the UbiOps platform API we need the name of your UbiOps project and an API token with `project-editor` permissions.\n",
    "\n",
    "Once you have your project name and API token, paste them below in the following cell before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ubiops\n",
    "import shutil\n",
    "import langchain\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"<UBIOPS_API_TOKEN>\"  # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"<PROJECT_NAME>\"  # Fill in your project name here\n",
    "\n",
    "HF_TOKEN = \"<ENTER YOUR HF TOKEN WITH ACCESS TO MISTRAL REPO HERE>\"\n",
    "\n",
    "configuration = ubiops.Configuration()\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "api_client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.api.CoreApi(api_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste the API keys from Pinecone and Cohere below. We will turn these API keys into [environment variables](https://ubiops.com/docs/environment-variables/)\n",
    "later on so we can access them from inside our deployment code we will define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"<PINECONE_API_TOKEN>\"\n",
    "COHERE_API_KEY = \"<COHERE_API_TOKEN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CohereEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.docstore.document import Document\n",
    "import cohere\n",
    "import pinecone\n",
    "import os\n",
    "\n",
    "embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY)\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=\"gcp-starter\")\n",
    "\n",
    "pinecone.create_index(\"ubiops-rag\", dimension=4096)\n",
    "docsearch = Pinecone.from_existing_index(index_name=\"ubiops-rag\", embedding=embeddings)\n",
    "\n",
    "new_doc = Document(page_content=\"description\", metadata={\"place\": \"location\"})\n",
    "inserted = docsearch.add_documents([new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the deployments for the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have established a connection with our UbiOps environment, we can start creating our deployment packages. Each\n",
    "package will consist of two files:\n",
    "- The `deployment.py`, which is where we will define the actual code to run the embedding model and LLM\n",
    "- The `requirements.txt`, which will contain additional dependencies that our codes needs to run properly\n",
    "\n",
    "These deployment packages will be zipped, and uploaded to UbiOps, after which we will add them to a pipeline. The pipeline\n",
    "will consist out of two deployments:\n",
    "- One deployment will host the embedding model\n",
    "- One will host the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DEPLOYMENT_NAME = \"instructor\"\n",
    "LLM_DEPLOYMENT_NAME = \"llm-mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Create the Instructor node deployment (Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first deployment we will be creating is the one with the embedding model. This deployment will instruct the LLM how to \n",
    "answer the question properly, and search for relevant places that will be added to the user prompt. Doing this will \"guide\"\n",
    "the Mistral 7B model in the second deployment to a better answer. In order for the code inside the deployment to work properly\n",
    "we will need to add the Pinecone and Cohere API tokens as environment variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir prompt_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the `deployment.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prompt_node/deployment.py\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "import pinecone\n",
    "import os\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        print(\"Loading embeddings\")\n",
    "        embeddings = CohereEmbeddings(cohere_api_key=os.environ['COHERE_API_KEY'])\n",
    "        pinecone.init(api_key=os.environ['PINECONE_API_KEY'],\n",
    "              environment=\"gcp-starter\")\n",
    "        print(\"Searching through embeddings\")\n",
    "        self.docsearch = Pinecone.from_existing_index(index_name=\"ubiops-rag\", embedding=embeddings).as_retriever()\n",
    "\n",
    "        self.template = \"\"\"\n",
    "        <s> [INST]You are an expert in travelling around the world. A user asked you an advice for the trip. \n",
    "        Recommend him to go to {location}, also mention facts from following context. [/INST] </s> \n",
    "        [INST] Question: {question}\n",
    "        Context: {context} \n",
    "        Recomendation: [/INST]\n",
    "        \"\"\"\n",
    "\n",
    "    def request(self, data, context):\n",
    "\n",
    "        question = data[\"request\"]\n",
    "        print(\"Processing request\")\n",
    "        place = self.docsearch.get_relevant_documents(question)[0]\n",
    "        instruction = self.template.format(location=place.metadata['place'], context=place.page_content, question=question)\n",
    "        return {\"location\": place.metadata['place'], \"instruction\": instruction}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create the `requirements.txt` so specify the required additional dependencies for the code above to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prompt_node/requirements.txt\n",
    "langchain\n",
    "pinecone-client\n",
    "cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we create the deployment \n",
    "\n",
    "For the deployment we will specify the in- and output for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_template = ubiops.DeploymentCreate(\n",
    "    name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    description=\"A deployment to create prompts for mistral\",\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"request\", \"data_type\": \"string\"},\n",
    "    ],\n",
    "    output_fields=[\n",
    "        {\"name\": \"location\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"instruction\", \"data_type\": \"string\"},\n",
    "    ],\n",
    "    labels={\"controll\": \"prompt\"},\n",
    ")\n",
    "\n",
    "llm_deployment = api.deployments_create(project_name=PROJECT_NAME, data=embed_template)\n",
    "print(llm_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And finally we create the version\n",
    "\n",
    "Each deployment can have multiple versions. The version of a deployment defines the coding environment, instance type (CPU or GPU) \n",
    "& size, and other settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=\"v1\",\n",
    "    environment=\"python3-11\",\n",
    "    instance_type_group_name=\"256 MB + 0.0625 vCPU\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=1800,  # = 30 minutes\n",
    "    request_retention_mode=\"full\",  # input/output of requests will be stored\n",
    "    request_retention_time=3600,  # requests will be stored for 1 hour\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=\"instructor\", data=version_template\n",
    ")\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we need to turn the API keys from Cohere & Pinecone into environment variables so we can access\n",
    "them from inside the deployment code. This is done in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment variable for the Pinecone API token\n",
    "pinecone_api_key = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"PINECONE_API_KEY\", value=PINECONE_API_KEY, secret=True\n",
    ")\n",
    "\n",
    "api.deployment_version_environment_variables_create(\n",
    "    PROJECT_NAME,\n",
    "    deployment_name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    data=pinecone_api_key,\n",
    ")\n",
    "\n",
    "# Create an environment variable for the Cohere API token\n",
    "cohere_api_key = ubiops.EnvironmentVariableCreate(\n",
    "    name=\"COHERE_API_KEY\", value=COHERE_API_KEY, secret=True\n",
    ")\n",
    "\n",
    "\n",
    "api.deployment_version_environment_variables_create(\n",
    "    PROJECT_NAME,\n",
    "    deployment_name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    data=cohere_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we zip the `deployment package` and upload it to UbiOps (this process can take between 5-10 minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"prompt_node\", \"zip\", \".\", \"prompt_node\")\n",
    "\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    file=\"prompt_node.zip\",\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=EMBEDDING_DEPLOYMENT_NAME,\n",
    "    version=\"v1\",\n",
    "    revision_id=file_upload_result.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create the LLM node deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the deployment that will contain the LLM itself. As mentioned before we will be making use of the \n",
    "Mistral 7B Instruct. The workflow for creating this deployment is the same as the deployment for the embeddings: first we \n",
    "will create a `deployment.py`, then a `requirements.txt`, then the deployment (specifying the models input & output), and finish\n",
    "off with creating a version for this deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `deployment.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llm_model/deployment.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import transformers\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \n",
    "        token=os.environ['HF_TOKEN']\n",
    "\n",
    "        login(token=token)\n",
    "\n",
    "        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "        print(\"Loading model weights\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "        print(\"Loading model\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            return_full_text=False,\n",
    "            max_new_tokens=500)\n",
    "\n",
    "    def request(self, data, context):\n",
    "\n",
    "        result = self.pipeline(data[\"prompt\"])[0][\"generated_text\"]\n",
    "        print(\"Processing request\")\n",
    "        return {\"generated_text\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the `requirements.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llm_model/requirements.txt\n",
    "transformers\n",
    "torch\n",
    "bitsandbytes\n",
    "accelerate\n",
    "scipy\n",
    "huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_template = ubiops.DeploymentCreate(\n",
    "    name=\"llm-mistral\",\n",
    "    description=\"A deployment to run mistral\",\n",
    "    input_type=\"structured\",\n",
    "    output_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"prompt\", \"data_type\": \"string\"},\n",
    "    ],\n",
    "    output_fields=[{\"name\": \"generated_text\", \"data_type\": \"string\"}],\n",
    "    labels={\"controll\": \"llm\"},\n",
    ")\n",
    "\n",
    "llm_deployment = api.deployments_create(project_name=PROJECT_NAME, data=llm_template)\n",
    "print(llm_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a version for the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=\"v1\",\n",
    "    environment=\"ubuntu22-04-python3-11-cuda11-7-1\",\n",
    "    instance_type_group_name=\"16384 MB + 4 vCPU + NVIDIA Tesla T4\",\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=1800,  # = 30 minutes\n",
    "    request_retention_mode=\"full\",  # input/output of requests will be stored\n",
    "    request_retention_time=3600,  # requests will be stored for 1 hour\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=\"llm-mistral\", data=version_template\n",
    ")\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip & upload the files to UbiOps (this process can take between 5-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"llm_model\", \"zip\", \".\", \"llm_model\")\n",
    "\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=\"llm-mistral\",\n",
    "    version=\"v1\",\n",
    "    file=\"llm_model.zip\",\n",
    ")\n",
    "\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=\"llm-mistral\",\n",
    "    version=\"v1\",\n",
    "    revision_id=file_upload_result.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an environment variable for our Huggingface token, to be able to download the model from Huggingface.\n",
    "Make sure that the token has access to the gated repo from MistralAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=\"llm-mistral\",\n",
    "    version=\"v1\",\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"HF_TOKEN\", value=HF_TOKEN, secret=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Create a pipeline and pipeline version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a pipeline that orchastrates the workflow between the deployments above. When a request will be made to this pipeline\n",
    "the first deployment will search for a location according to the user's prompt, and will search for additional documents about\n",
    "this location. This information will then be send to the LLM which will generate text on why that location is worth visiting.\n",
    "\n",
    "For a pipeline you will have to define an input & output and create a version, as with a deployment. In addition to this we\n",
    "will also need to define the objects (i.e, deployments) and how to orchestrate the workflow (i.e., how to attach each object\n",
    " to each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"llm-generator\"\n",
    "PIPELINE_VERSION = \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_template = ubiops.PipelineCreate(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"A pipeline to prepare prompts, and generate text using Mistral\",\n",
    "    input_type=\"structured\",\n",
    "    input_fields=[\n",
    "        {\"name\": \"request\", \"data_type\": \"string\"},\n",
    "    ],\n",
    "    output_type=\"structured\",\n",
    "    output_fields=[\n",
    "        {\"name\": \"location\", \"data_type\": \"string\"},\n",
    "        {\"name\": \"generated_text\", \"data_type\": \"string\"},\n",
    "    ],\n",
    "    labels={\"demo\": \"mistral-RAG\"},\n",
    ")\n",
    "\n",
    "api.pipelines_create(project_name=PROJECT_NAME, data=pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the objects, and how to attach the objects together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [\n",
    "    # preprocessor\n",
    "    {\n",
    "        \"name\": EMBEDDING_DEPLOYMENT_NAME,\n",
    "        \"reference_name\": \"instructor\",\n",
    "        \"version\": \"v1\",\n",
    "    },\n",
    "    # LLM-model\n",
    "    {\"name\": LLM_DEPLOYMENT_NAME, \"reference_name\": \"llm-mistral\", \"version\": \"v1\"},\n",
    "]\n",
    "\n",
    "attachments = [\n",
    "    # start --> instruction-generator\n",
    "    {\n",
    "        \"destination_name\": \"instructor\",\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": \"pipeline_start\",\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"request\",\n",
    "                        \"destination_field_name\": \"request\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # instruction-generator --> LLM\n",
    "    {\n",
    "        \"destination_name\": \"llm-mistral\",\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": \"instructor\",\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"instruction\",\n",
    "                        \"destination_field_name\": \"prompt\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # LLm -> pipeline end, instruction-generator -> pipeline end\n",
    "    {\n",
    "        \"destination_name\": \"pipeline_end\",\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source_name\": \"instructor\",\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"location\",\n",
    "                        \"destination_field_name\": \"location\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"source_name\": \"llm-mistral\",\n",
    "                \"mapping\": [\n",
    "                    {\n",
    "                        \"source_field_name\": \"generated_text\",\n",
    "                        \"destination_field_name\": \"generated_text\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we create a version for this pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_template = ubiops.PipelineVersionCreate(\n",
    "    version=PIPELINE_VERSION,\n",
    "    request_retention_mode=\"full\",\n",
    "    objects=objects,\n",
    "    attachments=attachments,\n",
    ")\n",
    "\n",
    "api.pipeline_versions_create(\n",
    "    project_name=PROJECT_NAME, pipeline_name=PIPELINE_NAME, data=pipeline_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there you have it!\n",
    "\n",
    "That is all you need to know about how to set-up a RAG framework in UbiOps, using Langchain, Cohere, and Pinecone. If you  \n",
    "want you can use the code block below to create a request to your newly created pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api.pipeline_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    data={\"request\": \"A place in the Europe\"},\n",
    ")\n",
    "\n",
    "print(response.result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
