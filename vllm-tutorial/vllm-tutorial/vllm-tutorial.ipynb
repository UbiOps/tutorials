{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4efdb03dd6076d6",
   "metadata": {},
   "source": [
    "# Deploy a streaming LLM server on UbiOps with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a984093040a62b",
   "metadata": {},
   "source": [
    "In this tutorial, we will explain how to run your LLMs on UbiOps with [vLLM](https://github.com/vllm-project/vllm) by setting up a vLLM server in your deployment.\n",
    "vLLM is an LLM serving framework that implements several techniques to increase model throughput, and allows a single LLM\n",
    "to process multiple requests concurrently.\n",
    "\n",
    "In our example, we spin up an [OpenAI-compatible server](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html?ref=blog.mozilla.ai). The request method of the deployment is used to route request data to the `v1/chat/completions` endpoint of this  vLLM server,\n",
    "allowing for chatlike use cases.\n",
    "\n",
    "The deployment accepts input in the OpenAI chat completion format, and returns output in the same standard.\n",
    "\n",
    "For demo purposes, we will deploy a vLLM server that hosts the [`meta-llama/Llama-3.2-1B`](https://huggingface.co/meta-llama/Llama-3.2-1B). To follow along, ensure that \n",
    "your UbiOps subscription contains GPUs, and that you have a Huggingface tokens with sufficient permissions to read out Llama v3.2 1B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e71d87fb785c4",
   "metadata": {},
   "source": [
    "The following steps will be performed in this tutorial:\n",
    "\n",
    "1. Set up a connection with UbiOps\n",
    "2. Setup the environment\n",
    "3. Create a UbiOps deployment that deploys the server\n",
    "4. Initiate different requests to be handled by the deployment that hosts the vLLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5d9c6500a0764",
   "metadata": {},
   "source": [
    "## 1. Set up a connection with the UbiOps API client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127718fe99bd54b5",
   "metadata": {},
   "source": [
    "First, we will need to install the UbiOps Python Client Library to interface with UbiOps from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aadd285b13702d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU ubiops openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84065c88db038835",
   "metadata": {},
   "source": [
    "Now, we will need to initialize all the necessary variables for the UbiOps deployment and the deployment directory,\n",
    "which we will zip and upload to UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ce4ead8c1ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"<INSERT API TOKEN WITH PROJECT EDITOR RIGHTS>\"\n",
    "PROJECT_NAME = \"<INSERT YOUR PROJECT NAME>\"\n",
    "DEPLOYMENT_NAME = \"vllm-server\"\n",
    "DEPLOYMENT_VERSION = \"v1\"  # Choose a name for the version.\n",
    "\n",
    "HF_TOKEN = \"<ENTER YOUR HF TOKEN WITH ACCESS TO A LLAMA REPO HERE>\"  # We need this token to download the model from Huggingface \n",
    "\n",
    "print(f\"Your new deployment will be named: {DEPLOYMENT_NAME}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375693b8498e222",
   "metadata": {},
   "source": [
    "And let's initialize the UbiOps client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22879828eacef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ubiops\n",
    "\n",
    "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.CoreApi(client)\n",
    "api.service_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf1f21",
   "metadata": {},
   "source": [
    "And let's create a deployment package directory, where will add our [deployment package files](https://ubiops.com/docs/deployments/deployment-package/deployment-structure/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0398f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_name = \"deployment_package\"\n",
    "# Create directory for the deployment if it does not exist\n",
    "os.makedirs(dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20ca641499059d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830481539446a5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Setup deployment environment\n",
    "In order to use vLLM inside the deployment, we need to set up the environment of the deployment so that everything will run smoothly.  \n",
    "This will be done by specifying the `requirements.txt`.\n",
    "More information on these files can be found in the [UbiOps docs](https://ubiops.com/docs/environments/#uploading-dependency-information)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e2c1a86257ba4",
   "metadata": {},
   "source": [
    "All we need to do now is to create the `requirements.txt` file. Note that `vllm` automatically installs the CUDA drivers\n",
    "that are required to load the underlying model on a (NVIDIA) GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09493bd140d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dir_name}/requirements.txt\n",
    "vllm\n",
    "openai\n",
    "requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dir_name}/ubiops.yaml\n",
    "apt:\n",
    "  packages:\n",
    "    - build-essential\n",
    "    - python3-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ae378b0de3c9f",
   "metadata": {},
   "source": [
    "## 3. Creating UbiOps deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c616a063c7778f8",
   "metadata": {},
   "source": [
    "In this section, we will create the UbiOps deployment. \n",
    "This will be done by creating the deployment code that will run on UbiOps. \n",
    "We will furthermore archive the deployment directory and upload it to UbiOps. \n",
    "This will create a deployment and a version of the deployment on UbiOps and make it available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179488efd3eec4f",
   "metadata": {},
   "source": [
    "## 3.1 Creating deployment code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec03206eee54a0",
   "metadata": {},
   "source": [
    "### Creating Deployment Code for UbiOps\n",
    "\n",
    "We will now create the deployment code that will run on UbiOps. This involves creating a `deployment.py` file containing \n",
    "a `Deployment` class with two key methods:\n",
    "\n",
    "- **`__init__` Method**  \n",
    "  This method runs when the deployment starts. It can be used to load models, data artifacts, and other requirements for inference.\n",
    "\n",
    "- **`request()` Method**  \n",
    "  This method executes every time a call is made to the model's REST API endpoint. It contains the logic for processing incoming data.\n",
    "\n",
    "We will configure [`instance_processes`](https://ubiops.com/docs/requests/request-concurrency/#request-concurrency-per-instance) to 10, \n",
    "allowing each deployment instance to handle 10 concurrent requests. The model will be loaded as a background process within the `__init__` \n",
    "of the first process. A client will also be initialized in each process to proxy requests from all running processes to the host LLM.\n",
    "\n",
    "For a complete overview of the deployment code structure, refer to the [UbiOps documentation](https://ubiops.com/docs/deployments/deployment-package/deployment-structure/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe04bd3dbafb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dir_name}/deployment.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "import torch\n",
    "import json\n",
    "from openai import OpenAI, BadRequestError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class PublicError(Exception):\n",
    "    def __init__(self, public_error_message):\n",
    "        super().__init__()\n",
    "        self.public_error_message = public_error_message\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self, context):\n",
    "        self.model_name = os.getenv(\"MODEL_NAME\", \"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "        self.model_length = os.getenv(\"MAX_MODEL_LEN\", 10000)\n",
    "        self.vllm_gpu_memory_utilization = os.getenv(\"GPU_MEMORY_UTILIZATION\", 0.9)\n",
    "        self.context = context\n",
    "\n",
    "        if int(context[\"process_id\"]) == 0:\n",
    "            logging.info(\"Initializing vLLM server...\")\n",
    "            self.vllm_process = self.start_vllm_server()\n",
    "            self.poll_health_endpoint()\n",
    "\n",
    "        self.client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"...\")\n",
    "\n",
    "    def request(self, data, context):\n",
    "        \"\"\"\n",
    "        Processes incoming requests using the vLLM OpenAI-compatible API.\n",
    "        \"\"\"\n",
    "        logging.info(\"Processing request\")\n",
    "        input_data = json.loads(data)\n",
    "        stream_boolean = input_data.get(\"stream\", False)  # Default to streaming\n",
    "        input_data[\"model\"] = self.model_name # Here we overwrite the model key to always match the only model in this deployment. \n",
    "                                              # This is optional and a design choice.\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(**input_data)\n",
    "        except BadRequestError as e:\n",
    "            raise PublicError(str(e))\n",
    "\n",
    "        if stream_boolean:\n",
    "            streaming_callback = context[\"streaming_update\"]\n",
    "            full_response = []\n",
    "            for partial_response in response:\n",
    "                chunk_dump = partial_response.model_dump()\n",
    "                streaming_callback(json.dumps(chunk_dump))\n",
    "                full_response.append(chunk_dump)\n",
    "            return full_response\n",
    "        return response.model_dump()\n",
    "\n",
    "    def start_vllm_server(self):\n",
    "        \"\"\"\n",
    "        Starts the vLLM server in a subprocess.\n",
    "        \"\"\"\n",
    "        self.vllm_path = find_executable(\"vllm\")\n",
    "        vllm_process = subprocess.Popen([\n",
    "            self.vllm_path, \"serve\", self.model_name,\n",
    "            \"--max_model_len\", str(self.model_length),\n",
    "            \"--gpu-memory-utilization\", str(self.vllm_gpu_memory_utilization),\n",
    "            \"--tensor-parallel-size\", str(torch.cuda.device_count()),\n",
    "            \"--max-log-len\", str(1000),\n",
    "        ])\n",
    "        logging.info(\"Starting vLLM server\")\n",
    "        return vllm_process\n",
    "\n",
    "    def poll_health_endpoint(self):\n",
    "        \"\"\"\n",
    "        Polls the /health endpoint to ensure the vLLM server is ready.\n",
    "        \"\"\"\n",
    "        logging.info(\"Waiting for vLLM server to be ready...\")\n",
    "        while True:\n",
    "            poll = self.vllm_process.poll()\n",
    "            if poll is not None:\n",
    "                logging.error(\"vLLM server process terminated unexpectedly.\")\n",
    "                raise RuntimeError(f\"vLLM server exited with code: {poll}\")\n",
    "            try:\n",
    "                resp = requests.get('http://localhost:8000/health', timeout=5)\n",
    "                if resp.status_code == 200:\n",
    "                    logging.info(\"vLLM server is ready\")\n",
    "                    break\n",
    "                else:\n",
    "                    logging.warning(f\"Unexpected status code: {resp.status_code}. Retrying...\")\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                time.sleep(5)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Request failed: {e}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "\n",
    "def find_executable(executable_name):\n",
    "    \"\"\"\n",
    "    Find the path to the executable.\n",
    "    \"\"\"\n",
    "    path = subprocess.run(['which', executable_name], capture_output=True, text=True, check=True).stdout.strip()\n",
    "    if os.path.isfile(path) and os.access(path, os.X_OK):\n",
    "        logging.info(f\"Found {executable_name} at: {path}\")\n",
    "        return path\n",
    "    raise FileNotFoundError(f\"{executable_name} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db837e4b2946a6ee",
   "metadata": {},
   "source": [
    "### 3.2 Create UbiOps deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17755684faf0329d",
   "metadata": {},
   "source": [
    "Now we can create the deployment, where we define the in- and outputs of the model. We set them of type `plain`, to support openai compatible payloads.\n",
    "\n",
    "Each deployment can have multiple versions. For each version you can deploy different code, environments, instance types etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4c39f319180b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data={\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"description\": \"vLLM deployment\",\n",
    "        \"input_type\": \"plain\",\n",
    "        \"output_type\": \"plain\",\n",
    "    }\n",
    ")\n",
    "print(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f8606e9a99bef",
   "metadata": {},
   "source": [
    "### 3.3 Create a deployment version\n",
    "Next we create a version for the deployment. For the version we set the name, environment and size of the instance (we're using a GPU instance here, check if the instance type specified here is available in your project!).\n",
    "\n",
    "We add labels such that our model will be shown using the [OpenAI compatible `/models` endpoint](https://ubiops.com/docs/requests/openai/#listing-available-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311bcf0b4c90a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_template = {\n",
    "    \"version\": DEPLOYMENT_VERSION,\n",
    "    \"environment\": \"python3-12\",\n",
    "    \"instance_type_group_name\": \"16384 MB + 4 vCPU + NVIDIA Ada Lovelace L4\",\n",
    "    \"maximum_instances\": 1,\n",
    "    \"minimum_instances\": 0,\n",
    "    \"instance_processes\": 10,\n",
    "    \"maximum_idle_time\": 900,\n",
    "    \"labels\": {\n",
    "        \"openai-compatible\": \"true\",\n",
    "        \"openai-model-names\": MODEL_NAME\n",
    "    }\n",
    "}\n",
    "\n",
    "deployment_version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template,\n",
    ")\n",
    "\n",
    "print(deployment_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe41c6",
   "metadata": {},
   "source": [
    "Here we create environment variables for the Huggingface token.\n",
    "We need this token to allow us to download models from gated HuggingFace repos.\n",
    "The standard model used in this deployment is `meta-llama/Meta-Llama-3.2-1B`.\n",
    "This model is available in a gated HuggingFace repo, so we need to provide the token to access it.\n",
    "If you want to use a different model, you can change the deployment code or add an `MODEL_NAME` environment variable by using similar code as the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api.deployment_version_environment_variables_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=ubiops.EnvironmentVariableCreate(name=\"HF_TOKEN\", value=HF_TOKEN, secret=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b77a2c5f26e7",
   "metadata": {},
   "source": [
    "### 3.4 Archive deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43bd64c2395fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Archive the deployment directory\n",
    "deployment_zip_path = shutil.make_archive(dir_name, 'zip', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e6c13e0b27590",
   "metadata": {},
   "source": [
    "### 3.5 Upload deployment\n",
    "We will now upload the deployment to UbiOps. In the background, This step will take some time, because UbiOps interprets\n",
    "the environment files and builds a docker container out of it. You can check the UI for any progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc988fc7a9419f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_response = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=deployment_zip_path,\n",
    ")\n",
    "print(upload_response)\n",
    "\n",
    "# Check if the deployment is finished building. This can take a few minutes\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=upload_response.revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b825c9fb039bc",
   "metadata": {},
   "source": [
    "## 4. Making requests to the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909dacad2a2b252",
   "metadata": {},
   "source": [
    "Our deployment is now live on UbiOps! Let's test it out by sending a bunch of requests to it.\n",
    "This request will be a simple prompt to the model, asking it to respond to a question.\n",
    "In case your deployment still needs to scale, it may take some time before your first request is picked up. You can check\n",
    "the logs of your deployment version to see if the vLLM server is ready to accept requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "request_template = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"{question}\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "questions = [\n",
    "    \"What is the weather like today?\",\n",
    "    \"How do I cook pasta?\",\n",
    "    \"Can you explain quantum physics?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I learn Python?\"\n",
    "]\n",
    "\n",
    "requests_data = []\n",
    "for question in questions:\n",
    "    filled_request = request_template.copy()\n",
    "    filled_request['messages'][1]['content'] = question\n",
    "    requests_data.append(filled_request)\n",
    "\n",
    "print(json.dumps(requests_data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a85f777b0a7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_plain_batch = [json.dumps(item) for item in requests_data]\n",
    "\n",
    "requests = api.batch_deployment_requests_create(\n",
    "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=send_plain_batch, timeout=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66689fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=requests_data[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# wait until q\n",
    "request_ids = [request.id for request in requests]\n",
    "\n",
    "while True:\n",
    "    request_statuses = [request.status for request in\n",
    "                        api.deployment_requests_batch_get(PROJECT_NAME, DEPLOYMENT_NAME, request_ids)]\n",
    "    if all(request_status == \"completed\" for request_status in request_statuses):\n",
    "        print(\"All requests handled succesfully!\")\n",
    "        break\n",
    "    if any(request_status == \"failed\" for request_status in request_statuses):\n",
    "        print(\"A request failed!\")\n",
    "        break\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27812f81",
   "metadata": {},
   "source": [
    "From the request start times, you can infer that all requests were processed simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_start_times = [request.time_started for request in\n",
    "                       api.deployment_requests_batch_get(PROJECT_NAME, DEPLOYMENT_NAME, request_ids)]\n",
    "\n",
    "for index, time in enumerate(request_start_times, start=1):\n",
    "    print(f\"{index}. {time.strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01636251a2310f2",
   "metadata": {},
   "source": [
    "### Sending a request with streaming output\n",
    "\n",
    "For this request, we will add the key `stream: true` to the input, enabling streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769733ea52cec1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Can you stream your response?\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "# Create a streaming deployment request\n",
    "for item in ubiops.utils.stream_deployment_request(\n",
    "        client=api.api_client,\n",
    "        project_name=PROJECT_NAME,\n",
    "        deployment_name=DEPLOYMENT_NAME,\n",
    "        version=DEPLOYMENT_VERSION,\n",
    "        data=request_data,\n",
    "        timeout=3600,\n",
    "        full_response=False,\n",
    "):\n",
    "    item_dict = json.loads(item)\n",
    "    if item_dict.get(\"choices\"):\n",
    "        print(item_dict[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd0e2afeecb216",
   "metadata": {},
   "source": [
    "That's it! Even though the model itself claims it does not stream, we still ensured it did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd5fb911e03e0b",
   "metadata": {},
   "source": [
    "### OpenAI Endpoint\n",
    "\n",
    "Let's now focus on OpenAI compatibility. OpenAI-compatible endpoints are the standard way to interface with LLM servers. \n",
    "It makes integrations with other platforms much easier.\n",
    "\n",
    "Let's first set up an OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=API_TOKEN.lstrip(\"Token \"),  # This is the default and can be omitted\n",
    "    base_url=f\"https://api.ubiops.com/v2.1/projects/{PROJECT_NAME}/openai-compatible/v1/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_models = client.models.list()\n",
    "print(openai_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = openai_models.data[0].id\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b6a57",
   "metadata": {},
   "source": [
    "We can also connect to this deployment with the UbiOps OpenAI endpoint.\n",
    "Let's send the same messages, but through the OpenAI endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stream_var = False\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Can you tell me more about openai in exactly two lines\"}],\n",
    "    stream=stream_var\n",
    ")\n",
    "\n",
    "if stream_var:\n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, 'choices') and chunk.choices:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")  # Extract and print only the text\n",
    "else:\n",
    "    if hasattr(response, 'choices') and response.choices:\n",
    "        print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e8fd2123bb970",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029ad048c6de619",
   "metadata": {},
   "source": [
    "At last, let's close our connection to UbiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605724b98813ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0089bb",
   "metadata": {},
   "source": [
    "We have set up a deployment that hosts a vLLM server. This tutorial just serves as an example. Feel free to reach out to\n",
    "our [support portal](https://www.support.ubiops.com) if you want to discuss your set-up in more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
