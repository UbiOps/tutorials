{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-yetF3Srnk3"
      },
      "source": [
        "# Deploying a Stable Diffusion model to UbiOps\n",
        "\n",
        "This notebook will help you create a cloud-based inference API endpoint for Stable Diffusion, using UbiOps. The Stable Diffusion version we'll be using is already pretrained and will be loaded from the Huggingface StableDiffusion library. The model has been developed by CompVis.\n",
        "\n",
        "In this notebook we will walk you through:\n",
        "\n",
        "- Connecting with the UbiOps API client\n",
        "- Creating a code environment for our deployment\n",
        "- Creating a deployment for the Stable Diffusion model\n",
        "- Calling the Stable Diffusion deployment API endpoint\n",
        "\n",
        "Stable Diffusion is a text-to-image model. Therefore we will make a deployment which takes a text prompt as an input, and returns an image:\n",
        "\n",
        "\n",
        "|Deployment input & output variables| | |\n",
        "|--------------------|--------------|----|\n",
        "| | **Variable name**| **Data type**|\n",
        "| **Input fields** | prompt | string |\n",
        "| **Output fields** | image | file |\n",
        "\n",
        "Note that we deploy to a GPU instance by default . If you do not have GPUs available in your account, you can modify the code so that it runs on a CPU instance instead, by changing the instance type from `16384mb_t4` to `16384mb`.\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1O0t37LOYJh"
      },
      "source": [
        "\n",
        "## 1. Connecting with the UbiOps API client\n",
        "To use the UbiOps API from our notebook, we need to install the UbiOps Python client library, and some other packages that we will use for visualisation of the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBDhY41Hw6pz"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade ubiops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBQfwWt4rnk4"
      },
      "source": [
        "To set up a connection with the UbiOps platform API we need the name of your UbiOps project and an API token with `project-editor` permissions.\n",
        "\n",
        "Once you have your project name and API token, paste them below in the following cell before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcMtxyTvrnk4"
      },
      "outputs": [],
      "source": [
        "import ubiops\n",
        "from datetime import datetime\n",
        "\n",
        "API_TOKEN = '<API TOKEN>' # Make sure this is in the format \"Token token-code\"\n",
        "PROJECT_NAME = '<PROJECT_NAME>'    # Fill in your project name here\n",
        "\n",
        "DEPLOYMENT_NAME = f\"stable-diffusion-{datetime.now().date()}\"\n",
        "DEPLOYMENT_VERSION = 'gpu-t4'\n",
        "\n",
        "# Initialize client library\n",
        "configuration = ubiops.Configuration(host=\"https://api.ubiops.com/v2.1\")\n",
        "configuration.api_key['Authorization'] = API_TOKEN\n",
        "\n",
        "# Establish a connection\n",
        "client = ubiops.ApiClient(configuration)\n",
        "api = ubiops.CoreApi(client)\n",
        "print(api.projects_get(PROJECT_NAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0uBONwUtCdm"
      },
      "source": [
        "### Setting up the environment\n",
        "\n",
        "Our environment code contains instructions to install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-AR2GFxlRN4"
      },
      "outputs": [],
      "source": [
        "environment_dir = 'environment_package'\n",
        "ENVIRONMENT_NAME = 'stable-diffusion-environment-gpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yESMyTdek6Pu"
      },
      "outputs": [],
      "source": [
        "%mkdir {environment_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLPngTwatSvQ"
      },
      "source": [
        "We first write a requirements.txt file. This contains the Python packages that we will use in our deployment code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyybhdawk8ju"
      },
      "outputs": [],
      "source": [
        "%%writefile {environment_dir}/requirements.txt\n",
        "# This file contains package requirements for the environment\n",
        "# installed via PIP.\n",
        "diffusers\n",
        "transformers\n",
        "scipy\n",
        "torch==1.13.0+cu117\n",
        "accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goTGrRu1thaV"
      },
      "source": [
        "Next we add a `ubiops.yaml` to set a remote pip index. This ensures that we install a CUDA-compatible version of PyTorch. CUDA allows models to be loaded and to run GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBEfEOyHlJc9"
      },
      "outputs": [],
      "source": [
        "%%writefile {environment_dir}/ubiops.yaml\n",
        "environment_variables:\n",
        "- PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1laiUgRt5K9"
      },
      "source": [
        "Now we create a UbiOps `environment`. We select Python 3.11 with CUDA pre-installed as the `base environment` if we want to run on GPUs. If we run on CPUs, then we use `python3-11`.\n",
        "\n",
        "Our additional dependencies are installed on top of this base environment, to create our new `custom_environment` called `stable-diffusion-environment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnFTXmNll5I0"
      },
      "outputs": [],
      "source": [
        "api_response = api.environments_create(\n",
        "        project_name=PROJECT_NAME,\n",
        "        data=ubiops.EnvironmentCreate(\n",
        "        name=ENVIRONMENT_NAME,\n",
        "        #display_name=ENVIRONMENT_NAME,\n",
        "        base_environment='python3-11-cuda', #use python3-11 when running on CPU\n",
        "        description='Environment to run Stable Diffusion from Huggingface',\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy_3Ih3Xs-At"
      },
      "source": [
        "Package and upload the environment instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqY61U8Us7S8"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "training_environment_archive = shutil.make_archive(environment_dir, 'zip', '.', environment_dir)\n",
        "api.environment_revisions_file_upload(\n",
        "        project_name=PROJECT_NAME,\n",
        "        environment_name=ENVIRONMENT_NAME,\n",
        "        file=training_environment_archive\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTlQ2iKy02n"
      },
      "source": [
        "## 3. Creating a deployment for the Stable Diffusion model\n",
        "\n",
        "Now that we have created our code environment in UbiOps, it is time to write the actual code to run the Stable Diffusion model and push it to UbiOps.\n",
        "\n",
        "As you can see we're uploading a `deployment.py` file with a `Deployment` class and two methods:\n",
        "- `__init__` will run when the deployment starts up and can be used to load models, data, artifacts and other requirements for inference.\n",
        "- `request()` will run every time a call is made to the model REST API endpoint and includes all the logic for processing data.\n",
        "\n",
        "Separating the logic between the two methods will ensure fast model response times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m75uYrAYmk9s"
      },
      "outputs": [],
      "source": [
        "deployment_code_dir = 'deployment_code'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOqhWrwpy9Sn"
      },
      "outputs": [],
      "source": [
        "!mkdir {deployment_code_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGKxscOizDa9"
      },
      "outputs": [],
      "source": [
        "%%writefile {deployment_code_dir}/deployment.py\n",
        "\n",
        "\"\"\"\n",
        "The file containing the deployment code needs to be called 'deployment.py' and should contain a 'Deployment'\n",
        "class a 'request' method.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "class Deployment:\n",
        "\n",
        "    def __init__(self, base_directory, context):\n",
        "        \"\"\"\n",
        "        Initialisation method for the deployment. Any code inside this method will execute when the deployment starts up.\n",
        "        It can for example be used for loading modules that have to be stored in memory or setting up connections.\n",
        "        \"\"\"\n",
        "        model_id = os.environ['model_id']\n",
        "        gpu_available = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda\") if gpu_available else torch.device(\"cpu\")\n",
        "\n",
        "        self.pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if gpu_available else torch.float32)\n",
        "        self.pipe = self.pipe.to(device)\n",
        "\n",
        "        print(\"Initialising deployment\")\n",
        "\n",
        "\n",
        "    def request(self, data):\n",
        "        \"\"\"\n",
        "        Method for deployment requests, called separately for each individual request.\n",
        "        \"\"\"\n",
        "        image = self.pipe(data[\"prompt\"]).images[0]\n",
        "        print(\"Saving result\")\n",
        "        image.save(\"result.png\")\n",
        "        # here we set our output parameters in the form of a json\n",
        "        return {\"image\": \"result.png\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkn-xQGyrzpp"
      },
      "source": [
        "### Create a UbiOps deployment\n",
        "\n",
        "Create a deployment. Here we define the in- and outputs of a model. We can create different deployment versions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUbHzsSbyhgC"
      },
      "outputs": [],
      "source": [
        "# Create the deployment\n",
        "deployment_template = ubiops.DeploymentCreate(\n",
        "    name=DEPLOYMENT_NAME,\n",
        "    input_type='structured',\n",
        "    output_type='structured',\n",
        "    input_fields=[{'name': 'prompt', 'data_type': 'string'}],\n",
        "    output_fields=[{'name': 'image', 'data_type': 'file'}]\n",
        ")\n",
        "\n",
        "api.deployments_create(project_name=PROJECT_NAME, data=deployment_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gou_ZJxZrnk9"
      },
      "source": [
        "### Create a deployment version\n",
        "\n",
        "Now we will create a version of the deployment. For the version we need to define the name, the environment, the type of instance (CPU or GPU) as well the size of the instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IQDht-Krnk-"
      },
      "outputs": [],
      "source": [
        "# Let's first create the version\n",
        "version_template = ubiops.DeploymentVersionCreate(\n",
        "    version=DEPLOYMENT_VERSION,\n",
        "    environment=ENVIRONMENT_NAME,\n",
        "    instance_type_group_name='16384 MB + 4 vCPU + NVIDIA Tesla T4', # You can use '16384 MB + 4 vCPU' if you run on CPU\n",
        "    maximum_instances=1,\n",
        "    minimum_instances=0,\n",
        "    maximum_idle_time=600, # = 10 minutes\n",
        "    request_retention_mode='full'\n",
        ")\n",
        "\n",
        "api.deployment_versions_create(project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=version_template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnD5H2onrnk-"
      },
      "source": [
        "Package and upload the code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7x9Gi7xOHSf"
      },
      "outputs": [],
      "source": [
        "# And now we zip our code (deployment package) and push it to the version\n",
        "\n",
        "import shutil\n",
        "deployment_code_archive = shutil.make_archive(deployment_code_dir, 'zip', deployment_code_dir)\n",
        "\n",
        "upload_response = api.revisions_file_upload(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    version=DEPLOYMENT_VERSION,\n",
        "    file= deployment_code_archive\n",
        ")\n",
        "print(upload_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can only send requests to our deployment version, after our environment has finished building. \n",
        "\n",
        "NOTE: Building the environment might take a while as we need to download and install all the packages and dependencies. We only need to build our environment once: next time that we spin up an instance of our deployment, we won't need to install all dependencies anymore. Toggle off `stream_logs` to not stream logs of the build process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create an environment variable\n",
        "\n",
        "Here we create an environment variable for the `model_id`, which is used to specify which model will be downloaded from Huggingface. If you want to use another version of the Stable Diffusion model you can replace the value of `MODEL_ID` in the cell below, with the `model_id` of the model that you would like to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"runwayml/stable-diffusion-v1-5\" # You can change this parameter if you want to use a different model from Huggingface.\n",
        "\n",
        "api_response = api.deployment_environment_variables_create(\n",
        "    project_name=PROJECT_NAME,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    data=ubiops.EnvironmentVariableCreate(\n",
        "        name='model_id',\n",
        "        value= MODEL_ID, \n",
        "        secret=False\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ubiops.utils.wait_for_deployment_version(api.api_client, \n",
        "                                        project_name=PROJECT_NAME, \n",
        "                                        deployment_name=DEPLOYMENT_NAME, \n",
        "                                        version=DEPLOYMENT_VERSION,\n",
        "                                        stream_logs = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZCNhtuprnk-"
      },
      "source": [
        "# 4. Calling the Stable Diffusion deployment API endpoint\n",
        "\n",
        "Our deployment is now ready to be requested! We can send requests to it via the `deployment-requests-create` or the `batch-deployment-requests-create` API endpoint. It is going to take some time before the request finishes. When our deployment first loads, a GPU node will need to spin up, and we will need to download the Stable Diffusion model from HuggingFace. Subsequent results to the deployment will be handled faster. We will use a batch request to kick off our instance. This way, we can stream the on-start logs, and monitor the progress of the request using the `ubiops.utils` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [{\n",
        "    \"prompt\": \"cyberpunk eiffel tower\",\n",
        "}]\n",
        "\n",
        "response = api.batch_deployment_requests_create(\n",
        "    project_name=PROJECT_NAME, deployment_name=DEPLOYMENT_NAME, data=data\n",
        ")\n",
        "response_id = response[0].id \n",
        "print(response[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ubiops.utils.wait_for_deployment_request(api.api_client, \n",
        "                                         project_name = PROJECT_NAME, \n",
        "                                         deployment_name= DEPLOYMENT_NAME, \n",
        "                                         request_id = response_id,\n",
        "                                         stream_logs = True\n",
        "                                         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now retrieve the result of our image and visualise it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_uri = api.deployment_requests_get(PROJECT_NAME, DEPLOYMENT_NAME, response_id).result[\"image\"]\n",
        "\n",
        "ubiops.utils.download_file(client,\n",
        "                        PROJECT_NAME,\n",
        "                        file_uri = file_uri,\n",
        "                        output_path='result.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBJ2j_DF5QWr"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# Provide the path to your image\n",
        "image_path = 'result.png'\n",
        "\n",
        "# Display the image\n",
        "Image(filename=image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SctNpKvZoaBy"
      },
      "source": [
        "So that's it! You now have your own on-demand, scalable Stable Diffusion model running in the cloud, with a REST API that you can reach from anywhere!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
