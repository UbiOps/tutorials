{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Whisper on UbiOps with vLLM and Services\n",
    "\n",
    "In this tutorial, we will deploy OpenAI's Whisper model on UbiOps using vLLM's optimized serving framework. We'll expose the model through UbiOps Services, which allows direct HTTP access to the vLLM server's OpenAI-compatible API endpoints.\n",
    "\n",
    "## What are UbiOps Services?\n",
    "\n",
    "[UbiOps Services](https://ubiops.com/docs/services/) let you expose your deployments through custom HTTP endpoints. Unlike standard UbiOps deployment endpoints that follow the UbiOps API request/response structure, Services enable you to send direct HTTP requests to your deployments.\n",
    "\n",
    "In this tutorial, we'll run a vLLM server using a [deployment package](https://ubiops.com/docs/deployments/deployment-package/deployment-structure/) and expose it directly via a Service. This allows us to use the standard [OpenAI transcription API format](https://platform.openai.com/docs/guides/speech-to-text) without any UbiOps API wrapper. Services provide automatic HTTPS and TLS certificate provisioning, load balancing across deployment replicas, and integration with UbiOps monitoring, logging, and permissions.\n",
    "\n",
    "## What is vLLM?\n",
    "\n",
    "[vLLM](https://docs.vllm.ai/) is an easy-to-use, high-performance inference and serving framework for Large Language Models and audio models.\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "We will set up a connection with UbiOps, configure the deployment environment, create [deployment code](https://ubiops.com/docs/deployments/) that starts a vLLM server, deploy to UbiOps on a T4 GPU instance you can do so by going to Project Settings > Instance type (group) to see what instances you have enabled, create a Service to expose the vLLM API, and test transcription.\n",
    "\n",
    "For demo purposes, we will deploy a vLLM server that hosts the [openai/whisper-small](https://huggingface.co/openai/whisper-small) model. To follow along, ensure that your UbiOps subscription contains GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a connection with the UbiOps API client\n",
    "\n",
    "First, we'll install the [UbiOps Python Client Library](https://ubiops.com/docs/python_client_library/) and initialize our connection to UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU ubiops openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will need to initialize all the necessary variables for the UbiOps deployment and the deployment directory, which we will zip and upload to UbiOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "API_TOKEN = \"<INSERT API TOKEN WITH PROJECT EDITOR RIGHTS>\"\n",
    "PROJECT_NAME = \"<INSERT YOUR PROJECT NAME>\"\n",
    "API_HOST_URL = \"<INSERT YOUR HOST API URL>\" # Standard UbiOps API URL is 'https://api.ubiops.com/v2.1', your URL may differ depending on your environment\n",
    "\n",
    "DEPLOYMENT_NAME = \"whisper-vllm\"\n",
    "DEPLOYMENT_VERSION = \"v1\"\n",
    "SERVICE_NAME = \"whisper-service\"\n",
    "\n",
    "print(f\"Your deployment will be named: {DEPLOYMENT_NAME}\")\n",
    "print(f\"Your service will be named: {SERVICE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize UbiOps client\n",
    "import ubiops\n",
    "\n",
    "configuration = ubiops.Configuration(host=f\"{API_HOST_URL}\")\n",
    "configuration.api_key[\"Authorization\"] = API_TOKEN\n",
    "\n",
    "client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.CoreApi(client)\n",
    "\n",
    "# Test connection\n",
    "api.service_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment package directory\n",
    "import os\n",
    "\n",
    "dir_name = \"deployment_package\"\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "print(f\"Created directory: {dir_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup deployment environment\n",
    "\n",
    "We'll configure the deployment environment with the necessary dependencies and system packages. This is done through two files: [requirements.txt](https://ubiops.com/docs/howto/howto-requirements-txt/) for Python packages and [ubiops.yaml](https://ubiops.com/docs/environments/ubiops-yaml/) for system-level configuration.\n",
    "\n",
    "### requirements.txt\n",
    "\n",
    "The requirements.txt file specifies Python packages to install. We use `vllm[audio]` which includes vLLM with audio processing dependencies like librosa and soundfile. The `openai` package is included for testing the OpenAI-compatible API, and `requests` is used for HTTP requests and health checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dir_name}/requirements.txt\n",
    "vllm[audio]\n",
    "openai\n",
    "requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ubiops.yaml\n",
    "\n",
    "The [ubiops.yaml](https://ubiops.com/docs/environments/ubiops-yaml/) file configures system-level dependencies and environment variables. We install `build-essential` and `python3-dev` for C/C++ compilation tools needed by some Python packages, and `ffmpeg` for audio processing which Whisper uses for audio resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dir_name}/ubiops.yaml\n",
    "apt:\n",
    "  packages:\n",
    "    - build-essential\n",
    "    - python3-dev\n",
    "    - ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating UbiOps deployment code\n",
    "\n",
    "### Understanding the Deployment Class\n",
    "\n",
    "The [deployment code](https://ubiops.com/docs/deployments/deployment-package/deployment-structure/) consists of a `Deployment` class with two key methods. The `__init__` method runs once when the deployment starts and is used to set up environment variables for vLLM, start the vLLM server as a subprocess, and wait for the server to be healthy before accepting requests. The `request()` method acts as a placeholder that returns server health status if called directly. When using Services, requests go directly to the vLLM server and bypass this method entirely.\n",
    "\n",
    "### vLLM Server Configuration\n",
    "\n",
    "The vLLM server is started with several flags. The `--task transcription` flag configures vLLM specifically for audio transcription, which is required for Whisper models as explained in the [vLLM transcription documentation](https://docs.vllm.ai/en/latest/contributing/model/transcription/). We use `--dtype float16` for FP16 precision which is neccessary for this gpu due to the older architecture that it's built on. The `--max-model-len 448` argument in vLLM, when used with transcription models like Whisper, sets the maximum sequence length (in tokens) that the model can process for a single request. We set `--gpu-memory-utilization 0.9` to use 90% of available GPU memory. The `--host 0.0.0.0 --port 8080` flags expose the server on port 8080, which is required for UbiOps Services to connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dir_name}/deployment.py\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "    def __init__(self, base_directory, context):\n",
    "        os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"TRITON_ATTN\"\n",
    "        \n",
    "        # Model configuration\n",
    "        self.model_name = \"openai/whisper-large-v3-turbo\"\n",
    "        self.task = os.getenv(\"WHISPER_TASK\", \"transcription\")  # or \"translation\", if the model supports it\n",
    "        self.max_model_len = int(os.getenv(\"MAX_MODEL_LEN\", \"448\"))\n",
    "\n",
    "        # Start vLLM server\n",
    "        logging.info(\"Initializing vLLM server for Whisper...\")\n",
    "        self.vllm_process = self.start_vllm_server()\n",
    "        self.wait_for_server()\n",
    "        logging.info(\"vLLM Whisper server is ready!\")\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Placeholder request method - returns server health status.\n",
    "        When using Services, requests go directly to the vLLM server.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            resp = requests.get('http://localhost:8080/health', timeout=5)\n",
    "            return {\"status\": \"healthy\", \"status_code\": resp.status_code}\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"unhealthy\", \"error\": str(e)}\n",
    "\n",
    "    def start_vllm_server(self):\n",
    "        \"\"\"\n",
    "        Starts the vLLM server for Whisper in a subprocess.\n",
    "        \"\"\"\n",
    "        vllm_path = self.find_executable(\"vllm\")\n",
    "        \n",
    "        # Build vLLM command\n",
    "        vllm_cmd = [\n",
    "            vllm_path, \"serve\",\n",
    "            self.model_name,\n",
    "            \"--task\", self.task,\n",
    "            \"--max-model-len\", str(self.max_model_len),\n",
    "            \"--gpu-memory-utilization\", \"0.9\",\n",
    "            \"--dtype\", \"float16\",\n",
    "            \"--tensor-parallel-size\", str(torch.cuda.device_count()),\n",
    "            \"--host\", \"0.0.0.0\",\n",
    "            \"--port\", \"8080\"  # Service will connect to this port, you can modify it accordingly \n",
    "        ]\n",
    "        \n",
    "        logging.info(f\"Starting vLLM server: {' '.join(vllm_cmd)}\")\n",
    "        vllm_process = subprocess.Popen(vllm_cmd)\n",
    "        logging.info(\"vLLM server starting...\")\n",
    "        \n",
    "        return vllm_process\n",
    "\n",
    "    def wait_for_server(self):\n",
    "        \"\"\"\n",
    "        Wait until the vLLM server is ready to accept requests.\n",
    "        \"\"\"\n",
    "        logging.info(\"Waiting for vLLM server to be ready...\")\n",
    "        max_retries = 60\n",
    "        \n",
    "        for retry_count in range(max_retries):\n",
    "            # Check if process crashed\n",
    "            poll = self.vllm_process.poll()\n",
    "            if poll is not None:\n",
    "                logging.error(\"vLLM server process terminated unexpectedly.\")\n",
    "                raise RuntimeError(f\"vLLM server exited with code: {poll}\")\n",
    "            \n",
    "            # Try health check\n",
    "            try:\n",
    "                resp = requests.get('http://localhost:8080/health', timeout=5)\n",
    "                if resp.status_code == 200:\n",
    "                    logging.info(\"vLLM server is ready!\")\n",
    "                    return\n",
    "            except requests.exceptions.RequestException:\n",
    "                time.sleep(5)\n",
    "        \n",
    "        raise RuntimeError(\"vLLM server failed to start within timeout period\")\n",
    "\n",
    "    @staticmethod\n",
    "    def find_executable(executable_name):\n",
    "        \"\"\"\n",
    "        Find the path to the vLLM executable.\n",
    "        \"\"\"\n",
    "        result = subprocess.run(\n",
    "            ['which', executable_name], \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            check=False\n",
    "        )\n",
    "        path = result.stdout.strip()\n",
    "        \n",
    "        if path and os.path.isfile(path) and os.access(path, os.X_OK):\n",
    "            logging.info(f\"Found {executable_name} at: {path}\")\n",
    "            return path\n",
    "        \n",
    "        raise FileNotFoundError(f\"{executable_name} not found in PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create UbiOps deployment\n",
    "\n",
    "### Deployment Configuration\n",
    "\n",
    "We create a [deployment](https://ubiops.com/docs/deployments/) with `input_type: \"plain\"` and `output_type: \"plain\"` to accept and return JSON data. However, when accessed through Services, these input/output types don't matter because requests bypass the UbiOps API structure and go directly to the vLLM server.\n",
    "\n",
    "If you try the `request()` method it will return the server health status if called directly as we configured it earlier in the `deployment.py` file. When using Services, requests go directly to the vLLM server and bypass this method entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment\n",
    "deployment = api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data={\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"description\": \"Whisper small with vLLM\",\n",
    "        \"input_type\": \"plain\",\n",
    "        \"output_type\": \"plain\",\n",
    "    }\n",
    ")\n",
    "print(f\"Created deployment: {deployment.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Version Configuration\n",
    "\n",
    "We create a [deployment version](https://ubiops.com/docs/deployments/deployment-versions/) with specific settings. We use Python 3.12 as the runtime environment and, for example, an NVIDIA T4 (Tesla Architecture). We set `maximum_instances: 1` and `minimum_instances: 0` to allow the deployment to scale to zero when idle, saving costs. The `maximum_idle_time: 900` keeps the instance alive for 15 minutes after the last request. We add labels to mark the deployment as OpenAI-compatible for easier discovery.\n",
    "\n",
    "**Note** : Make sure you have that instance type by going to Project Settings > Instance type (group) pages to check what compute you have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment version\n",
    "version_template = {\n",
    "    \"version\": DEPLOYMENT_VERSION,\n",
    "    \"environment\": \"python3-12\",\n",
    "    \"instance_type_group_name\": \"12288 MB + NVIDIA Tesla T4\",  # T4 GPU instance\n",
    "    \"maximum_instances\": 1,\n",
    "    \"minimum_instances\": 0,\n",
    "    \"maximum_idle_time\": 900,  # 15 minutes\n",
    "    \"labels\": {\n",
    "        \"openai-compatible\": \"true\",\n",
    "        \"openai-model-names\": \"openai/whisper-small\",\n",
    "        \"model-type\": \"speech-to-text\"\n",
    "    }\n",
    "}\n",
    "\n",
    "deployment_version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template,\n",
    ")\n",
    "\n",
    "print(f\"Created deployment version: {deployment_version.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding custom environment variables\n",
    "\n",
    "You can configure the deployment by adding environment variables such as `WHISPER_TASK` for \"transcription\" or \"translation\", or `MAX_MODEL_LEN` for audio sequence length (default: 448) which lets you modify the deployment without having to upload a revision. Note that no [HuggingFace](https://huggingface.co/) token is needed as Whisper models are public and ungated but one can be set up as an environment variable if you want to use a model that is available there.\n",
    "\n",
    "The critical environment variable here is `VLLM_ATTENTION_BACKEND=TRITON_ATTN`. This forces vLLM to use the [Triton attention backend](https://docs.vllm.ai/en/v0.10.1/api/vllm/v1/attention/backends/triton_attn.html), which is essential for Whisper's encoder-decoder cross-attention pattern. Without this setting, vLLM will fail to start on T4 GPUs with Whisper models due to incompatibility between the default attention backend and the encoder-decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_vars = [\n",
    "    {\"name\": \"VLLM_ATTENTION_BACKEND\", \"value\": \"TRITON_ATTN\"},\n",
    "    {\"name\": \"WHISPER_TASK\", \"value\": \"transcription\"},\n",
    "    {\"name\": \"MAX_MODEL_LEN\", \"value\": \"448\"}\n",
    "]\n",
    "\n",
    "for env_var in env_vars:\n",
    "    env_data = ubiops.EnvironmentVariableCreate(\n",
    "        name=env_var[\"name\"],\n",
    "        value=env_var[\"value\"],\n",
    "        secret=False\n",
    "    )\n",
    "    \n",
    "    api.deployment_version_environment_variables_create(\n",
    "        PROJECT_NAME,\n",
    "        DEPLOYMENT_NAME,\n",
    "        DEPLOYMENT_VERSION,\n",
    "        env_data\n",
    "    )\n",
    "    print(f\"Created environment variable: {env_var['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Archive and upload deployment\n",
    "\n",
    "Now we package and upload our deployment code to UbiOps. This will trigger a build process that installs all dependencies, and prepares the deployment for execution. Building can take 10-15 minutes due to the size of vLLM and the other dependencies.\n",
    "\n",
    "**Note:** To check the progress you can either check the UI, or uncomment `stream_logs=True` in the `wait_for` method to see the logs in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Archive the deployment directory\n",
    "deployment_zip_path = shutil.make_archive(dir_name, 'zip', dir_name)\n",
    "print(f\"Created archive: {deployment_zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload deployment package\n",
    "upload_response = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file=deployment_zip_path\n",
    ")\n",
    "print(f\"Upload started. Revision ID: {upload_response.revision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for deployment to be ready\n",
    "print(\"Waiting for deployment build to complete...\")\n",
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=upload_response.revision,\n",
    "    # stream_logs = True\n",
    ")\n",
    "print(\"Deployment is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Service to expose the vLLM API\n",
    "\n",
    "### Understanding Services\n",
    "\n",
    "Now we'll create a [UbiOps Service](https://ubiops.com/docs/services/) that exposes our vLLM server to the internet. The Service connects to port 8080 where vLLM is running, provides a public HTTPS endpoint, automatically handles TLS certificates, and load balances across deployment replicas.\n",
    "\n",
    "### Service Configuration\n",
    "\n",
    "The service requires a name, the deployment to connect to, the specific version to use, and the port number (8080) which must match the port vLLM listens on. After creation, the service will be accessible at `https://[service-id].services.ubiops.com`. This URL will route directly to your vLLM server's OpenAI-compatible API. We also specify the `health_check_path` to point to the `/health` endpoint so UbiOps can monitor service availability. For authentication, we configure the service to require a UbiOps API token passed in the `Authorization` header of requests. This ensures only authorized users can access the service, leveraging UbiOps' existing permission system to control who can make requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Service\n",
    "service = api.services_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data={\n",
    "        \"name\": SERVICE_NAME,\n",
    "        \"deployment\": DEPLOYMENT_NAME,\n",
    "        \"version\": DEPLOYMENT_VERSION,\n",
    "        \"port\": 8080,  # Port where vLLM server listens\n",
    "        \"health_check_path\" : \"/health\",\n",
    "        \"health_cheack_interval\" : 30,\n",
    "        \"request_storage_enabled\" : True,\n",
    "        \"authentication_required\" : True\n",
    "    }\n",
    ")\n",
    "\n",
    "SERVICE_URL = f\"https://{service.id}.services.ubiops.com\"\n",
    "\n",
    "print(f\"Service created: {service.name}\")\n",
    "print(f\"Service ID: {service.id}\")\n",
    "print(f\"\\n Service URL: {SERVICE_URL}\")\n",
    "print(f\"\\n Transcription endpoint: {SERVICE_URL}/v1/audio/transcriptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the transcription service\n",
    "\n",
    "Now let's test our Whisper service by sending audio files to the transcription endpoint and getting back the transcribed text.\n",
    "\n",
    "### Test 1: Check server health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"{API_TOKEN}\"\n",
    "}\n",
    "\n",
    "# Test health endpoint\n",
    "health_response = requests.get(f\"{SERVICE_URL}/health\", headers=headers)\n",
    "print(f\"Server health: {health_response.status_code}\")\n",
    "print(health_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Check available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available models\n",
    "models_response = requests.get(f\"{SERVICE_URL}/v1/models\", headers=headers)\n",
    "print(f\"Available models: {models_response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Transcribe an audio file\n",
    "\n",
    "For this test, you'll need an audio file in formats like .wav, .mp3, or .m4a. Replace `\"your_audio.wav\"` with the path to your audio file. If you don't have a file you can find one here: https://www.kaggle.com/datasets/pavanelisetty/sample-audio-files-for-speech-recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe audio file\n",
    "audio_file_path = \"your_audio.wav\"  # Replace with your audio file path\n",
    "\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    response = requests.post(\n",
    "        f\"{SERVICE_URL}/v1/audio/transcriptions\",\n",
    "        files={\"file\": (\"audio.wav\", audio_file, \"audio/wav\")},\n",
    "        headers=headers,\n",
    "        data={\n",
    "            \"model\": \"openai/whisper-small\",\n",
    "            \"language\": \"en\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Transcription result:\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: cURL command for transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cURL transcription test\n",
    "!curl -X POST \\\n",
    "  -H \"Authorization: {API_TOKEN}\" \\\n",
    "  -F \"file=@your_audio.wav\" \\\n",
    "  -F \"model=openai/whisper-small\" \\\n",
    "  -F \"language=en\" \\\n",
    "  -F \"response_format=json\" \\\n",
    "  {SERVICE_URL}/v1/audio/transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Using OpenAI Python client\n",
    "\n",
    "The service is OpenAI-compatible, so you can use the [official OpenAI Python client](https://platform.openai.com/docs/guides/speech-to-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client pointing to your service\n",
    "client = OpenAI(\n",
    "    api_key=\"dummy\",  # Not needed for UbiOps Services\n",
    "    base_url=f\"{SERVICE_URL}/v1\"\n",
    ")\n",
    "\n",
    "# Transcribe audio\n",
    "with open(\"your_audio.wav\", \"rb\") as audio_file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=\"openai/whisper-small\",\n",
    "        headers=headers,\n",
    "        file=audio_file,\n",
    "        language=\"en\"\n",
    "    )\n",
    "\n",
    "print(f\"Transcription: {transcription.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7: Multilingual transcription\n",
    "\n",
    "Here we can try and translate an audio in a different language by setting the `language` parameter. Whisper [supports 99 languages](https://github.com/openai/whisper#available-models-and-languages), the parameter is set to `es` for spanish but can be changed depending on your audio file and languages supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect language\n",
    "with open(\"your_audio.wav\", \"rb\") as audio_file:\n",
    "    response = requests.post(\n",
    "        f\"{SERVICE_URL}/v1/audio/transcriptions\",\n",
    "        files={\"file\": audio_file},\n",
    "        headers=headers,\n",
    "        data={\n",
    "            \"model\": \"openai/whisper-small\",\n",
    "            # No language specified - will auto-detect\n",
    "        }\n",
    "    )\n",
    "    result = response.json()\n",
    "    print(f\"Detected language: {result.get('language')}\")\n",
    "    print(f\"Text: {result.get('text')}\")\n",
    "\n",
    "# Transcribe Spanish audio\n",
    "# with open(\"spanish_audio.wav\", \"rb\") as audio_file:\n",
    "#     response = requests.post(\n",
    "#         f\"{SERVICE_URL}/v1/audio/transcriptions\",\n",
    "#         files={\"file\": audio_file},\n",
    "#         data={\n",
    "#             \"model\": \"openai/whisper-small\",\n",
    "#             \"language\": \"es\"  # Spanish\n",
    "#         }\n",
    "#     )\n",
    "#     print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing API Documentation with Browser Authentication\n",
    "\n",
    "The vLLM API documentation is available at `/docs` endpoint. However, since authentication is required at the UbiOps Service level, you'll need to inject the Authorization header using a browser extension.\n",
    "\n",
    "### Using Requestly Browser Extension\n",
    "\n",
    "[Requestly](https://requestly.io/) is a browser extension (available for Chrome, Firefox, Edge) that allows you to modify HTTP headers for specific URLs.\n",
    "\n",
    "**Steps:**\n",
    "1. Install the Requestly extension for your browser\n",
    "2. Choose HTTP Interceptor > Modify headers\n",
    "3. Configure the rule so that it includes your service URL ('services.ubiops.com')\n",
    "4. Add a Request Header and choose 'authorization', fill the Header Value with the UbiOps token in the form 'Token ...'\n",
    "5. Save the rule\n",
    "6. You can now navigate to the endpoints in your browser \n",
    "\n",
    "**Example configuration:**\n",
    "```\n",
    "URL Pattern: your-service-id.services.ubiops.com\n",
    "Header Name: Authorization\n",
    "Header Value: Token your-api-token-here\n",
    "```\n",
    "\n",
    "Once configured, you can access:\n",
    "- ReDoc: `{SERVICE_URL}/docs`\n",
    "\n",
    "\n",
    "**Note:** The exact workflow for creating rules differs per browser and Requestly version. Refer to [Requestly's documentation](https://docs.requestly.com/general/getting-started/introduction) for browser-specific instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "When you're done testing, scale down the deployment version to avoid extra charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raise SystemExit(\"Stopped from running all cells to avoid scaling down the deployment before completing all chapters.\\nYou can execute the next cells manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_template = ubiops.DeploymentVersionUpdate(\n",
    "    minimum_instances=0\n",
    ")\n",
    "\n",
    "deployment_version = api.deployment_versions_update(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=version_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can close the api client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close API client\n",
    "client.close()\n",
    "print(\"Closed UbiOps connection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
