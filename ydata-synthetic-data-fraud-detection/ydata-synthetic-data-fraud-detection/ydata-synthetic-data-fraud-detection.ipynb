{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with massively imbalanced datasets using YData and UbiOps\n",
    "This notebook will show you how to create a model from an imbalanced dataset and serve it on UbiOps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Fraud - a highy imbalanced dataset\n",
    "The dataset in this example use case is from Kaggle - [\"Credit Card Fraud detection\"](https://kaggle.com/mlg-ulb/creditcardfraud), as for demonstration purposes we are only able to use datasets from the public domain.\n",
    "This dataset includes labeled transactions from European credit card holders, and the data provided is a result from a dimensionality reduction, containing 27 continous features and a time column indicating the number of seconds elapsed between the first and the last transaction of the dataset.\n",
    "\n",
    "### Installing and importing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn==0.22.0 scipy==1.5.4 \\\n",
    "    xgboost==1.3.3 ubiops==3.15.0 ydata-synthetic==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything\n",
    "import xgboost as xgb\n",
    "import shutil\n",
    "import ubiops\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as m\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score, classification_report\n",
    "from ydata_synthetic.synthesizers.regular import WGAN_GP\n",
    "from xgboost import XGBClassifier\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download** the dataset [creditcard.csv](https://storage.googleapis.com/ubiops/data/creditcard.csv) and put it in your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "Split the dataset in train and test sets. The test set will be used again at the end of our iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit.drop('Class', axis=1)\n",
    "cols = X.columns\n",
    "X = X.values\n",
    "y = credit['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Number transactions train dataset: \", len(X_train))\n",
    "print(\"Number transactions test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_original = np.unique(y, return_counts=True)\n",
    "count_train = np.unique(y_train, return_counts=True)\n",
    "count_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "print(\"Ratio between fraud and normal events for the \\033[1mfull\\033[0m  dataset:\"+ \" {:.2}%\".format(count_train[1][1]/count_train[1][0]))\n",
    "print(\"Ratio between fraud and normal events for the \\033[1mtrain\\033[0m dataset:\"+ \" {:.2}%\".format(count_train[1][1]/count_train[1][0]))\n",
    "print(\"Ratio between fraud and normal events for the \\033[1mtest\\033[0m dataset:\"+\" {:.2}%\".format(count_test[1][1]/count_test[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first model\n",
    "Let's try to develop a model based on the assumption that everything is ok with our dataset, and understand how good our classifier is in indentifying fraudulent events.\n",
    "Here we've decided to develop a classifier using [RandomForest from the scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=cols)\n",
    "X_test = pd.DataFrame(X_test, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scaling and preprocessing before training the model\n",
    "\n",
    "def preprocess_df(df, std_scaler, rob_scaler):\n",
    "    df['Amount'] = std_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "    df['Time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler = StandardScaler()\n",
    "robscaler = RobustScaler()\n",
    "\n",
    "X_train = preprocess_df(X_train, stdscaler, robscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation to the test dataset\n",
    "X_test = preprocess_df(X_test, stdscaler, robscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XGBOOST model to train the model\n",
    "\n",
    "def XGBoost_Classifier(X, y, Xtest):\n",
    "    \"\"\"XGBoost training code\"\"\"\n",
    "    classifier = XGBClassifier()\n",
    "    print('Start fitting XGBoost classifier')\n",
    "    classifier.fit(X, y)\n",
    "    y_pred = classifier.predict(Xtest)\n",
    "    print('Classifier trained.')\n",
    "    return classifier, y_pred\n",
    "\n",
    "classifier_model, y_pred = XGBoost_Classifier(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check the real metrics for this classifier\n",
    "\n",
    "def print_confusion_matrix(model, X_test, y_test):\n",
    "    \"\"\" Plot normalized and non-normalized confusion matrices \"\"\"\n",
    "    titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                      (\"Normalized confusion matrix\", 'true')]\n",
    "\n",
    "    fig, axes = plt.subplots(1,2,figsize=(20,8))\n",
    "    for (title, normalize), ax in zip(titles_options, axes):\n",
    "\n",
    "        disp = plot_confusion_matrix(model, X_test, y_test,\n",
    "                                     display_labels=[\"Normal\", \"Fraud\"],\n",
    "                                     cmap=plt.cm.OrRd,\n",
    "                                     normalize=normalize,\n",
    "                                     ax=ax)\n",
    "\n",
    "        ax.set_title(title, fontsize=20, pad=10)\n",
    "\n",
    "print_confusion_matrix(classifier_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data to improve the detection of fraud\n",
    "\n",
    "### Synthetic data with YData synthesizer package\n",
    "\n",
    "In this case the objective is to synthesize only the fraudulent events. Through the augmentation of fraudulent events we are able to improve the results of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter by fraudulent events only\n",
    "aux = X_train.copy()\n",
    "aux['y'] = y_train.reset_index()['Class']\n",
    "\n",
    "non_fraud = aux[aux['y'] == 0]\n",
    "fraud = aux[aux['y']==1]\n",
    "\n",
    "del aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 164\n",
    "\n",
    "log_step = 100\n",
    "epochs = 1000+1\n",
    "learning_rate = 5e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.9\n",
    "models_dir = './cache'\n",
    "\n",
    "gan_args = [batch_size, learning_rate, beta_1, beta_2, noise_dim, fraud.shape[1]-1, dim]\n",
    "train_args = ['', epochs, log_step]\n",
    "\n",
    "fraud_synth = WGAN_GP(gan_args, n_critic=2)\n",
    "fraud_synth.train(fraud.drop('y', axis=1), train_args)\n",
    "\n",
    "synthetic_fraud = fraud_synth.sample(400)\n",
    "synthetic_fraud.columns = fraud.drop('y', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_fraud.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the classifier capacity after adding more fraudulent events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_df = synthetic_fraud.copy()\n",
    "org_df = X_train.copy()\n",
    "\n",
    "org_df['Class'] = y_train.reset_index()['Class']\n",
    "org_df['color'] = np.where(org_df['Class']==1, 2, 1)\n",
    "\n",
    "synth_df['Class'] = 1\n",
    "synth_df['color'] = 3\n",
    "\n",
    "full_data = pd.concat([org_df, synth_df])\n",
    "\n",
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_y_train = synth_df['Class']\n",
    "synth_train = synth_df.drop(['Class', 'color'], axis=1)\n",
    "\n",
    "X_augmented = pd.concat([X_train, synth_train], axis=0)\n",
    "y_augmented = pd.concat([y_train, synth_y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_classmodel, y_pred = XGBoost_Classifier(X_augmented, y_augmented, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix(synth_classmodel, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results have improved! Now let's continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model to UbiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for deployment\n",
    "\n",
    "joblib.dump(synth_classmodel, 'fraud_deployment/fraud_model.joblib') \n",
    "print('XGBoost model built and saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of the `deployment.py`. This is the code that runs on UbiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fraud_deployment/deployment.py\n",
    "\"\"\"\n",
    "The file containing the deployment code is required to be called 'deployment.py' and should contain the 'Deployment'\n",
    "class and 'request' method.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import load\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        print(\"Initialising xgboost model\")\n",
    "\n",
    "        XGBOOST_MODEL = os.path.join(base_directory, \"fraud_model.joblib\")\n",
    "        self.model = load(XGBOOST_MODEL)\n",
    "\n",
    "    def request(self, data):\n",
    "        print('Loading data')\n",
    "        input_data = pd.read_csv(data['input'])\n",
    "        \n",
    "        print(\"Prediction being made\")\n",
    "        prediction = self.model.predict(input_data)\n",
    "        \n",
    "        # Writing the prediction to a csv for further use\n",
    "        print('Writing prediction to csv')\n",
    "        pd.DataFrame(prediction).to_csv('prediction.csv', header = ['Class prediction'], index_label= 'index')\n",
    "        \n",
    "        return {\n",
    "            \"output\": 'prediction.csv'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the deployment package folder so it is ready to be uploaded to UbiOps\n",
    "\n",
    "shutil.make_archive('fraud_deployment', 'zip', '.', 'fraud_deployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to UbiOps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the UbiOps api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = 'Token 12344'\n",
    "PROJECT_NAME = 'YOUR PROJECT NAME'\n",
    "DEPLOYMENT_NAME = 'ydata-fraud-model'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "configuration = ubiops.Configuration()\n",
    "configuration.api_key['Authorization'] = API_TOKEN\n",
    "\n",
    "client = ubiops.ApiClient(configuration)\n",
    "api = ubiops.api.CoreApi(client)\n",
    "\n",
    "print(api.service_status())\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a deployment and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='XGBoost Fraud Detection',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[{'name':'input', 'data_type':'file'}],\n",
    "    output_fields=[{'name':'output', 'data_type':'file'}]\n",
    ")\n",
    "\n",
    "deployment = api.deployments_create(project_name=PROJECT_NAME, data=deployment_template)\n",
    "print(deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    environment='python3-7',\n",
    "    instance_type='512mb',\n",
    "    maximum_instances=1,\n",
    "    minimum_instances=0,\n",
    "    maximum_idle_time=1800, # = 30 minutes\n",
    "    request_retention_mode='none' # We don't need request storage in this example\n",
    ")\n",
    "\n",
    "version = api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_upload_result =api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='fraud_deployment.zip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployment after building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubiops.utils.wait_for_deployment_version(\n",
    "    client=api.api_client,\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    revision_id=file_upload_result.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the request data to csv\n",
    "test_data = X_test.to_csv('test_data.csv', index=False)\n",
    "\n",
    "# Upload the test_data.csv to UbiOps\n",
    "file_uri = ubiops.utils.upload_file(client, PROJECT_NAME, 'test_data.csv')\n",
    "\n",
    "# Make the request using the csv\n",
    "# Note this could take a minute because of the model cold start(first time the model is started).\n",
    "request_result = api.deployment_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data={'input': file_uri}\n",
    ")\n",
    "print(request_result)\n",
    "\n",
    "# Download the output file\n",
    "ubiops.utils.download_file(client, PROJECT_NAME, file_uri=request_result.result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done! Let's close the client properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have succesfully trained a model on syntethic data using the libraries provided by YData. Aftwerwards we deployed this model on UbiOps for professional serving.\n",
    "\n",
    "See these links for more info on UbiOps and YData:\n",
    "https://ydata.ai/\n",
    "https://ubiops.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
